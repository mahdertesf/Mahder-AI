{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1><b>Pretraining a Transformer Model from Scratch on an Amharic Dataset and Fine-Tuning for Amharic Hate Speech Recognition Task</b></h1>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents  \n",
    "1. [Introduction](#introduction)  \n",
    "2. [Importing Packages](#importing-packages)  \n",
    "3. [Dataset Collection & Preprocessing](#dataset-collection--preprocessing)  \n",
    "   - 3.1 [Data Collection](#data-collection)  \n",
    "   - 3.2 [Data Cleaning](#data-cleaning)  \n",
    "   - 3.3 [Tokenization](#tokenization)  \n",
    "   - 3.4 [Tokenizing and Masking](#tokenizing-and-masking)  \n",
    "   - 3.5 [Creating Training Data Pairs](#creating-training-data-pairs)  \n",
    "4. [Pretraining the Transformer Model](#pretraining-the-transformer-model)  \n",
    "   - 4.1 [Positional Encoding](#positional-encoding)  \n",
    "   - 4.2 [Masking](#masking)  \n",
    "   - 4.3 [Self Attention](#self-attention)  \n",
    "   - 4.4 [Encoder](#encoder)  \n",
    "   - 4.5 [Decoder](#decoder)  \n",
    "   - 4.6 [Transformer](#transformer)  \n",
    "   - 4.7 [Initialize_Model](#initialize-model)  \n",
    "   - 4.8 [Pre-training](#pre-training)  \n",
    "5. [Fine-Tuning for Hate Speech Recognition](#fine-tuning-for-hate-speech-recognition)  \n",
    "   - 5.1 [Preparing the Fine-Tuning Dataset](#preparing-the-fine-tuning-dataset)  \n",
    "   - 5.2 [Adjusting the Model Architecture](#adjusting-the-model-architecture)  \n",
    "   - 5.3 [Training the Fine-Tuned Model](#training-the-fine-tuned-model)  \n",
    "6. [Evaluation](#evaluation)  \n",
    "7. [Deployment on Mahder AI App](#deployment-on-mahder-ai-app)  \n",
    "8. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction  \n",
    "\n",
    "In this notebook, I will pretrain a Transformer network on an Amharic dataset collected from a variety of Telegram channels, using the Masked Language Model (MLM). The primary objective of pretraining is to enable the model to learn contextualized word and phrase representations, thereby enhancing its understanding of language semantics. The TransformerРђЎs self-attention mechanism plays a crucial role by allowing the model to dynamically weigh different parts of the input sequence, effectively capturing long-range dependencies in the data.  \n",
    "\n",
    "After pretraining, I will fine-tune the model on a labeled dataset of hate speech and deploy the resulting model in the **Mahder AI** app.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Importing the Packages\n",
    "\n",
    "Let's start by importing all the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import emoji\n",
    "import sentencepiece as spm\n",
    "import string\n",
    "from collections import Counter\n",
    "import time\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Collection & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Collection  \n",
    "\n",
    "In order to pretrain the Transformer network from scratch, we will use **self-supervised learning**, which requires a large corpus of unlabeled text. We will apply a **Masked Language Model (MLM)** to pre-train the model.  \n",
    "\n",
    "#### **Why Telegram Channels?**  \n",
    "Telegram is the most widely used platform for information storage in Ethiopia. For this reason, I have chosen **Telegram channels** as the primary data source. Most of the selected channels are news channels, ensuring a diverse and rich dataset.  \n",
    "\n",
    "#### **Data Collection Method**  \n",
    "To collect the data, I used the **Telethon Python library** and the **Telegram API** to scrape text from selected channels.  \n",
    "\n",
    "#### **Selected Telegram Channels**  \n",
    "The dataset has been collected from the following Telegram channels:  \n",
    "\n",
    "- [Tikvah Ethiopia](https://t.me/tikvahethiopia)  \n",
    "- [Addis Standard Amharic](https://t.me/AddisstandardAmh)  \n",
    "- [Tarikn Wedehuala](https://t.me/TariknWedehuala)  \n",
    "- [Addis News](https://t.me/Addis_News)  \n",
    "- [Zena 24 Now](https://t.me/zena24now)  \n",
    "- [Tikvah University](https://t.me/TikvahUniversity)  \n",
    "- [Tikvah Ethiopia Magazine](https://t.me/tikvahethmagazine)  \n",
    "- [Tikvah Ethiopia Sport](https://t.me/tikvahethsport)  \n",
    "- [Philosophy Thoughts](https://t.me/Philosophy_Thoughts1)  \n",
    "- [Mudenyaz](https://t.me/Mudenyaz)  \n",
    "- [Yemeri Terekoch](https://t.me/yemeri_terekoch)  \n",
    "- [Bemnet Library](https://t.me/Bemnet_Library)  \n",
    "- [Amazing Fact](https://t.me/amazing_fact_433)  \n",
    "- [Zephilosophy](https://t.me/Zephilosophy)  \n",
    "- [Huluezih](https://t.me/huluezih)  \n",
    "\n",
    "#### **Accessing Collected Data**  \n",
    "To access the code and all the raw data collected from each channel, visit the following GitHub repository:  \n",
    "[GitHub Repository Link](https://github.com/your-repo-link-here).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a function that will load data from a JSON file as an array of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    return_data=[]\n",
    "    with open (filepath, \"r\", encoding=\"utf-8\", errors=\"replace\") as file:\n",
    "        datas=json.load(file)\n",
    "        for data in datas:\n",
    "            return_data.append(data[\"text\"])\n",
    "            \n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows how the news look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#рѕўрЅёрІХріЋрІФ\n",
      "\n",
      "рѕ░рІЇріЋ рѕѕрѕўрѕГрІ│рЅх рѕ░рІЇ рѕўрѕєріЋ рЅарЅѓ ріљрІЇ !\n",
      "\n",
      "рЅхрѕІріЋрЅх рІеріФрЅ▓рЅх 1/2017 рІЊ/рѕЮ рЅарїђрѕўрѕерІЇ рІерѕўрЅёрІХріЋрІФ рІеріарѕерїІрІірІФріЋ ріЦріЊ рІеріаріЦрѕЮрѕ« рѕЁрѕЎрѕЏріЋ рѕўрѕГрїЃ рѕЏрІЋріерѕЇ рІерІхрїІрЇЇ рѕЏрѕ░рЅБрѕ░рЅЦ рІўрѕўрЅ╗ ріЦрѕхріЕріЋ 120,000,000 рЅЦрѕГ рЅ░рѕ░рЅЦрѕхрЅДрѕЇрЇб\n",
      "\n",
      "рѕўрЅёрІХріЋрІФ рЅарѕџрІФрѕхрїѕріљрЅБрІЇ рѕєрѕхрЇњрЅ│рѕЇ рїГрѕЮрѕГ рІФрѕѕрІЇ рѕЁріЋрЇЃ рѕѕрѕЏрїаріЊрЅђрЅЁ рІерїѕріЋрІўрЅЦ ріЦрїЦрѕерЅх ріарїІрїЦрѕърЅ│рѕЇрЇб рѕЁріЋрЇЃрІЇ рѕѕрѕЏрїаріЊрЅђрЅЁ рїѕріЋрІўрЅЦ рЅ░рЅИрїЇрѕеріЊрѕЇрЇб рѕѕрѕЏрїаріЊрЅђрЅЁ рІѕрІ░ 5 рЅбрѕірІ«ріЋ рЅЦрѕГ рІФрѕхрЇѕрѕЇрїІрѕЇрЇб\n",
      "\n",
      "рЅарЅђрїЦрЅ│ рІГріерЅ│рЅ░рѕЅ ­ЪЉЄ\n",
      "https://www.youtube.com/live/q0bMjwt9PvM?feature=shared\n",
      "\n",
      "рІерѕЮрЅхрЅйрѕЅрЅхріЋ рѕЂрѕЅ рІхрїІрЇЇ ріарІхрѕГрїЅрЇб\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "­Ъћі #рІерѕарѕФрЅ░ріърЅйрІхрѕЮрїй\n",
      "\n",
      "\" рЅІрѕџ рѕарѕФрЅ░ріърЅй рѕєріљріЋ рѕ│рѕѕ рЅарІ░рѕърІЮ рѕЏрѕ╗рѕ╗рІФрІЇ ріарѕЇрЅ░ріФрЅ░рЅхріЋрѕЮ \" - рІерѕђрІІрѕ│ рІЎрѕфрІФ рІѕрѕерІ│ рѕўріЋрїЇрѕхрЅх рѕарѕФрЅ░ріърЅй\n",
      "\n",
      "рІерѕЏріГрѕ« рібрі«ріќрѕџ рѕЏрѕ╗рѕ╗рІФ рѕфрЇјрѕГрѕЎріЋ рЅ░ріерЅхрѕј рІерѕџріерѕ░рЅ▒ рІеріЉрѕ« рІЅрІхріљрЅхріЊ рЅ░рІФрІФрІЦ рїЅрІ│рІ«рЅйріЋ рЅ│рѕ│рЅб рЅарѕЏрІхрѕерїЇ рІерѕўріЋрїЇрѕхрЅх рѕарѕФрЅ░ріърЅй рІ░рѕърІЮ рѕЏрѕ╗рѕ╗рІФ рЅ░рІ░рѕГрїј ріерїЦрЅЁрѕЮрЅх рІѕрѕГ 2017 рІЊ/рѕЮ рїђрѕЮрѕ« рЅ░рїЇрЅБрѕФрІі рІерЅ░рІ░рѕерїѕ рѕўрѕєріЉ рІГрЅ│рІѕрЅЃрѕЇрЇб\n",
      "\n",
      "рЅарѕ▓рІ│рѕЏ ріГрѕЇрѕЇрЇц рѕ░рѕюріЊрІі рѕ▓рІ│рѕЏ рІъріЋрЇц рѕђрІІрѕ│ рІЎрѕфрІФ рІѕрѕерІ│ рЅарЅ░рѕѕрІФрІЕ рІерѕўріЋрїЇрѕхрЅх рѕўрѕхрѕфрІФ рЅцрЅХрЅй рІерѕџрѕ░рѕЕ рІерѕўріЋрїЇрѕхрЅх рѕарѕФрЅ░ріърЅй рїЇріЋ \" ріе2012 рІЊ/рѕЮ рїђрѕЮрѕ« рЅарЅІрѕџріљрЅх рЅ░рЅђрїЦрѕеріЋ ріЦрІерѕ░рѕФріЋ рІФрѕѕріЋ рЅбрѕєріЋрѕЮ рЅаріарІ▓рѕ▒ рІерѕўріЋрїЇрѕхрЅх рѕарѕФрЅ░ріърЅй рІерІ░рѕўрІѕрІЮ рѕЏрѕ╗рѕ╗рІФ ріарѕЇрЅ░ріФрЅ░рЅхріЋрѕЮ \" рѕ▓рѕЅ рЅЁрѕгрЅ│рЅИрІЅріЋ рѕѕрЅ▓ріГрЅФрѕЁ рібрЅхрІ«рїхрІФ ріарѕхрїѕрЅЦрЅ░рІІрѕЇрЇб\n",
      "\n",
      "рЅЁрѕгрЅ│рЅИрІЅріЋ ріФрІ░рѕерѕ▒ріЋ рѕўріФріерѕЇ рЇд\n",
      "- рЅаріерЅ░рѕЏ рѕЇрѕЏрЅхріЊ рі«ріЋрѕхрЅхрѕФріГрѕйріЋрЇБ\n",
      "- рѕЏрІўрїІрїЃ рЅцрЅХрЅйрЇБ\n",
      "- рЅарЅхрѕЮрѕЁрѕГрЅх рІўрѕГрЇЇ рЇБ\n",
      "- рЅарѕ┤рЅХрЅйріЊ рѕЋрЇЃріЊрЅх ріЦріЋрІ▓рѕЂрѕЮ рЅарѕЋрЅЦрѕерЅх рѕхрѕФ рїй/рЅцрЅХрЅй рІерѕџрѕ░рѕЕ рѕарѕФрЅ░ріърЅй ріЊрЅИрІЇрЇб\n",
      "\n",
      "\" рЅарІѕрЅЁрЅ▒ рЅаріарїЇрЅБрЅА рѕЏрѕхрЅ│рІѕрЅѓрІФ рІѕрїЦрЅХ рЅ░рѕўрІЮрїЇрЅаріЋріЊ рЅ░рІѕрІ│рІхрѕеріЋ рѕЏрѕѕрЇІрЅйріЋ рЅ░рѕерїІрїЇрїд рІерЅІрѕџріљрЅх рІ░рЅЦрІ│рЅц рЅ░рѕ░рїЦрЅХріЋ рѕІрѕѕрЇЅрЅх ріарѕЮрѕхрЅхріЊ рѕхрІхрѕхрЅх рІЊрѕўрЅ│рЅх рІ░рѕърІЮ рѕ▓ріерЇѕрѕѕріЋ рЅарЅєрІеріЋрЅБрЅИрІЇ рѕўрІ░рЅдрЅй рѕІрІГ ріЦрІерѕ░рѕФріЋ рЅБрѕѕріЋрЅарЅх рЅаріарІ▓рѕ▒ рІерІ░рѕърІЮ рѕЏрѕ╗рѕ╗рІФ ріарѕѕрѕўріФрЅ░рЅ│рЅйріЋ рѕѕрІўрѕГрЇѕ рЅЦрІЎ рЅйрїЇрѕ«рЅй рІ│рѕГрїјріЊрѕЇ \" рЅЦрѕѕрІІрѕЇрЇб\n",
      "\n",
      "\" рѕѕрІѕрѕерІ│рІЅ рЇљрЅЦрѕіріГ рѕ░рѕГрЅфрѕхріЊ рІерѕ░рІЅ рѕЃрЅЦрЅх рѕЇрѕЏрЅх рїй/рЅцрЅх ріЦріЊ рѕѕріГрѕЇрѕЅ рЇљрЅЦрѕіріГ рѕ░рѕГрЅфрѕх рЅбрѕ« рЅЁрѕгрЅ│рЅйріЋріЋ рЅаріаріФрѕЇріЊ рЅарЇЁрѕЂрЇЇ рЅЦріЊрЅђрѕГрЅЦрѕЮ рЅ░рїѕрЅбрІЅ рѕЮрѕІрѕй ріарѕЇрЅ░рѕ░рїаріЋрѕЮ рїЅрІ│рІЕріЋ рѕѕрібрЅхрІ«рїхрІФ ріЦрѕЮрЅБ рїарЅБрЅѓ рЅ░рЅІрѕЮ рѕѕрѕЏрЅЁрѕерЅЦ рѕўрѕерїЃ ріЦрІФрІ░рѕФрїђріЋ ріљрІЇ \" рѕ▓рѕЅ рЅ░ріЊрїЇрѕерІІрѕЇрЇб\n",
      "\n",
      "рЅЃрѕІрЅИрІЇріЋ рѕѕрЅ▓ріГрЅФрѕЁ рібрЅхрІ«рїхрІФ рІерѕ░рїАрЅх рЇц рІерѕђрІІрѕ│ рІЎрѕфрІФ рІѕрѕерІ│ рЇљрЅЦрѕіріГ рѕ░рѕГрЅфрѕх ріЦріЊ рІерѕ░рІЅ рѕЃрЅЦрЅх рѕЇрѕЏрЅх рїй/рЅцрЅх ріЃрѕІрЇі ріарЅХ рѕЃрІГрѕЅ ріарЅбріќ рЇЦ \" рЅарІѕрѕерІ│рІЅ рЅа2012 рІЊ/рѕЮ рІеріљрЅарѕерІЇ ріарїЇрЅБрЅЦріљрЅх рЅарѕїрѕѕрІЇ рЅЁрїЦрѕГ рЅаріаріЋрІх рѕўрІ░рЅЦ рѕХрѕхрЅхріЊ ріарѕФрЅх рѕ░рІјрЅйріЋ рЅарЅ░рІ░рѕФрѕФрЅбріљрЅх рІерѕўрЅЁрїарѕГ рѕЂріћрЅ│рІјрЅй ріарѕЂріЋ рѕѕрЅ░рЇѕрїарѕерІЇ рЅйрїЇрѕГ рІІріљріЏ рѕЮріГріЋрІФрЅх рѕєріЌрѕЇ \" рѕ▓рѕЅ рїѕрѕЇрїИрІІрѕЇрЇб\n",
      "\n",
      "ріерІъріЉріЊ рІеріГрѕЇрѕЅ рЇљрЅЦрѕЇріГ рѕ░рѕГрЅфрѕх рїІрѕГ рЅарѕўріЊрЅарЅЦ рѕўрЇЇрЅхрѕћ ріЦрІФрЇѕрѕІрѕѕрїЅ рѕхрѕѕрѕўрѕєріЉрѕЮ рїарЅЂрѕўрІІрѕЇрЇб\n",
      "\n",
      "рЅарІѕрЅЁрЅ▒ рІГрѕЁріЋ рЅ░рїЇрЅБрѕГ рІерЇѕрЇђрѕЎ ріарѕўрѕФрѕ«рЅй ріЦріЊ рІерѕ░рІЅ рѕЃрЅЦрЅх рѕЇрѕЏрЅх ріЃрѕІрЇірІјрЅй рѕІрІГ ріЦрѕГрѕЮрїЃ рѕўрІѕрѕ░рІ▒ріЋ рІерѕџріЊрїѕрѕЕрЅх ріЃрѕІрЇірІЅ рЅарІѕрѕерІ│рІЅ рЅарІџрѕЁ рѕўрѕЇріГ рЅ░рїарЅђрїЦрѕерІЅ рЅаріарІ▓рѕ▒ рІерІ░рѕўрІѕрІЮ рѕЏрѕ╗рѕ╗рІФ рІФрѕЇріФрЅ░рЅ▒ріЊ рЅарЅђрїБрІГ рѕўрЇЇрЅхрѕћ рІерѕџрЇѕрѕѕрїЇрѕІрЅИрІЅ 470 рЅарЅ░рѕѕрІФрІЕ рѕўрѕхрѕфрІФ рЅцрЅХрЅй рІЅрѕхрїЦ рІерЅ░рѕѕрІЕ рѕ░рѕФрЅ░ріърЅй рѕхрѕѕрѕўріќрѕФрЅИрІЅ ріаріГрѕѕрІІрѕЇрЇб\n",
      "\n",
      "рІерѕ░рѕюріЊрІі рѕ▓рІ│рѕЏ рІъріЋ рЇљрЅЦрѕЇріГ рѕ░рѕГрЅфрѕхріЊ рІерѕ░рІЅ рѕЃрІГрѕЇ рѕЇрѕЏрЅх рѕўрѕЮрѕфрІФ ріЃрѕІрЇі ріарЅХ рЅарІЏрЅЦрѕЁ рЅБрѕГрѕХ рЅарЅаріЕрѕІрЅИрІЇ рЅа2011 ріЦріЊ 2012 рЅаріаріерЅБрЅбрІЇ рѕЋрїѕрІѕрїЦ рЅЁрїЦрѕ«рЅй рѕўрЇѕрЇђрѕЏрЅИрІЇріЋ рїѕрѕЇрїИрІІрѕЇрЇб\n",
      "\n",
      "рЅарІѕрѕерІ│рІЅ ріарїБрѕф рЅАрІхріЋ рЅ░рЅІрЅЂрѕъ рЅаріарІ▓рѕ▒ рІ░рѕърІЮ рІФрѕЇрЅ░ріФрЅ░рЅ▒ріЋріЊ рЅарІѕрѕерІ│рІЇ рЅЁрїЦрѕГ рІФрѕЇрЅ░рЇѕрЇђрѕўрЅБрЅИрІЅ ріГрЇЇрЅх рѕўрІ░рЅдрЅйріЋ рІерѕўрѕѕрІерЅх рѕхрѕФ рѕўріеріЊрІѕріЉріЋ ріаріЋрѕхрЅ░рІЅ рЅарѕђрІІрѕ│ рІЎрѕфрІФ рІѕрѕерІ│ рЅЦрЅ╗ 407 ріГрЇЇрЅх рѕўрІ░рЅдрЅй рѕўріќрѕФрЅИрІЅріЋ рѕѕрѕЏрІѕрЅЁ рѕўрЅ╗рѕЅріЋ рїѕрѕЇрЇђрІІрѕЇрЇб\n",
      "\n",
      "рІеріГрѕЇрѕЅ рІерЅарѕІрІГ ріарѕўрѕФрѕ«рЅй рЅарѕџрІФрѕхрЅђрѕЮрїАрЅх ріарЅЁрїБрїФ рѕўрѕ░рѕерЅх ріЦріљрІџрѕЁріЋ рѕарѕФрЅ░ріърЅй рЅаріљрІџрѕЁ ріГрЇЇрЅх рѕўрІ░рЅдрЅй рІерѕўрІ░рѕЇрІ░рѕЇріЊ рѕїрѕјрЅйрѕЮ рѕЋрїІрІі ріарѕўрѕФрї«рЅй рЅарѕўрЇѕрѕѕрїЇ рЅаріарїГрѕГ рїірІю рІЅрѕхрїЦ ріЦрѕЇрЅБрЅх рѕѕрѕўрѕхрїарЅх ріЦрІерЅ░рѕ░рѕФ рѕўрѕєріЉріЋ ріарѕхрЅ│рІЇрЅђрІІрѕЇрЇб\n",
      "\n",
      "рЅ▓ріГрЅФрѕЁ рібрЅхрІ«рїхрІФ рїЅрІ│рІЕріЋ ріЦрѕхріерѕўрїерѕерѕ╗ рЅ░ріерЅ│рЅхрѕј рѕўрѕерїЃрІЇріЋ рІГрѕЇріФрѕЇрЇб\n",
      "\n",
      "#TikvahEthiopiaFamilyHW\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "рІеIMF рѕЏріћрїѓріЋрїЇ рІ│рІГрѕгріГрЅ░рѕ» рѕЮріЋ ріарѕЅ ?\n",
      "\n",
      "рІерІЊрѕѕрѕЮ ріарЅђрЇЇ рІерїѕріЋрІўрЅЦ рЅ░рЅІрѕЮ (IMF) рѕЏріћрїѓріЋрїЇ рІ│рІГрѕгріГрЅ░рѕГ ріГрѕфрѕхрЅ│рѕіріЊ рїєрѕГрїѓрІгрЅФ рЅарібрЅхрІ«рїхрІФ рІерѕхрѕФ рЅєрІГрЅ│ ріарІхрѕГрїѕрІІрѕЇрЇб\n",
      "\n",
      "рЅарІџрѕЁрѕЮ рІѕрЅЁрЅх ріерїа/рѕџ рІљрЅбрІГ ріарѕЁрѕўрІх (рІХ/рѕГ) рїІрѕГ рїерѕЮрѕ« ріерЇїрІ┤рѕФрѕЇ ріерЇЇрЅ░ріЏ рЅБрѕѕрѕхрѕЇрїБріЊрЅх рїІрѕГ рѕўріГрѕерІІрѕЇрЇб\n",
      "\n",
      "рІеріљрЅарѕФрЅИрІЇріЋ рЅєрІГрЅ│ рЅарЅ░рѕўрѕѕріерЅ░ ріерїѕріЋрІўрЅЦ рѕџріњрѕхрЅхрѕЕ ріарЅХ ріарѕЁрѕўрІх рѕйрІ┤ рїІрѕГ рЅарїІрѕФ рѕўрїЇрѕѕрїФ рѕ░рїЦрЅ░рІЇ ріљрЅарѕГрЇб\n",
      "\n",
      "рѕЮріЋ ріарѕЅ ?\n",
      "\n",
      "рІ│рІГрѕгріГрЅ░рѕ» рЇц \" рІерібрЅхрІ«рїхрІФ рѕфрЇјрѕГрѕЮ ріерЅБрІх ріЦріЊ рїірІю рІерѕџрІѕрѕхрІх ріљрІЇ рЇц ріЦрЅБріФрЅйрѕЂ рЅ│рїѕрѕ▒ \" рІерѕџрѕЇ рїЦрѕф ріарЅЁрѕГрЅарІІрѕЇрЇб\n",
      "\n",
      "рібрЅхрІ«рїхрІФрІЇрІФріЋ рѕѕрЅхрІЋрїЇрѕхрЅх ріЦріЋрІ▓рІФрѕ│рІЕ ріЦріЊ ріерѕўріЋрїЇрѕхрЅх рІерібрі«ріќрѕџ рѕЏрѕ╗рѕ╗рІФ рїЦрѕерЅХрЅй рїјріЋ ріЦріЋрІ▓рЅєрѕЎ рїарІГрЅђрІІрѕЇрЇб\n",
      "\n",
      "рїєрѕГрїѓрІгрЅФ рЇЦ \" рІерѕфрЇјрѕГрѕЎріЋ рїЇрЅдрЅй рѕѕрѕЏрѕ│ріФрЅх рІеріаріЋрІхріљрЅх ріарѕхрЇѕрѕІрїі ріљрІЇ \" рѕ▓рѕЅ ріарЇЁріЋрідрЅх рѕ░рїЦрЅ░рІІрѕЇрЇб\n",
      "\n",
      "\" рібрЅхрІ«рїхрІФ рІерЅ░рЅђрЅарѕѕрЅйрІЇ рѕфрЇјрѕГрѕЮ ріерЅБрІх ріЦріЊ рїірІю рІерѕџрІѕрѕхрІх рЅбрѕєріЋрѕЮ ріЦрїЁрїЇ рЅхрѕЇрЅЁ рІЇрїцрЅх рІФрѕхрїѕріЏрѕЇ \" рѕ▓рѕЅ рЅ░ріЊрїЇрѕерІІрѕЇрЇб\n",
      "\n",
      "\" рѕЁрІЮрЅА рЅарЅхрІЋрїЇрѕхрЅх ріЦріЋрІ▓рїарЅЦрЅЁ рїЦрѕфрІгріЋ ріарЅђрѕГрЅБрѕѕрѕЂ \" рІФрѕЅрЅх рѕЏріћрїѓріЋрїЇ рІ│рІГрѕгріГрЅ░рѕ» \" рѕЁрЅЦрѕерЅ░рѕ░рЅА ріерѕфрЇјрѕГрѕЎ рїђрѕГрЅБ рЅарѕўрѕ░рЅБрѕ░рЅЦ рЅаріаріЋрІхріљрЅх рІхрїІрЇЇ рѕЏрІхрѕерїЇ ріарѕѕрЅарЅх \" рЅЦрѕѕрІІрѕЇрЇб\n",
      "\n",
      "рїєрѕГрїѓрІгрЅФ рЇЦ рібрі«ріќрѕџрІЇріЋ рІерЅарѕѕрїа ріарїЦрїІрЅбріЊ рЅЦрЅЂ рѕѕрѕЏрІхрѕерїЇ рЅЦрІЎ рІерѕџрѕарѕФ рѕЦрѕФ ріарѕѕ \" рЅЦрѕѕрІЇ \" ріЦрЅБріФрЅйрѕЂ рѕўріЋрїЇрѕЦрЅх рѕЦрѕФрІЇріЋ ріЦріЋрІ▓рІФрїаріЊрЅЁрЅЁ рІхрїІрЇЇ ріарІхрѕГрїЅ \" рІерѕџрѕЇ рїЦрѕф ріарЅЁрѕГрЅарІІрѕЇрЇб\n",
      "\n",
      "рІерІІрїІ ріЋрѕерЅхріЋ рѕѕрѕўрЇЇрЅ│рЅх рІерѕџрѕ░рѕФрІЇ рѕхрѕФ рІЇрѕхрЅЦрѕхрЅЦ рѕўрѕєріЉріЋ рІФрѕЇрѕИрѕИрїЅрЅх рІ│рІГрѕгріГрЅ░рѕФ \" рІерІІрїІ ріЋрѕерЅхріЋ рІѕрІ░ рЅ│рЅй рѕѕрѕЏрІЇрѕерІх рїаріЋріФрѕФ рІерїѕріЋрІўрЅЦріЊ рІерЇірѕхріФрѕЇ рЇќрѕірѕ▓рІјрЅйрЇБ рІерібрі«ріќрѕџрІЇріЋ рІерѕЏрѕЮрѕерЅх ріарЅЁрѕЮ рѕЏрѕхрЇІрЅхрЇБ рІерІѕрїф ріЋрїЇрІхріЊ рІерІЇрїГ рѕЮріЋрІЏрѕф рїѕрЅбріЋ рѕЏрѕ│рІ░рїЇ ріЦріЊ рІерїЇрѕЅ рѕ┤ріГрЅ░рѕГріЋ рѕЏрЅЦрЅЃрЅх рІГрїарІГрЅЃрѕЇ \" рЅЦрѕѕрІІрѕЇрЇб\n",
      "\n",
      "рѕїрѕІрІЇ рІФріљрѕ▒рЅх рїЅрІ│рІГ рЅаG20 рІерїІрѕФ рѕЏрІЋрЅђрЇЇ рібрЅхрІ«рїхрІФ ріЦрІФріФрѕёрІ░рЅй рІФрѕѕрЅйрІЇріЋ рІерІЋрІ│ рѕўрѕЇрѕХ рѕЏрІ░рѕФрїђрЅх рІхрѕГрІхрѕГ рЅарЅ░рѕўрѕѕріерЅ░ ріљрІЇрЇб\n",
      "\n",
      "рїєрѕГрїѓрІгрЅФ рЇц \" рІерІЋрІ│ рѕўрѕЇрѕХ рѕЏрІІрЅђрѕГ рѕѓрІ░рЅх рІерѕўрїерѕерѕ╗ рІ░рѕерїЃ рѕІрІГ рІГрїѕріЏрѕЇ рЇц ріерібрЅхрІ«рїхрІФ ріарЅарІ│рѕфрІјрЅй рїІрѕГ рЅБрѕѕріЮ рїЇріЋріЎріљрЅх рІГрѕЁ рЅЁрІхрѕџрІФ рІерѕџрѕ░рїарІЇ рїЅрІ│рІГ ріљрІЇ \" рѕ▓рѕЅ рїѕрѕЇрїИрІІрѕЇрЇб \n",
      "\n",
      "рІеIMF рЇЋрѕ«рїЇрѕФрѕЮ ріаріФрѕЇ рѕєріљрІЇріЋ рІерЅ│ріГрѕх ріЦрѕГрѕЮрїЃрІјрЅйріЋ рЅарЅ░рѕўрѕѕріерЅ░рѕЮ рЇц рІерібрЅхрІ«рїхрІФ рЅБрѕѕрѕхрѕЇрїБріЊрЅх рѕѕрЅЦрѕёрѕФрІі рЅарїђрЅ▒ рІхрїІрЇЇ рѕѕрѕЏрІхрѕерїЇ рІѕрѕ│ріЮ рІерѕєріЉ рІерЅ│ріГрѕх ріарЅЁрѕърЅйріЋ рѕўрѕѕрІерЅ│рЅИрІЇріЋ рїарЅЂрѕўрІІрѕЇрЇб \n",
      "\n",
      "рїєрѕГрїѓрІгрЅФ рЇЦ рІерібрЅхрІ«рїхрІФ ріарїарЅЃрѕІрІГ рІерѕђрїѕрѕГ рІЇрѕхрїЦ рѕЮрѕГрЅх рІЋрІхрїѕрЅх ріеIMF рІерѕўрїђрѕўрѕфрІФ рЅхріЋрЅарІФрІјрЅй рѕўрЅЦрѕѕрїАріЋ рѕЏрЅЦрѕФрѕФрЅ│рЅИрІЇріЋ рІўрѕфрЇќрѕГрЅ░рѕГ ріарѕхріљрЅЦрЅДрѕЇрЇб\n",
      "\n",
      "рІерѕЏріћрїѓріЋрїЇ рІ│рІГрѕгріГрЅ░рѕФ ріЋрїЇрїЇрѕГ рЅ░ріерЅхрѕј \" рѕўрѕгрЅх рѕІрІГ ріФрѕѕрІЇ ріЦрІЇріљрЅ│ рїІрѕГ рІерѕџрїѕріЊріЮ ріарІГрІ░рѕѕрѕЮ \" рІерѕџрѕЅ ріарѕхрЅ░рІФрІерЅХрЅй рѕ▓рѕ░рїАрѕЮ рЅ░рѕўрѕЇріГрЅ░ріЊрѕЇрЇб\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "рѕЇрЅ│рѕхрѕўрѕГрЅђрІЅ ріерЅцрЅ░рѕ░рЅдрЅ┐ рЅ░рІ░рЅЦрЅЃ рІерѕўрїБрЅй ріЦрї«ріЏрІЅріЋ рїГріФріћ рЅарЅ░рѕърѕІрЅарЅх рѕЂріћрЅ│ рІерїѕрІ░рѕѕрІЅ рїЇрѕѕрѕ░рЅЦ рЅа20 рІЊрѕўрЅх рїйріЉ ріЦрѕхрѕФрЅх рЅ░рЅђрїБрЇб\n",
      "\n",
      "рЅарІ░рЅАрЅЦ рібрЅхрІ«рїхрІФ ріГрѕЇрѕЇ рїІрѕъ рІъріЋ ріарѕГрЅБрѕЮріЋрїГ ріерЅ░рѕЏ ріарѕхрЅ░рІ│рІ░рѕГ рїЅрѕГрЅБ рЅарѕџрЅБрѕЇ рЅђрЅарѕї рІерїѕрІЏ ріЦрї«ріЏрІЅріЋ рїГріФріћ рЅарЅ░рѕърѕІрЅарЅх рѕўрѕЇріЕ рЅаріарѕ░рЅЃрЅѓ рѕЂріћрЅ│ рЅарѕхрѕѕрЅх ріаріЋрїѕрЅиріЋ рЅарѕўрЅЂрѕерїЦ рѕЋрІГрІѕрЅи ріЦріЋрІ▓рІФрѕЇрЇЇ рІФрІ░рѕерїѕрІЅ рІѕрїБрЅх рЅарЇЁріЉ ріЦрѕхрѕФрЅх рѕўрЅђрїБрЅ▒ріЋ рІеріарѕГрЅБ рѕЮріЋрїГ ріерЅ░рѕЏ ріарѕхрЅ░рІ│рІ░рѕГ рЇќрѕірѕх рѕўрѕЮрѕфрІФ ріарІЏрІЦ рѕЮ/рібріЋрѕхрЇћріГрЅ░рѕГ рїІрЇІрѕ« рЅХрѕЏрѕх рѕѕрЅ▓ріГрЅФрѕЁ рібрЅхрІ«рїхрІФ рЅ░ріЊрїЇрѕерІІрѕЇрЇб\n",
      "\n",
      "рІерІѕріЋрїђрѕЇ рІхрѕГрїірЅ▒ рІерЅ░рЇѕрЇђрѕўрІЅ ріљрѕљрѕ┤ 16/2016 рІЊ/рѕЮ рЅаріарѕГрЅБрѕЮріЋрїГ ріерЅ░рѕЏ рїЅрѕГрЅБ рЅђрЅарѕї ріљрІЇрЇб\n",
      "\n",
      "рЅ░ріерѕ│рѕй рІ«ріЊрѕх рїФрЇірЅё рІерЅ░рЅБрѕѕрІЇ рїЇрѕѕрѕ░рЅЦ рІерїѓріЋріФ рІЕріњрЅерѕГрѕ▓рЅ▓ 1ріЏ┬а рІЊрѕўрЅх рЅ░рѕЏрѕф рІерѕєріљрЅйрІЅріЋ рѕЪрЅй рѕірІ▓рІФ рІ«рѕљріЋрѕх ріЦрѕ▒ріЋ рѕѕрѕЏрѕхрѕўрѕерЅЁ рІѕрІ░ ріарѕГрЅБ рѕЮріЋрїГ ріерЅ░рѕЏ рЅарѕўрїБрЅйрЅарЅх рЅ░ріерѕФрІГрЅХ рЅарѕџріќрѕГрЅарЅх рЅцрЅх ріарѕ░рЅЃрЅѓ рІхрѕГрїірЅ▒ріЋ рѕўрЇѕрЇђрѕЎріЋ рІерѕЮрѕГрѕўрѕФ рѕўрІЮрїѕрЅА рІФрѕхрѕерІ│рѕЇрЇб\n",
      "\n",
      "рІѕрїБрЅи \" ріЦрї«ріЏрІг рІГрѕўрѕерЅЁрѕЇріЏрѕЇ \" рЅарѕџрѕЇ рІ░рѕхрЅ│ ріерЅцрЅ░рѕ░рЅдрЅ┐ рЅ░рІ░рЅЦрЅЃ рЅ░ріерѕ│рѕй рЅ░ріерѕФрІГрЅХ рІѕрІ░ рѕџрѕЏрѕГрЅарЅх┬а рЅцрЅх рѕўрїЦрЅ│ рЅарІІрІюрѕЏрІЅ рѕѕрѕЮрѕерЅЃрІЅ рІерѕџрѕєріЉ рІерІ▓рі«рѕГрЇБ рІерІ│рЅдріЊ рѕѕрѕхрѕІрѕ│ рѕўрїарїдрЅйріЊ рЅарЅАріЊ рІЮрїЇрїЁрЅх рЅцрЅ▒ріЋ ріарѕ░рѕЏрѕЮрѕФ рЅарѕЮрѕйрЅ▒рѕЮ рїЇрЅб рІЅрѕхрїЦ рІФрѕЅ рЅ░ріерѕФрІ«рЅйріЋ рїарѕГрЅ░рІЅ ріерѕИріЎ рЅаріІрѕІ рѕЪрЅй рѕђрїѕрѕГ рѕ░рѕІрѕЮ рЅЦрѕІ рЅарЅ░ріЏрЅйрЅарЅх ріерѕїрѕірЅ▒ 7 рѕ░рІЊрЅх рїѕрІ░рѕЏ ріЦрѕФрѕиріЋ рѕўріерѕІріерѕЇ рЅарѕЏрЅхрЅйрѕЇрЅарЅх рѕЂріћрЅ│ рЅарЅбрѕІрІІ ріаріЋрїѕрЅиріЋ ріарѕГрІХ рѕўрїЇрІ░рѕЅріЋ рІерѕЮрѕГрѕўрѕФ рѕўрІЮрїѕрЅАріЋ рІІрЅб ріарІхрѕГрїѕрІЇ рЇќрѕірѕх ріарІЏрІА рїѕрѕЇрЇђрІІрѕЇрЇб\n",
      "\n",
      "рЇќрѕірѕх ріарІЏрІА ріаріГрѕѕрІЇ ріЦріЋрІ░рїѕрѕѕрї╣рЅх рЇЦ рЅарІѕрЅЁрЅ▒ рЅарЅ░рІ░рѕерїѕрІЅ рѕЏрїБрѕФрЅхрѕЮ рѕєріљ рЅаріГрѕх рѕўрІЮрїѕрЅА рѕІрІГ ріЦріЋрІ░рѕ░рЇѕрѕерІЅ рІѕріЋрїђрѕѕріЏрІЇ  \" рІѕрІ░ рІЕріњрЅерѕГрѕ▓рЅ▓ рЅарѕёрІхрѕйрЅарЅх рѕїрѕІ рІерІѕріЋрІх рїЊрІ░ріЏ рІГрІўрѕ╗рѕЇ \" рЅарѕџрѕЇ ріљрІЇ рЅарѕГ рІўрїЇрЅХ ріарѕ░рЅЃрЅѓ рІерІѕріЋрїђрѕЇ рІхрѕГрїірЅ▒ріЋ рІерЇѕрїИрѕўрІЇрЇб\n",
      "\n",
      "рЇќрѕірѕх рЅарІџрѕЁ рІўрїЇріЊріЮ рІѕріЋрїђрѕЇ рІЎрѕфрІФ рЅ░рїѕрЅбрІЅріЋ рѕЏрїБрѕФрЅхріЊ рѕЮрѕГрѕўрѕФ ріарІхрѕГрїј рѕѕрІљрЅЃрЅц рѕЋрїЇ рѕЏрЅЁрѕерЅАріЋ ріарѕхрЅ│рІЇрЅђрІІрѕЇрЇб\n",
      "\n",
      "рІљрЅЃрЅц рѕЋрїЇрѕЮ ріГрѕх рЅарѕўрѕўрѕхрѕерЅх рѕѕрЇЇрѕГрІх рЅцрЅ▒ рЅ░ріерѕ│рѕй рІерІѕріЋрїђрѕЇ рІхрѕГрїірЅ▒ріЋ рѕўрЇѕрЇђрѕЎріЋ рІерѕ░рІЅрЇБ рІерѕ░ріљрІхріЊ рІерѕЁріГрѕЮріЊ рѕЏрѕхрѕерїЃрІјрЅйріЋ рЅарѕЏрЅЁрѕерЅЦ ріарѕхрѕерІхрЅирѕЇрЇб\n",
      "\n",
      "рЅарЅђрѕерЅА рѕЏрѕхрѕерїЃрІјрЅй ріЦріЊ рѕЮрѕхріГрѕ«рЅй рїЇрѕФ рЅђріЎріЋ рѕ▓рІФрїБрѕФ рІерЅєрІерІЅ рІерїІрѕъ рІъріЋ ріерЇЇрЅ░ріЏрІЅ рЇЇрѕГрІх рЅцрЅх рЅарЅђріЋ 29/5/2017 рІЊрѕЮ рЅарІІрѕѕрІЅ рЅйрѕјрЅх рЅ░ріерѕ│рѕй рІ«ріЊрѕх рїерЇірЅё рЅарЅ░ріерѕ░рѕ░рЅарЅх рЅаріарѕ░рЅЃрЅѓ рѕЂріћрЅ│ ріљрЅЦрѕх рІерѕЏрїЦрЇІрЅх рІѕріЋрїђрѕЇ рїЦрЇІрЅ░ріЏ рѕўрѕєріЉріЋ рЅарѕЏрѕерїІрїѕрїЦ рЅа20 рІЊрѕўрЅх рЇЁріЉ ріЦрѕхрѕФрЅх ріЦріЋрІ▓рЅђрїБ рѕўрІѕрѕ░ріЉріЋрѕЮ рібріЋрѕхрЇћріГрЅ░рѕГ рїІрЇІрѕ« рЅХрѕЏрѕх рѕѕрЅ▓ріГрЅФрѕЁ рібрЅхрІ«рїхрІФ рЅ░ріЊрїЇрѕерІІрѕЇрЇб\n",
      "\n",
      "#TikvahEthiopiaFamilyHW\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "#рѕўрЅёрІХріЋрІФ\n",
      "\n",
      "\" рѕ░рІЇріЋ рѕѕрѕўрѕГрІ│рЅх рѕ░рІЇ рѕўрѕєріЋ рЅарЅѓ ріљрІЇ !! \"\n",
      "\n",
      "рѕўрЅёрІХріЋрІФ рІеріарѕерїІрІірІФріЋріЊ рІеріаріЦрѕЮрѕ« рѕЁрѕЎрѕЏріЋ рѕўрѕГрїЃ рѕЏрІЋріерѕЇ рЅарѕџрІФрѕхрїѕріљрЅБрІЇ рѕєрѕхрЇњрЅ│рѕЇ рїГрѕЮрѕГ рІФрѕѕрІЇ рѕЁріЋрЇЃ рѕѕрѕЏрїаріЊрЅђрЅЁ рІерїѕріЋрІўрЅЦ ріЦрїЦрѕерЅх рѕхрѕѕрїѕрїарѕўрІЇ ріЦрїЇрІЏ ріЦріЋрІ▓рІ░рѕерїЇ рѕѕрѕўрѕІрІЇ рібрЅхрІ«рїхрІФрІЇрІФріЋ рїЦрѕф рѕўрЅЁрѕерЅА рІГрЅ│рІѕрѕ│рѕЇрЇб\n",
      "\n",
      "рІЏрѕг Рђю рЅарѕ░рІГрЇЅ рібрЅбріцрѕх рІерІЕрЅ▒рЅЦ рЅ╗ріљрѕЇ РђЮ рІхрїІрЇЇ рѕЏрІхрѕерїірІФ рѕўрѕГрѕђ рїЇрЅЦрѕГ ріЦрІерЅ░ріФрѕёрІ░ ріљрІЇрЇб\n",
      "\n",
      "рІ░рїІрїјрЅй рѕЂрѕЅ рІхрїІрЇЇ ріЦріЋрІхрЅ│рІ░рѕГрїЅ рїЦрѕф ріЦріЊрЅђрѕГрЅБрѕѕріЋрЇб\n",
      "\n",
      "рІерІхрїІрЇЇ рѕЏрІхрѕерїірІФ ріарѕЏрѕФрї«рЅй ріерѕІрІГ рЅ░рІФрІГрІўрІІрѕЇрЇб\n",
      "\n",
      "рѕўрЅёрІХріЋрІФ Рђю рѕЁріЋрЇЃрІЇ рѕѕрѕЏрїаріЊрЅђрЅЁ рїѕріЋрІўрЅЦ рЅ░рЅИрїЇрѕеріЊрѕЇрЇб рѕѕрѕЏрїаріЊрЅђрЅЁ рІѕрІ░ 5 рЅбрѕірІ«ріЋ рЅЦрѕГ рІФрѕхрЇѕрѕЇрїѕріЊрѕЇ РђЮ рѕЏрѕѕрЅ▒ ріарІГрІўріљрїІрѕЮрЇб\n",
      "\n",
      "рЅарЅђрїЦрЅ│ рІГріерЅ│рЅ░рѕЅ ­ЪЉЄ\n",
      "https://www.youtube.com/live/q0bMjwt9PvM?feature=shared\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample=load_data(\"datas/sample.json\")\n",
    "for news in sample[:5]:\n",
    "    print(news)\n",
    "    \n",
    "    print(\"---------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with a Telegram dataset, we aim to clean the text by removing substrings that are commonly used on the platform, such as hashtagged entities, usernames, hyperlinks,emojis, and english words. To achieve this, we will use Python's re library to perform regular expression operations. We will define specific search patterns and use the sub() method to remove matches by replacing them with an empty string (''). We will also remove unecessary multiple spaces to a single space\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    text=re.sub(r'@\\S+', '', text)\n",
    "    text=emoji.replace_emoji(text,\" \")\n",
    "    english_pattern = re.compile(r'\\b[A-Za-z]+\\b')   \n",
    "    cleaned_text = re.sub(english_pattern, '', text)    \n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's test the above function on sample data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's load our training data and see how many contents we have and what the first 5 contents look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of contents: 193419\n",
      "\n",
      "First 5 contents: \n",
      "\n",
      "#рѕўрЅёрІХріЋрІФ\n",
      "\n",
      "рѕ░рІЇріЋ рѕѕрѕўрѕГрІ│рЅх рѕ░рІЇ рѕўрѕєріЋ рЅарЅѓ ріљрІЇ !\n",
      "\n",
      "рЅхрѕІріЋрЅх рІеріФрЅ▓рЅх 1/2017 рІЊ/рѕЮ рЅарїђрѕўрѕерІЇ рІерѕўрЅёрІХріЋрІФ рІеріарѕерїІрІірІФріЋ ріЦріЊ рІеріаріЦрѕЮрѕ« рѕЁрѕЎрѕЏріЋ рѕўрѕГрїЃ рѕЏрІЋріерѕЇ рІерІхрїІрЇЇ рѕЏрѕ░рЅБрѕ░рЅЦ рІўрѕўрЅ╗ ріЦрѕхріЕріЋ 120,000,000 рЅЦрѕГ рЅ░рѕ░рЅЦрѕхрЅДрѕЇрЇб\n",
      "\n",
      "рѕўрЅёрІХріЋрІФ рЅарѕџрІФрѕхрїѕріљрЅБрІЇ рѕєрѕхрЇњрЅ│рѕЇ рїГрѕЮрѕГ рІФрѕѕрІЇ рѕЁріЋрЇЃ рѕѕрѕЏрїаріЊрЅђрЅЁ рІерїѕріЋрІўрЅЦ ріЦрїЦрѕерЅх ріарїІрїЦрѕърЅ│рѕЇрЇб рѕЁріЋрЇЃрІЇ рѕѕрѕЏрїаріЊрЅђрЅЁ рїѕріЋрІўрЅЦ рЅ░рЅИрїЇрѕеріЊрѕЇрЇб рѕѕрѕЏрїаріЊрЅђрЅЁ рІѕрІ░ 5 рЅбрѕірІ«ріЋ рЅЦрѕГ рІФрѕхрЇѕрѕЇрїІрѕЇрЇб\n",
      "\n",
      "рЅарЅђрїЦрЅ│ рІГріерЅ│рЅ░рѕЅ ­ЪЉЄ\n",
      "https://www.youtube.com/live/q0bMjwt9PvM?feature=shared\n",
      "\n",
      "рІерѕЮрЅхрЅйрѕЅрЅхріЋ рѕЂрѕЅ рІхрїІрЇЇ ріарІхрѕГрїЅрЇб\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "­Ъћі #рІерѕарѕФрЅ░ріърЅйрІхрѕЮрїй\n",
      "\n",
      "\" рЅІрѕџ рѕарѕФрЅ░ріърЅй рѕєріљріЋ рѕ│рѕѕ рЅарІ░рѕърІЮ рѕЏрѕ╗рѕ╗рІФрІЇ ріарѕЇрЅ░ріФрЅ░рЅхріЋрѕЮ \" - рІерѕђрІІрѕ│ рІЎрѕфрІФ рІѕрѕерІ│ рѕўріЋрїЇрѕхрЅх рѕарѕФрЅ░ріърЅй\n",
      "\n",
      "рІерѕЏріГрѕ« рібрі«ріќрѕџ рѕЏрѕ╗рѕ╗рІФ рѕфрЇјрѕГрѕЎріЋ рЅ░ріерЅхрѕј рІерѕџріерѕ░рЅ▒ рІеріЉрѕ« рІЅрІхріљрЅхріЊ рЅ░рІФрІФрІЦ рїЅрІ│рІ«рЅйріЋ рЅ│рѕ│рЅб рЅарѕЏрІхрѕерїЇ рІерѕўріЋрїЇрѕхрЅх рѕарѕФрЅ░ріърЅй рІ░рѕърІЮ рѕЏрѕ╗рѕ╗рІФ рЅ░рІ░рѕГрїј ріерїЦрЅЁрѕЮрЅх рІѕрѕГ 2017 рІЊ/рѕЮ рїђрѕЮрѕ« рЅ░рїЇрЅБрѕФрІі рІерЅ░рІ░рѕерїѕ рѕўрѕєріЉ рІГрЅ│рІѕрЅЃрѕЇрЇб\n",
      "\n",
      "рЅарѕ▓рІ│рѕЏ ріГрѕЇрѕЇрЇц рѕ░рѕюріЊрІі рѕ▓рІ│рѕЏ рІъріЋрЇц рѕђрІІрѕ│ рІЎрѕфрІФ рІѕрѕерІ│ рЅарЅ░рѕѕрІФрІЕ рІерѕўріЋрїЇрѕхрЅх рѕўрѕхрѕфрІФ рЅцрЅХрЅй рІерѕџрѕ░рѕЕ рІерѕўріЋрїЇрѕхрЅх рѕарѕФрЅ░ріърЅй рїЇріЋ \" ріе2012 рІЊ/рѕЮ рїђрѕЮрѕ« рЅарЅІрѕџріљрЅх рЅ░рЅђрїЦрѕеріЋ ріЦрІерѕ░рѕФріЋ рІФрѕѕріЋ рЅбрѕєріЋрѕЮ рЅаріарІ▓рѕ▒ рІерѕўріЋрїЇрѕхрЅх рѕарѕФрЅ░ріърЅй рІерІ░рѕўрІѕрІЮ рѕЏрѕ╗рѕ╗рІФ ріарѕЇрЅ░ріФрЅ░рЅхріЋрѕЮ \" рѕ▓рѕЅ рЅЁрѕгрЅ│рЅИрІЅріЋ рѕѕрЅ▓ріГрЅФрѕЁ рібрЅхрІ«рїхрІФ ріарѕхрїѕрЅЦрЅ░рІІрѕЇрЇб\n",
      "\n",
      "рЅЁрѕгрЅ│рЅИрІЅріЋ ріФрІ░рѕерѕ▒ріЋ рѕўріФріерѕЇ рЇд\n",
      "- рЅаріерЅ░рѕЏ рѕЇрѕЏрЅхріЊ рі«ріЋрѕхрЅхрѕФріГрѕйріЋрЇБ\n",
      "- рѕЏрІўрїІрїЃ рЅцрЅХрЅйрЇБ\n",
      "- рЅарЅхрѕЮрѕЁрѕГрЅх рІўрѕГрЇЇ рЇБ\n",
      "- рЅарѕ┤рЅХрЅйріЊ рѕЋрЇЃріЊрЅх ріЦріЋрІ▓рѕЂрѕЮ рЅарѕЋрЅЦрѕерЅх рѕхрѕФ рїй/рЅцрЅХрЅй рІерѕџрѕ░рѕЕ рѕарѕФрЅ░ріърЅй ріЊрЅИрІЇрЇб\n",
      "\n",
      "\" рЅарІѕрЅЁрЅ▒ рЅаріарїЇрЅБрЅА рѕЏрѕхрЅ│рІѕрЅѓрІФ рІѕрїЦрЅХ рЅ░рѕўрІЮрїЇрЅаріЋріЊ рЅ░рІѕрІ│рІхрѕеріЋ рѕЏрѕѕрЇІрЅйріЋ рЅ░рѕерїІрїЇрїд рІерЅІрѕџріљрЅх рІ░рЅЦрІ│рЅц рЅ░рѕ░рїЦрЅХріЋ рѕІрѕѕрЇЅрЅх ріарѕЮрѕхрЅхріЊ рѕхрІхрѕхрЅх рІЊрѕўрЅ│рЅх рІ░рѕърІЮ рѕ▓ріерЇѕрѕѕріЋ рЅарЅєрІеріЋрЅБрЅИрІЇ рѕўрІ░рЅдрЅй рѕІрІГ ріЦрІерѕ░рѕФріЋ рЅБрѕѕріЋрЅарЅх рЅаріарІ▓рѕ▒ рІерІ░рѕърІЮ рѕЏрѕ╗рѕ╗рІФ ріарѕѕрѕўріФрЅ░рЅ│рЅйріЋ рѕѕрІўрѕГрЇѕ рЅЦрІЎ рЅйрїЇрѕ«рЅй рІ│рѕГрїјріЊрѕЇ \" рЅЦрѕѕрІІрѕЇрЇб\n",
      "\n",
      "\" рѕѕрІѕрѕерІ│рІЅ рЇљрЅЦрѕіріГ рѕ░рѕГрЅфрѕхріЊ рІерѕ░рІЅ рѕЃрЅЦрЅх рѕЇрѕЏрЅх рїй/рЅцрЅх ріЦріЊ рѕѕріГрѕЇрѕЅ рЇљрЅЦрѕіріГ рѕ░рѕГрЅфрѕх рЅбрѕ« рЅЁрѕгрЅ│рЅйріЋріЋ рЅаріаріФрѕЇріЊ рЅарЇЁрѕЂрЇЇ рЅЦріЊрЅђрѕГрЅЦрѕЮ рЅ░рїѕрЅбрІЅ рѕЮрѕІрѕй ріарѕЇрЅ░рѕ░рїаріЋрѕЮ рїЅрІ│рІЕріЋ рѕѕрібрЅхрІ«рїхрІФ ріЦрѕЮрЅБ рїарЅБрЅѓ рЅ░рЅІрѕЮ рѕѕрѕЏрЅЁрѕерЅЦ рѕўрѕерїЃ ріЦрІФрІ░рѕФрїђріЋ ріљрІЇ \" рѕ▓рѕЅ рЅ░ріЊрїЇрѕерІІрѕЇрЇб\n",
      "\n",
      "рЅЃрѕІрЅИрІЇріЋ рѕѕрЅ▓ріГрЅФрѕЁ рібрЅхрІ«рїхрІФ рІерѕ░рїАрЅх рЇц рІерѕђрІІрѕ│ рІЎрѕфрІФ рІѕрѕерІ│ рЇљрЅЦрѕіріГ рѕ░рѕГрЅфрѕх ріЦріЊ рІерѕ░рІЅ рѕЃрЅЦрЅх рѕЇрѕЏрЅх рїй/рЅцрЅх ріЃрѕІрЇі ріарЅХ рѕЃрІГрѕЅ ріарЅбріќ рЇЦ \" рЅарІѕрѕерІ│рІЅ рЅа2012 рІЊ/рѕЮ рІеріљрЅарѕерІЇ ріарїЇрЅБрЅЦріљрЅх рЅарѕїрѕѕрІЇ рЅЁрїЦрѕГ рЅаріаріЋрІх рѕўрІ░рЅЦ рѕХрѕхрЅхріЊ ріарѕФрЅх рѕ░рІјрЅйріЋ рЅарЅ░рІ░рѕФрѕФрЅбріљрЅх рІерѕўрЅЁрїарѕГ рѕЂріћрЅ│рІјрЅй ріарѕЂріЋ рѕѕрЅ░рЇѕрїарѕерІЇ рЅйрїЇрѕГ рІІріљріЏ рѕЮріГріЋрІФрЅх рѕєріЌрѕЇ \" рѕ▓рѕЅ рїѕрѕЇрїИрІІрѕЇрЇб\n",
      "\n",
      "ріерІъріЉріЊ рІеріГрѕЇрѕЅ рЇљрЅЦрѕЇріГ рѕ░рѕГрЅфрѕх рїІрѕГ рЅарѕўріЊрЅарЅЦ рѕўрЇЇрЅхрѕћ ріЦрІФрЇѕрѕІрѕѕрїЅ рѕхрѕѕрѕўрѕєріЉрѕЮ рїарЅЂрѕўрІІрѕЇрЇб\n",
      "\n",
      "рЅарІѕрЅЁрЅ▒ рІГрѕЁріЋ рЅ░рїЇрЅБрѕГ рІерЇѕрЇђрѕЎ ріарѕўрѕФрѕ«рЅй ріЦріЊ рІерѕ░рІЅ рѕЃрЅЦрЅх рѕЇрѕЏрЅх ріЃрѕІрЇірІјрЅй рѕІрІГ ріЦрѕГрѕЮрїЃ рѕўрІѕрѕ░рІ▒ріЋ рІерѕџріЊрїѕрѕЕрЅх ріЃрѕІрЇірІЅ рЅарІѕрѕерІ│рІЅ рЅарІџрѕЁ рѕўрѕЇріГ рЅ░рїарЅђрїЦрѕерІЅ рЅаріарІ▓рѕ▒ рІерІ░рѕўрІѕрІЮ рѕЏрѕ╗рѕ╗рІФ рІФрѕЇріФрЅ░рЅ▒ріЊ рЅарЅђрїБрІГ рѕўрЇЇрЅхрѕћ рІерѕџрЇѕрѕѕрїЇрѕІрЅИрІЅ 470 рЅарЅ░рѕѕрІФрІЕ рѕўрѕхрѕфрІФ рЅцрЅХрЅй рІЅрѕхрїЦ рІерЅ░рѕѕрІЕ рѕ░рѕФрЅ░ріърЅй рѕхрѕѕрѕўріќрѕФрЅИрІЅ ріаріГрѕѕрІІрѕЇрЇб\n",
      "\n",
      "рІерѕ░рѕюріЊрІі рѕ▓рІ│рѕЏ рІъріЋ рЇљрЅЦрѕЇріГ рѕ░рѕГрЅфрѕхріЊ рІерѕ░рІЅ рѕЃрІГрѕЇ рѕЇрѕЏрЅх рѕўрѕЮрѕфрІФ ріЃрѕІрЇі ріарЅХ рЅарІЏрЅЦрѕЁ рЅБрѕГрѕХ рЅарЅаріЕрѕІрЅИрІЇ рЅа2011 ріЦріЊ 2012 рЅаріаріерЅБрЅбрІЇ рѕЋрїѕрІѕрїЦ рЅЁрїЦрѕ«рЅй рѕўрЇѕрЇђрѕЏрЅИрІЇріЋ рїѕрѕЇрїИрІІрѕЇрЇб\n",
      "\n",
      "рЅарІѕрѕерІ│рІЅ ріарїБрѕф рЅАрІхріЋ рЅ░рЅІрЅЂрѕъ рЅаріарІ▓рѕ▒ рІ░рѕърІЮ рІФрѕЇрЅ░ріФрЅ░рЅ▒ріЋріЊ рЅарІѕрѕерІ│рІЇ рЅЁрїЦрѕГ рІФрѕЇрЅ░рЇѕрЇђрѕўрЅБрЅИрІЅ ріГрЇЇрЅх рѕўрІ░рЅдрЅйріЋ рІерѕўрѕѕрІерЅх рѕхрѕФ рѕўріеріЊрІѕріЉріЋ ріаріЋрѕхрЅ░рІЅ рЅарѕђрІІрѕ│ рІЎрѕфрІФ рІѕрѕерІ│ рЅЦрЅ╗ 407 ріГрЇЇрЅх рѕўрІ░рЅдрЅй рѕўріќрѕФрЅИрІЅріЋ рѕѕрѕЏрІѕрЅЁ рѕўрЅ╗рѕЅріЋ рїѕрѕЇрЇђрІІрѕЇрЇб\n",
      "\n",
      "рІеріГрѕЇрѕЅ рІерЅарѕІрІГ ріарѕўрѕФрѕ«рЅй рЅарѕџрІФрѕхрЅђрѕЮрїАрЅх ріарЅЁрїБрїФ рѕўрѕ░рѕерЅх ріЦріљрІџрѕЁріЋ рѕарѕФрЅ░ріърЅй рЅаріљрІџрѕЁ ріГрЇЇрЅх рѕўрІ░рЅдрЅй рІерѕўрІ░рѕЇрІ░рѕЇріЊ рѕїрѕјрЅйрѕЮ рѕЋрїІрІі ріарѕўрѕФрї«рЅй рЅарѕўрЇѕрѕѕрїЇ рЅаріарїГрѕГ рїірІю рІЅрѕхрїЦ ріЦрѕЇрЅБрЅх рѕѕрѕўрѕхрїарЅх ріЦрІерЅ░рѕ░рѕФ рѕўрѕєріЉріЋ ріарѕхрЅ│рІЇрЅђрІІрѕЇрЇб\n",
      "\n",
      "рЅ▓ріГрЅФрѕЁ рібрЅхрІ«рїхрІФ рїЅрІ│рІЕріЋ ріЦрѕхріерѕўрїерѕерѕ╗ рЅ░ріерЅ│рЅхрѕј рѕўрѕерїЃрІЇріЋ рІГрѕЇріФрѕЇрЇб\n",
      "\n",
      "#TikvahEthiopiaFamilyHW\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "рІеIMF рѕЏріћрїѓріЋрїЇ рІ│рІГрѕгріГрЅ░рѕ» рѕЮріЋ ріарѕЅ ?\n",
      "\n",
      "рІерІЊрѕѕрѕЮ ріарЅђрЇЇ рІерїѕріЋрІўрЅЦ рЅ░рЅІрѕЮ (IMF) рѕЏріћрїѓріЋрїЇ рІ│рІГрѕгріГрЅ░рѕГ ріГрѕфрѕхрЅ│рѕіріЊ рїєрѕГрїѓрІгрЅФ рЅарібрЅхрІ«рїхрІФ рІерѕхрѕФ рЅєрІГрЅ│ ріарІхрѕГрїѕрІІрѕЇрЇб\n",
      "\n",
      "рЅарІџрѕЁрѕЮ рІѕрЅЁрЅх ріерїа/рѕџ рІљрЅбрІГ ріарѕЁрѕўрІх (рІХ/рѕГ) рїІрѕГ рїерѕЮрѕ« ріерЇїрІ┤рѕФрѕЇ ріерЇЇрЅ░ріЏ рЅБрѕѕрѕхрѕЇрїБріЊрЅх рїІрѕГ рѕўріГрѕерІІрѕЇрЇб\n",
      "\n",
      "рІеріљрЅарѕФрЅИрІЇріЋ рЅєрІГрЅ│ рЅарЅ░рѕўрѕѕріерЅ░ ріерїѕріЋрІўрЅЦ рѕџріњрѕхрЅхрѕЕ ріарЅХ ріарѕЁрѕўрІх рѕйрІ┤ рїІрѕГ рЅарїІрѕФ рѕўрїЇрѕѕрїФ рѕ░рїЦрЅ░рІЇ ріљрЅарѕГрЇб\n",
      "\n",
      "рѕЮріЋ ріарѕЅ ?\n",
      "\n",
      "рІ│рІГрѕгріГрЅ░рѕ» рЇц \" рІерібрЅхрІ«рїхрІФ рѕфрЇјрѕГрѕЮ ріерЅБрІх ріЦріЊ рїірІю рІерѕџрІѕрѕхрІх ріљрІЇ рЇц ріЦрЅБріФрЅйрѕЂ рЅ│рїѕрѕ▒ \" рІерѕџрѕЇ рїЦрѕф ріарЅЁрѕГрЅарІІрѕЇрЇб\n",
      "\n",
      "рібрЅхрІ«рїхрІФрІЇрІФріЋ рѕѕрЅхрІЋрїЇрѕхрЅх ріЦріЋрІ▓рІФрѕ│рІЕ ріЦріЊ ріерѕўріЋрїЇрѕхрЅх рІерібрі«ріќрѕџ рѕЏрѕ╗рѕ╗рІФ рїЦрѕерЅХрЅй рїјріЋ ріЦріЋрІ▓рЅєрѕЎ рїарІГрЅђрІІрѕЇрЇб\n",
      "\n",
      "рїєрѕГрїѓрІгрЅФ рЇЦ \" рІерѕфрЇјрѕГрѕЎріЋ рїЇрЅдрЅй рѕѕрѕЏрѕ│ріФрЅх рІеріаріЋрІхріљрЅх ріарѕхрЇѕрѕІрїі ріљрІЇ \" рѕ▓рѕЅ ріарЇЁріЋрідрЅх рѕ░рїЦрЅ░рІІрѕЇрЇб\n",
      "\n",
      "\" рібрЅхрІ«рїхрІФ рІерЅ░рЅђрЅарѕѕрЅйрІЇ рѕфрЇјрѕГрѕЮ ріерЅБрІх ріЦріЊ рїірІю рІерѕџрІѕрѕхрІх рЅбрѕєріЋрѕЮ ріЦрїЁрїЇ рЅхрѕЇрЅЁ рІЇрїцрЅх рІФрѕхрїѕріЏрѕЇ \" рѕ▓рѕЅ рЅ░ріЊрїЇрѕерІІрѕЇрЇб\n",
      "\n",
      "\" рѕЁрІЮрЅА рЅарЅхрІЋрїЇрѕхрЅх ріЦріЋрІ▓рїарЅЦрЅЁ рїЦрѕфрІгріЋ ріарЅђрѕГрЅБрѕѕрѕЂ \" рІФрѕЅрЅх рѕЏріћрїѓріЋрїЇ рІ│рІГрѕгріГрЅ░рѕ» \" рѕЁрЅЦрѕерЅ░рѕ░рЅА ріерѕфрЇјрѕГрѕЎ рїђрѕГрЅБ рЅарѕўрѕ░рЅБрѕ░рЅЦ рЅаріаріЋрІхріљрЅх рІхрїІрЇЇ рѕЏрІхрѕерїЇ ріарѕѕрЅарЅх \" рЅЦрѕѕрІІрѕЇрЇб\n",
      "\n",
      "рїєрѕГрїѓрІгрЅФ рЇЦ рібрі«ріќрѕџрІЇріЋ рІерЅарѕѕрїа ріарїЦрїІрЅбріЊ рЅЦрЅЂ рѕѕрѕЏрІхрѕерїЇ рЅЦрІЎ рІерѕџрѕарѕФ рѕЦрѕФ ріарѕѕ \" рЅЦрѕѕрІЇ \" ріЦрЅБріФрЅйрѕЂ рѕўріЋрїЇрѕЦрЅх рѕЦрѕФрІЇріЋ ріЦріЋрІ▓рІФрїаріЊрЅЁрЅЁ рІхрїІрЇЇ ріарІхрѕГрїЅ \" рІерѕџрѕЇ рїЦрѕф ріарЅЁрѕГрЅарІІрѕЇрЇб\n",
      "\n",
      "рІерІІрїІ ріЋрѕерЅхріЋ рѕѕрѕўрЇЇрЅ│рЅх рІерѕџрѕ░рѕФрІЇ рѕхрѕФ рІЇрѕхрЅЦрѕхрЅЦ рѕўрѕєріЉріЋ рІФрѕЇрѕИрѕИрїЅрЅх рІ│рІГрѕгріГрЅ░рѕФ \" рІерІІрїІ ріЋрѕерЅхріЋ рІѕрІ░ рЅ│рЅй рѕѕрѕЏрІЇрѕерІх рїаріЋріФрѕФ рІерїѕріЋрІўрЅЦріЊ рІерЇірѕхріФрѕЇ рЇќрѕірѕ▓рІјрЅйрЇБ рІерібрі«ріќрѕџрІЇріЋ рІерѕЏрѕЮрѕерЅх ріарЅЁрѕЮ рѕЏрѕхрЇІрЅхрЇБ рІерІѕрїф ріЋрїЇрІхріЊ рІерІЇрїГ рѕЮріЋрІЏрѕф рїѕрЅбріЋ рѕЏрѕ│рІ░рїЇ ріЦріЊ рІерїЇрѕЅ рѕ┤ріГрЅ░рѕГріЋ рѕЏрЅЦрЅЃрЅх рІГрїарІГрЅЃрѕЇ \" рЅЦрѕѕрІІрѕЇрЇб\n",
      "\n",
      "рѕїрѕІрІЇ рІФріљрѕ▒рЅх рїЅрІ│рІГ рЅаG20 рІерїІрѕФ рѕЏрІЋрЅђрЇЇ рібрЅхрІ«рїхрІФ ріЦрІФріФрѕёрІ░рЅй рІФрѕѕрЅйрІЇріЋ рІерІЋрІ│ рѕўрѕЇрѕХ рѕЏрІ░рѕФрїђрЅх рІхрѕГрІхрѕГ рЅарЅ░рѕўрѕѕріерЅ░ ріљрІЇрЇб\n",
      "\n",
      "рїєрѕГрїѓрІгрЅФ рЇц \" рІерІЋрІ│ рѕўрѕЇрѕХ рѕЏрІІрЅђрѕГ рѕѓрІ░рЅх рІерѕўрїерѕерѕ╗ рІ░рѕерїЃ рѕІрІГ рІГрїѕріЏрѕЇ рЇц ріерібрЅхрІ«рїхрІФ ріарЅарІ│рѕфрІјрЅй рїІрѕГ рЅБрѕѕріЮ рїЇріЋріЎріљрЅх рІГрѕЁ рЅЁрІхрѕџрІФ рІерѕџрѕ░рїарІЇ рїЅрІ│рІГ ріљрІЇ \" рѕ▓рѕЅ рїѕрѕЇрїИрІІрѕЇрЇб \n",
      "\n",
      "рІеIMF рЇЋрѕ«рїЇрѕФрѕЮ ріаріФрѕЇ рѕєріљрІЇріЋ рІерЅ│ріГрѕх ріЦрѕГрѕЮрїЃрІјрЅйріЋ рЅарЅ░рѕўрѕѕріерЅ░рѕЮ рЇц рІерібрЅхрІ«рїхрІФ рЅБрѕѕрѕхрѕЇрїБріЊрЅх рѕѕрЅЦрѕёрѕФрІі рЅарїђрЅ▒ рІхрїІрЇЇ рѕѕрѕЏрІхрѕерїЇ рІѕрѕ│ріЮ рІерѕєріЉ рІерЅ│ріГрѕх ріарЅЁрѕърЅйріЋ рѕўрѕѕрІерЅ│рЅИрІЇріЋ рїарЅЂрѕўрІІрѕЇрЇб \n",
      "\n",
      "рїєрѕГрїѓрІгрЅФ рЇЦ рІерібрЅхрІ«рїхрІФ ріарїарЅЃрѕІрІГ рІерѕђрїѕрѕГ рІЇрѕхрїЦ рѕЮрѕГрЅх рІЋрІхрїѕрЅх ріеIMF рІерѕўрїђрѕўрѕфрІФ рЅхріЋрЅарІФрІјрЅй рѕўрЅЦрѕѕрїАріЋ рѕЏрЅЦрѕФрѕФрЅ│рЅИрІЇріЋ рІўрѕфрЇќрѕГрЅ░рѕГ ріарѕхріљрЅЦрЅДрѕЇрЇб\n",
      "\n",
      "рІерѕЏріћрїѓріЋрїЇ рІ│рІГрѕгріГрЅ░рѕФ ріЋрїЇрїЇрѕГ рЅ░ріерЅхрѕј \" рѕўрѕгрЅх рѕІрІГ ріФрѕѕрІЇ ріЦрІЇріљрЅ│ рїІрѕГ рІерѕџрїѕріЊріЮ ріарІГрІ░рѕѕрѕЮ \" рІерѕџрѕЅ ріарѕхрЅ░рІФрІерЅХрЅй рѕ▓рѕ░рїАрѕЮ рЅ░рѕўрѕЇріГрЅ░ріЊрѕЇрЇб\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "рѕЇрЅ│рѕхрѕўрѕГрЅђрІЅ ріерЅцрЅ░рѕ░рЅдрЅ┐ рЅ░рІ░рЅЦрЅЃ рІерѕўрїБрЅй ріЦрї«ріЏрІЅріЋ рїГріФріћ рЅарЅ░рѕърѕІрЅарЅх рѕЂріћрЅ│ рІерїѕрІ░рѕѕрІЅ рїЇрѕѕрѕ░рЅЦ рЅа20 рІЊрѕўрЅх рїйріЉ ріЦрѕхрѕФрЅх рЅ░рЅђрїБрЇб\n",
      "\n",
      "рЅарІ░рЅАрЅЦ рібрЅхрІ«рїхрІФ ріГрѕЇрѕЇ рїІрѕъ рІъріЋ ріарѕГрЅБрѕЮріЋрїГ ріерЅ░рѕЏ ріарѕхрЅ░рІ│рІ░рѕГ рїЅрѕГрЅБ рЅарѕџрЅБрѕЇ рЅђрЅарѕї рІерїѕрІЏ ріЦрї«ріЏрІЅріЋ рїГріФріћ рЅарЅ░рѕърѕІрЅарЅх рѕўрѕЇріЕ рЅаріарѕ░рЅЃрЅѓ рѕЂріћрЅ│ рЅарѕхрѕѕрЅх ріаріЋрїѕрЅиріЋ рЅарѕўрЅЂрѕерїЦ рѕЋрІГрІѕрЅи ріЦріЋрІ▓рІФрѕЇрЇЇ рІФрІ░рѕерїѕрІЅ рІѕрїБрЅх рЅарЇЁріЉ ріЦрѕхрѕФрЅх рѕўрЅђрїБрЅ▒ріЋ рІеріарѕГрЅБ рѕЮріЋрїГ ріерЅ░рѕЏ ріарѕхрЅ░рІ│рІ░рѕГ рЇќрѕірѕх рѕўрѕЮрѕфрІФ ріарІЏрІЦ рѕЮ/рібріЋрѕхрЇћріГрЅ░рѕГ рїІрЇІрѕ« рЅХрѕЏрѕх рѕѕрЅ▓ріГрЅФрѕЁ рібрЅхрІ«рїхрІФ рЅ░ріЊрїЇрѕерІІрѕЇрЇб\n",
      "\n",
      "рІерІѕріЋрїђрѕЇ рІхрѕГрїірЅ▒ рІерЅ░рЇѕрЇђрѕўрІЅ ріљрѕљрѕ┤ 16/2016 рІЊ/рѕЮ рЅаріарѕГрЅБрѕЮріЋрїГ ріерЅ░рѕЏ рїЅрѕГрЅБ рЅђрЅарѕї ріљрІЇрЇб\n",
      "\n",
      "рЅ░ріерѕ│рѕй рІ«ріЊрѕх рїФрЇірЅё рІерЅ░рЅБрѕѕрІЇ рїЇрѕѕрѕ░рЅЦ рІерїѓріЋріФ рІЕріњрЅерѕГрѕ▓рЅ▓ 1ріЏ┬а рІЊрѕўрЅх рЅ░рѕЏрѕф рІерѕєріљрЅйрІЅріЋ рѕЪрЅй рѕірІ▓рІФ рІ«рѕљріЋрѕх ріЦрѕ▒ріЋ рѕѕрѕЏрѕхрѕўрѕерЅЁ рІѕрІ░ ріарѕГрЅБ рѕЮріЋрїГ ріерЅ░рѕЏ рЅарѕўрїБрЅйрЅарЅх рЅ░ріерѕФрІГрЅХ рЅарѕџріќрѕГрЅарЅх рЅцрЅх ріарѕ░рЅЃрЅѓ рІхрѕГрїірЅ▒ріЋ рѕўрЇѕрЇђрѕЎріЋ рІерѕЮрѕГрѕўрѕФ рѕўрІЮрїѕрЅА рІФрѕхрѕерІ│рѕЇрЇб\n",
      "\n",
      "рІѕрїБрЅи \" ріЦрї«ріЏрІг рІГрѕўрѕерЅЁрѕЇріЏрѕЇ \" рЅарѕџрѕЇ рІ░рѕхрЅ│ ріерЅцрЅ░рѕ░рЅдрЅ┐ рЅ░рІ░рЅЦрЅЃ рЅ░ріерѕ│рѕй рЅ░ріерѕФрІГрЅХ рІѕрІ░ рѕџрѕЏрѕГрЅарЅх┬а рЅцрЅх рѕўрїЦрЅ│ рЅарІІрІюрѕЏрІЅ рѕѕрѕЮрѕерЅЃрІЅ рІерѕџрѕєріЉ рІерІ▓рі«рѕГрЇБ рІерІ│рЅдріЊ рѕѕрѕхрѕІрѕ│ рѕўрїарїдрЅйріЊ рЅарЅАріЊ рІЮрїЇрїЁрЅх рЅцрЅ▒ріЋ ріарѕ░рѕЏрѕЮрѕФ рЅарѕЮрѕйрЅ▒рѕЮ рїЇрЅб рІЅрѕхрїЦ рІФрѕЅ рЅ░ріерѕФрІ«рЅйріЋ рїарѕГрЅ░рІЅ ріерѕИріЎ рЅаріІрѕІ рѕЪрЅй рѕђрїѕрѕГ рѕ░рѕІрѕЮ рЅЦрѕІ рЅарЅ░ріЏрЅйрЅарЅх ріерѕїрѕірЅ▒ 7 рѕ░рІЊрЅх рїѕрІ░рѕЏ ріЦрѕФрѕиріЋ рѕўріерѕІріерѕЇ рЅарѕЏрЅхрЅйрѕЇрЅарЅх рѕЂріћрЅ│ рЅарЅбрѕІрІІ ріаріЋрїѕрЅиріЋ ріарѕГрІХ рѕўрїЇрІ░рѕЅріЋ рІерѕЮрѕГрѕўрѕФ рѕўрІЮрїѕрЅАріЋ рІІрЅб ріарІхрѕГрїѕрІЇ рЇќрѕірѕх ріарІЏрІА рїѕрѕЇрЇђрІІрѕЇрЇб\n",
      "\n",
      "рЇќрѕірѕх ріарІЏрІА ріаріГрѕѕрІЇ ріЦріЋрІ░рїѕрѕѕрї╣рЅх рЇЦ рЅарІѕрЅЁрЅ▒ рЅарЅ░рІ░рѕерїѕрІЅ рѕЏрїБрѕФрЅхрѕЮ рѕєріљ рЅаріГрѕх рѕўрІЮрїѕрЅА рѕІрІГ ріЦріЋрІ░рѕ░рЇѕрѕерІЅ рІѕріЋрїђрѕѕріЏрІЇ  \" рІѕрІ░ рІЕріњрЅерѕГрѕ▓рЅ▓ рЅарѕёрІхрѕйрЅарЅх рѕїрѕІ рІерІѕріЋрІх рїЊрІ░ріЏ рІГрІўрѕ╗рѕЇ \" рЅарѕџрѕЇ ріљрІЇ рЅарѕГ рІўрїЇрЅХ ріарѕ░рЅЃрЅѓ рІерІѕріЋрїђрѕЇ рІхрѕГрїірЅ▒ріЋ рІерЇѕрїИрѕўрІЇрЇб\n",
      "\n",
      "рЇќрѕірѕх рЅарІџрѕЁ рІўрїЇріЊріЮ рІѕріЋрїђрѕЇ рІЎрѕфрІФ рЅ░рїѕрЅбрІЅріЋ рѕЏрїБрѕФрЅхріЊ рѕЮрѕГрѕўрѕФ ріарІхрѕГрїј рѕѕрІљрЅЃрЅц рѕЋрїЇ рѕЏрЅЁрѕерЅАріЋ ріарѕхрЅ│рІЇрЅђрІІрѕЇрЇб\n",
      "\n",
      "рІљрЅЃрЅц рѕЋрїЇрѕЮ ріГрѕх рЅарѕўрѕўрѕхрѕерЅх рѕѕрЇЇрѕГрІх рЅцрЅ▒ рЅ░ріерѕ│рѕй рІерІѕріЋрїђрѕЇ рІхрѕГрїірЅ▒ріЋ рѕўрЇѕрЇђрѕЎріЋ рІерѕ░рІЅрЇБ рІерѕ░ріљрІхріЊ рІерѕЁріГрѕЮріЊ рѕЏрѕхрѕерїЃрІјрЅйріЋ рЅарѕЏрЅЁрѕерЅЦ ріарѕхрѕерІхрЅирѕЇрЇб\n",
      "\n",
      "рЅарЅђрѕерЅА рѕЏрѕхрѕерїЃрІјрЅй ріЦріЊ рѕЮрѕхріГрѕ«рЅй рїЇрѕФ рЅђріЎріЋ рѕ▓рІФрїБрѕФ рІерЅєрІерІЅ рІерїІрѕъ рІъріЋ ріерЇЇрЅ░ріЏрІЅ рЇЇрѕГрІх рЅцрЅх рЅарЅђріЋ 29/5/2017 рІЊрѕЮ рЅарІІрѕѕрІЅ рЅйрѕјрЅх рЅ░ріерѕ│рѕй рІ«ріЊрѕх рїерЇірЅё рЅарЅ░ріерѕ░рѕ░рЅарЅх рЅаріарѕ░рЅЃрЅѓ рѕЂріћрЅ│ ріљрЅЦрѕх рІерѕЏрїЦрЇІрЅх рІѕріЋрїђрѕЇ рїЦрЇІрЅ░ріЏ рѕўрѕєріЉріЋ рЅарѕЏрѕерїІрїѕрїЦ рЅа20 рІЊрѕўрЅх рЇЁріЉ ріЦрѕхрѕФрЅх ріЦріЋрІ▓рЅђрїБ рѕўрІѕрѕ░ріЉріЋрѕЮ рібріЋрѕхрЇћріГрЅ░рѕГ рїІрЇІрѕ« рЅХрѕЏрѕх рѕѕрЅ▓ріГрЅФрѕЁ рібрЅхрІ«рїхрІФ рЅ░ріЊрїЇрѕерІІрѕЇрЇб\n",
      "\n",
      "#TikvahEthiopiaFamilyHW\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "#рѕўрЅёрІХріЋрІФ\n",
      "\n",
      "\" рѕ░рІЇріЋ рѕѕрѕўрѕГрІ│рЅх рѕ░рІЇ рѕўрѕєріЋ рЅарЅѓ ріљрІЇ !! \"\n",
      "\n",
      "рѕўрЅёрІХріЋрІФ рІеріарѕерїІрІірІФріЋріЊ рІеріаріЦрѕЮрѕ« рѕЁрѕЎрѕЏріЋ рѕўрѕГрїЃ рѕЏрІЋріерѕЇ рЅарѕџрІФрѕхрїѕріљрЅБрІЇ рѕєрѕхрЇњрЅ│рѕЇ рїГрѕЮрѕГ рІФрѕѕрІЇ рѕЁріЋрЇЃ рѕѕрѕЏрїаріЊрЅђрЅЁ рІерїѕріЋрІўрЅЦ ріЦрїЦрѕерЅх рѕхрѕѕрїѕрїарѕўрІЇ ріЦрїЇрІЏ ріЦріЋрІ▓рІ░рѕерїЇ рѕѕрѕўрѕІрІЇ рібрЅхрІ«рїхрІФрІЇрІФріЋ рїЦрѕф рѕўрЅЁрѕерЅА рІГрЅ│рІѕрѕ│рѕЇрЇб\n",
      "\n",
      "рІЏрѕг Рђю рЅарѕ░рІГрЇЅ рібрЅбріцрѕх рІерІЕрЅ▒рЅЦ рЅ╗ріљрѕЇ РђЮ рІхрїІрЇЇ рѕЏрІхрѕерїірІФ рѕўрѕГрѕђ рїЇрЅЦрѕГ ріЦрІерЅ░ріФрѕёрІ░ ріљрІЇрЇб\n",
      "\n",
      "рІ░рїІрїјрЅй рѕЂрѕЅ рІхрїІрЇЇ ріЦріЋрІхрЅ│рІ░рѕГрїЅ рїЦрѕф ріЦріЊрЅђрѕГрЅБрѕѕріЋрЇб\n",
      "\n",
      "рІерІхрїІрЇЇ рѕЏрІхрѕерїірІФ ріарѕЏрѕФрї«рЅй ріерѕІрІГ рЅ░рІФрІГрІўрІІрѕЇрЇб\n",
      "\n",
      "рѕўрЅёрІХріЋрІФ Рђю рѕЁріЋрЇЃрІЇ рѕѕрѕЏрїаріЊрЅђрЅЁ рїѕріЋрІўрЅЦ рЅ░рЅИрїЇрѕеріЊрѕЇрЇб рѕѕрѕЏрїаріЊрЅђрЅЁ рІѕрІ░ 5 рЅбрѕірІ«ріЋ рЅЦрѕГ рІФрѕхрЇѕрѕЇрїѕріЊрѕЇ РђЮ рѕЏрѕѕрЅ▒ ріарІГрІўріљрїІрѕЮрЇб\n",
      "\n",
      "рЅарЅђрїЦрЅ│ рІГріерЅ│рЅ░рѕЅ ­ЪЉЄ\n",
      "https://www.youtube.com/live/q0bMjwt9PvM?feature=shared\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=load_data(\"datas/totaldata.json\")\n",
    "number_of_contents=len(data)\n",
    "print(f'Total number of contents: {number_of_contents}\\n')\n",
    "print(f'First 5 contents: \\n')\n",
    "for news in data[:5]:\n",
    "    print(news)\n",
    "    print(\"---------------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean our data using clean_text function and sample our data to see the difference between the original and cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data=[clean_text(content) for content in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the total number of words and total number of unique words in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the dataset: 15290227\n",
      "\n",
      "Total number of unique words in the dataset: 832978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total=0\n",
    "for news in cleaned_data:\n",
    "    news_array=news.split()\n",
    "    total+=len(news_array)\n",
    "print(f'Total number of words in the dataset: {total}\\n')\n",
    "alldata=\" \".join(cleaned_data)\n",
    "counter=Counter(alldata.split())\n",
    "print(f'Total number of unique words in the dataset: {len(counter)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at index 37430 before cleaning: \n",
      "\n",
      " updateРгє№ИЈрІерЅ┤рЇњ ріерЅ░рѕЏ рІерІЏрѕг рїЦрІІрЅх рІхрЅБрЅЦ ріЦріЊ ріарїарЅЃрѕІрІГ рІеріерЅ░рѕЏрІІ рѕЂріћрЅ│ ріеріерЅ░рѕЏрІЇ ріљрІІрѕф ріарѕЮрІ░рЅарЅхрЇб\n",
      "@tseabwolde @tikvahethiopia\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Data at index 37430 after cleaning: \n",
      "\n",
      " рІерЅ┤рЇњ ріерЅ░рѕЏ рІерІЏрѕг рїЦрІІрЅх рІхрЅБрЅЦ ріЦріЊ ріарїарЅЃрѕІрІГ рІеріерЅ░рѕЏрІІ рѕЂріћрЅ│ ріеріерЅ░рѕЏрІЇ ріљрІІрѕф ріарѕЮрІ░рЅарЅхрЇб\n"
     ]
    }
   ],
   "source": [
    "index=37430\n",
    "print(f\"Data at index {index} before cleaning: \\n\\n\",data[index])\n",
    "print(\"\\n---------------------------------------------------------------------------------\\n\\n\")\n",
    "print(f\"Data at index {index} after cleaning: \\n\\n\",cleaned_data[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, We Will Train the Tokenizer\n",
    "\n",
    "Tokenization is a critical step in natural language processing (NLP) as it converts raw text into smaller, meaningful units (tokens) that can be processed by machine learning models. Effective tokenization ensures that the model can understand and interpret the text accurately, which is essential for tasks like text classification, machine translation, and sentiment analysis.\n",
    "\n",
    "For this task, we will use the **SentencePiece tokenizer** instead of traditional word-based tokenization. The [SentencePieceTokenizer](https://www.tensorflow.org/text/api_docs/python/text/SentencepieceTokenizer) is a powerful tool that tokenizes text into **subword units**, which offers several advantages:\n",
    "\n",
    "1. **Handling Complex Word Structures**: SentencePiece breaks words into smaller subword units, making it effective for handling complex word structures and morphological variations, which are common in languages like Amharic.\n",
    "2. **Out-of-Vocabulary (OOV) Words**: By using subword tokenization, SentencePiece can handle out-of-vocabulary words more gracefully, as it can decompose them into known subword units.\n",
    "3. **Multilingual Support**: SentencePiece is language-agnostic, making it suitable for multilingual datasets. This is particularly useful for Amharic, as it can handle the repetition of common subwords and morphological patterns unique to the language.\n",
    "4. **Simplified Preprocessing**: SentencePiece works directly on raw text, eliminating the need for extensive preprocessing steps like word segmentation or stemming.\n",
    "5. **Seamless Integration**: It integrates seamlessly with popular machine learning frameworks like TensorFlow and PyTorch, ensuring consistent tokenization across training and inference pipelines.\n",
    "\n",
    "Given these benefits, SentencePiece is an ideal choice for tokenizing Amharic text, as it can effectively capture the language's unique characteristics while simplifying the overall preprocessing workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train sentencepiece tokenizer model first. in order to do that we need to save our cleaned data into a single corpus of text in .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"datas/cleaned_data.txt\", \"a\") as file:\n",
    "  #  for content in cleaned_data:\n",
    "   #     file.write(content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SentencePiece model...\n",
      "Training complete! Check 'amharic_bpe.model' and 'amharic_bpe.vocab'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: datas/cleaned_data.txt\n",
      "  input_format: \n",
      "  model_prefix: amharic_sp_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 100000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 8192\n",
      "  num_threads: 6\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  РЂЄ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: datas/cleaned_data.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (9644 > 8192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(391) LOG(INFO) Reserved chars are found. Skipped: РЋ»РќЁРЋ░РЋ▒РќћРќћРќћРќћРќћРќћРќћРЋ▓РЋ»Рў╝ РќЋРќЋРЋ▒РЋ▒РЋ▒РЋ▒РЋ▒РЋ▒РЋ▒РЋ▒РћЏРќѓРЋ▓РЋ▓ РЋ▒РќѓРќѓРќѓРќѓРќѓРќѓРЋ▒РЋ▒РћЈРќЋРЋІРќЈРЋ▓РЋ▓ РќћРќЈРќѓРћЌРћЊРќѓРќЋРќћРћЏРќѓРћЈРќћРќѓРќЋРќћ РќЋРќЋРЋІРќЈРќЋРЋІРќЈРќЈРќЋРћЈРќЈРќЋРЋІРќЈ| ріЦріЏ рІерѕЎрІх ріЦріЋ№┐й№┐й№┐йрІЮ рЅцрЅ░рѕ░рЅдрЅй рЅхріФрІю рІхріЋрІЏрІю рЇІрІЏрІю ріЉрІЏрІю рІЅрѕхрїБрЅйріЋ ріарІ░рѕѕрѕЮ рЇѕрЅ│ рѕЮрЅђріЏ ріЦріЋрІ▓рЅЃрїарѕЇ\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 191827 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 553 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=75721594\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.5055% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=222\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.995055\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 191827 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=37769524\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 768596 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 191827\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 784912\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 784912 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=355352 obj=13.161 num_tokens=1612991 num_tokens/piece=4.53914\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=303986 obj=11.552 num_tokens=1618339 num_tokens/piece=5.32373\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=227957 obj=11.5203 num_tokens=1690775 num_tokens/piece=7.41708\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=227779 obj=11.5045 num_tokens=1695108 num_tokens/piece=7.4419\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=170832 obj=11.5413 num_tokens=1794991 num_tokens/piece=10.5073\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=170822 obj=11.5417 num_tokens=1795487 num_tokens/piece=10.5109\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=128116 obj=11.5906 num_tokens=1899315 num_tokens/piece=14.825\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=128116 obj=11.5718 num_tokens=1899287 num_tokens/piece=14.8247\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=110000 obj=11.6223 num_tokens=1952244 num_tokens/piece=17.7477\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=110000 obj=11.6115 num_tokens=1952426 num_tokens/piece=17.7493\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: amharic_sp_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: amharic_sp_model.vocab\n"
     ]
    }
   ],
   "source": [
    "input_file=\"datas/cleaned_data.txt\"\n",
    "model_prefix=\"amharic_sp_model\"\n",
    "print(\"Training SentencePiece model...\")\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=input_file,\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=100000,  \n",
    "    model_type=\"unigram\",  \n",
    "    character_coverage=0.995, \n",
    "    num_threads=6,  \n",
    "    max_sentence_length=8192, \n",
    "    split_by_whitespace=True,\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    \n",
    ")\n",
    "print(\"Training complete! Check 'amharic_bpe.model' and 'amharic_bpe.vocab'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the sentencepeice tokenizer the next step is to load the trainied tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=spm.SentencePieceProcessor(model_file=\"amharic_sp_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code shows the process of tokenizing individual words from a given text, in this case, the first entry of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\t-->\tTokenization\n",
      "----------------------------------------\n",
      "рЅаріарѕЏрѕФ    \t-->\t[503]\n",
      "ріГрѕЇрѕЇ     \t-->\t[41]\n",
      "рѕўрІ▓ріЊ     \t-->\t[2571]\n",
      "рЅарЅБрѕЁрѕГ    \t-->\t[3954]\n",
      "рІ│рѕГ      \t-->\t[577]\n",
      "ріерЅ░рѕЏ     \t-->\t[25]\n",
      "рЅђрЅарѕї     \t-->\t[556]\n",
      "14      \t-->\t[589]\n",
      "рЅхрѕІріЋрЅх    \t-->\t[414]\n",
      "рѕўрїІрЅбрЅх    \t-->\t[915]\n",
      "29      \t-->\t[1161]\n",
      "рІерѕўрїЇрѕфрЅЦ   \t-->\t[40582, 39560]\n",
      "рѕ░рѕІрЅх     \t-->\t[22531]\n",
      "рѕ░рїЇрІ░рІЇ    \t-->\t[80378]\n",
      "рѕ▓рѕўрѕѕрѕ▒    \t-->\t[5520]\n",
      "рІеріљрЅарѕЕ    \t-->\t[296]\n",
      "ріарЅБрЅх     \t-->\t[1033]\n",
      "ріе3      \t-->\t[12, 391]\n",
      "рѕЇрїєрЅ╣     \t-->\t[8106]\n",
      "ріЦріЋрІ▓рѕЂрѕЮ   \t-->\t[56]\n",
      "ріаріЋрІх     \t-->\t[60]\n",
      "рїјрѕерЅцрЅ│рЅИрІЇріЋ \t-->\t[63799, 5]\n",
      "рїерѕЮрѕ«     \t-->\t[166]\n",
      "ріарїарЅЃрѕІрІГ   \t-->\t[462]\n",
      "5       \t-->\t[189]\n",
      "рѕ░рІјрЅй     \t-->\t[27]\n",
      "рЅарЅ░ріерЇѕрЅ░рЅБрЅИрІЇ\t-->\t[6, 31096]\n",
      "рІерїЦрІГрЅх    \t-->\t[9820]\n",
      "ріЦрѕЕрѕЮрЅ│    \t-->\t[22829]\n",
      "рѕўрїѕрІ░рѕІрЅИрІЇ  \t-->\t[4926]\n",
      "рЅ░ріљрїЇрѕ»рѕЇрЇб  \t-->\t[528]\n",
      "рЅхрѕІріЋрЅхріЊ   \t-->\t[9146]\n",
      "рѕЮрѕйрЅх     \t-->\t[368]\n",
      "рЅарїЇрЇЇ     \t-->\t[7180]\n",
      "рІерЅ░рїѕрІ░рѕЅрЅх  \t-->\t[6444]\n",
      "рЇЦ       \t-->\t[4, 1]\n",
      "ріарЅХ      \t-->\t[44]\n",
      "рѕЎрѕё      \t-->\t[51148]\n",
      "рЇБ       \t-->\t[33]\n",
      "рѕЇрїЃрЅИрІЇ    \t-->\t[4786]\n",
      "ріарЅарЅБрІЅ    \t-->\t[43, 124]\n",
      "рѕЎрѕё      \t-->\t[51148]\n",
      "рЇБ       \t-->\t[33]\n",
      "рѕйріЕрѕГ     \t-->\t[24929]\n",
      "рѕЎрѕё      \t-->\t[51148]\n",
      "рЇБ       \t-->\t[33]\n",
      "рѕЎрѕІрЅх     \t-->\t[9158]\n",
      "рѕЎрѕё      \t-->\t[51148]\n",
      "ріЦріЊ      \t-->\t[7]\n",
      "рїјрѕерЅцрЅ│рЅИрІЇ  \t-->\t[63799]\n",
      "ріарЅХ      \t-->\t[44]\n",
      "ріЦріЋрІхрѕфрѕх   \t-->\t[17857]\n",
      "рІерЅ░рЅБрѕЅ    \t-->\t[1376]\n",
      "рѕ▓рѕєріЉ     \t-->\t[947]\n",
      "рѕхрѕГрІЊрЅх    \t-->\t[769]\n",
      "рЅђрЅЦрѕФрЅИрІЇ   \t-->\t[13051]\n",
      "рЅарІЏрѕгрІЇ    \t-->\t[259]\n",
      "рІЋрѕѕрЅх     \t-->\t[207]\n",
      "рЅ░рЇѕрЇЁрѕЪрѕЇрЇб  \t-->\t[9115]\n",
      "ріЦрѕхріФрѕЂріЋ   \t-->\t[307]\n",
      "рїѕрІ│рІ«рЅй    \t-->\t[19551]\n",
      "рѕхрѕѕрѕўрІФрІЏрЅИрІЇ \t-->\t[45001]\n",
      "рІерЅ░рЅБрѕѕ    \t-->\t[801]\n",
      "ріљрїѕрѕГ     \t-->\t[67]\n",
      "рІерѕѕрѕЮрЇб    \t-->\t[984]\n",
      "рЅаріерЅ░рѕЏрІІ   \t-->\t[1197]\n",
      "ріерЅ░рїѕрІ░рѕЅрЅх  \t-->\t[10342]\n",
      "рѕ░рІјрЅй     \t-->\t[27]\n",
      "рЅБрѕ╗рїѕрѕГ    \t-->\t[2231]\n",
      "рЅБрѕЁрѕГрІ│рѕГ   \t-->\t[2982]\n",
      "ріерЅ░рѕЏ     \t-->\t[25]\n",
      "ріарЅБрІГ     \t-->\t[4770]\n",
      "рѕЏрІХ      \t-->\t[8609]\n",
      "рІерѕџрїѕріўрІЇ   \t-->\t[506]\n",
      "рѕўрѕхрїѓрІх    \t-->\t[8041]\n",
      "ріерЇЇрЅ░ріЏ    \t-->\t[53]\n",
      "рІерѕўрѕ│рѕфрІФ   \t-->\t[14257]\n",
      "рІхрЅЦрІ░рЅБ    \t-->\t[2667]\n",
      "ріЦріЋрІ░рЅ░рЇѕрЇђрѕўрЅарЅх\t-->\t[33102]\n",
      "рЅ░рїѕрѕЇрї┐рѕЇрЇб  \t-->\t[141]\n",
      "ріерІџрѕЂ     \t-->\t[5286]\n",
      "рїІрѕГ      \t-->\t[17]\n",
      "рЅарЅ░рІФрІФрІў   \t-->\t[526]\n",
      "рІЏрѕг      \t-->\t[57]\n",
      "рІерЅБрѕЁрѕГ    \t-->\t[1839]\n",
      "рІ│рѕГ      \t-->\t[577]\n",
      "рѕЎрѕхрѕірѕърЅй   \t-->\t[8870]\n",
      "рЅаріГрѕЇрѕЅ    \t-->\t[491]\n",
      "рЅарѕЎрѕхрѕірѕърЅй  \t-->\t[63342]\n",
      "рѕІрІГ      \t-->\t[10]\n",
      "ріаріљрїБрїЦрѕерІІрѕЇ \t-->\t[94902, 20007]\n",
      "рІФрѕЅрЅхріЋ    \t-->\t[2078]\n",
      "рїЇрІхрІФ     \t-->\t[1090]\n",
      "ріЦріЊ      \t-->\t[7]\n",
      "ріЦрїѕрЅ│     \t-->\t[5724]\n",
      "рЅарѕўрЅЃрІѕрѕЮ   \t-->\t[3574]\n",
      "рѕ░рѕЇрЇЇ     \t-->\t[822]\n",
      "рѕЏрІхрѕерїІрЅИрІЇріЋ \t-->\t[2988]\n",
      "\"       \t-->\t[14]\n",
      "рѕђрѕЕріЋ     \t-->\t[33660]\n",
      "рѕџрІ▓рІФ     \t-->\t[549]\n",
      "\"       \t-->\t[14]\n",
      "рІўрїЇрЅДрѕЇрЇб   \t-->\t[427]\n",
      "ріЦрѕхріФрѕЂріЋ   \t-->\t[307]\n",
      "рЅаріарѕЏрѕФ    \t-->\t[503]\n",
      "ріГрѕЇрѕЇ     \t-->\t[41]\n",
      "ріЦрѕхрѕЇрѕЮріЊ   \t-->\t[3278]\n",
      "рїЅрІ│рІ«рЅй    \t-->\t[231]\n",
      "ріерЇЇрЅ░ріЏ    \t-->\t[53]\n",
      "рѕЮріГрѕГ     \t-->\t[170]\n",
      "рЅцрЅхрѕЮ     \t-->\t[28, 8]\n",
      "рѕєріљ      \t-->\t[289]\n",
      "рЅарібрЅхрІ«рїхрІФ  \t-->\t[126]\n",
      "ріЦрѕхрѕЇрѕЮріЊ   \t-->\t[3278]\n",
      "рїЅрІ│рІ«рЅй    \t-->\t[231]\n",
      "рїарЅЁрѕІрІГ    \t-->\t[158]\n",
      "рѕЮріГрѕГ     \t-->\t[170]\n",
      "рЅцрЅх      \t-->\t[28]\n",
      "рІерЅ░рѕ░рїа    \t-->\t[2476]\n",
      "ріарѕхрЅ░рІФрІерЅх  \t-->\t[620]\n",
      "рІерѕѕрѕЮрЇб    \t-->\t[984]\n",
      "рЅ▓ріГрЅФрѕЁ    \t-->\t[1134]\n",
      "рібрЅхрІ«рїхрІФ   \t-->\t[54]\n",
      "рЅаріГрѕЇрѕЅ    \t-->\t[491]\n",
      "рЅ░рЇѕрЇЁрѕўрІІрѕЇ  \t-->\t[32369]\n",
      "рѕхрѕѕрЅ░рЅБрѕЅ   \t-->\t[62519]\n",
      "рїЇрІхрІФрІјрЅй   \t-->\t[9726]\n",
      "рЇБ       \t-->\t[33]\n",
      "рїЦрЅЃрЅХрЅй    \t-->\t[2225]\n",
      "рЇБ       \t-->\t[33]\n",
      "рІўрѕерЇІріЊ    \t-->\t[27038]\n",
      "ріЦрїѕрЅ│рІјрЅй   \t-->\t[46541]\n",
      "рІеріарѕЏрѕФ    \t-->\t[409]\n",
      "ріГрѕЇрѕЇ     \t-->\t[41]\n",
      "ріЦрѕхрѕЇрѕЮріЊ   \t-->\t[3278]\n",
      "рїЅрІ│рІ«рЅй    \t-->\t[231]\n",
      "ріерЇЇрЅ░ріЏ    \t-->\t[53]\n",
      "рѕЮріГрѕГ     \t-->\t[170]\n",
      "рЅцрЅх      \t-->\t[28]\n",
      "ріЦріЊ      \t-->\t[7]\n",
      "рІерѕџрѕўрѕѕріерЅ│рЅИрІЇ\t-->\t[2664]\n",
      "ріаріФрѕІрЅхріЋ   \t-->\t[2983]\n",
      "рѕѕрѕЏріљрїІрїѕрѕГ  \t-->\t[12139]\n",
      "рїЦрѕерЅх     \t-->\t[377]\n",
      "ріЦрІФрІ░рѕерїѕ   \t-->\t[998]\n",
      "рІГрїѕріЏрѕЇрЇц   \t-->\t[23033]\n",
      "рѕЮрѕІрѕй     \t-->\t[305]\n",
      "ріЦріЋрІ│рїѕріў   \t-->\t[14546]\n",
      "рЅ░рїерѕЏрѕф    \t-->\t[224]\n",
      "рѕўрѕерїЃрІјрЅйріЋ  \t-->\t[1140]\n",
      "рІФрЅђрѕГрЅБрѕЇрЇб  \t-->\t[10129]\n"
     ]
    }
   ],
   "source": [
    "# printing the encoding of each word to see how subwords are tokenized\n",
    "tokenized_text = [(list(tokenizer.tokenize(word)), word) for word in cleaned_data[3000].split()]\n",
    "\n",
    "print(\"Word\\t\\t-->\\tTokenization\")\n",
    "print(\"-\"*40)\n",
    "for element in tokenized_text:\n",
    "    print(f\"{element[1]:<8}\\t-->\\t{element[0]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take data from the cleaned_data  and see how the tokenization of the whole content looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at index 890 before tokenization: \" рѕ░рѕІрѕЮ ріеріаріЋрїѕрЅх рЅарѕІрІГріЊ рІЮрѕЮ рѕІрѕѕрѕЏрѕѕрЅх рІФрѕЁрѕЇ рІерѕЮріЋріЊрїѕрѕерІЇ рѕ│рІГрѕєріЋ рІІрїІ ріерЇЇрѕѕріЋ рІерѕЮріЊрѕўрїБрІЇ ріљрІЇ \" - рЅЁрІ▒рѕхріљрЅ│рЅИрІЇ рІЏрѕг рІерѕ░рѕІрѕЮ рѕџріњрѕхрЅ┤рѕГ ріаріЋрІх рІЊрѕѕрѕЮ ріарЅђрЇЇ рі«ріЋрЇѕрѕеріЋрѕх ріарІўрїІрїЁрЅХ ріљрЅарѕГрЇб рЅарІџрѕЁ рѕўрІхрѕеріГ рѕІрІГрѕЮ рІерѕ░рѕІрѕЮ рѕџріњрѕхрЅхрѕГ ріарЅХ рЅЦріЊрѕЇрЇЇ ріарІ▒рІЊрѕѕрѕЮ рЇБ рІерібрЅхрІ«рїхрІФ рІерѕЃрІГрѕЏріќрЅх рЅ░рЅІрѕЏрЅх рІерЅарѕІрІГ рїарЅБрЅѓ ріарЅБрЅХрЅйрЇБ рЅЦрЇЂрІЊріЋ ріарЅарІЇ рѕірЅЃріљ рї│рї│рѕ│рЅх рІѕріцрї▓рѕх рЅєрїХрѕ│рЅх рЇБ рІерѕўріЋрїЇрѕЦрЅх рЅБрѕѕрѕхрѕЇрїБріЊрЅх рЇБ ріарЅБрѕ│рІ░рѕ«рЅй рїГрѕЮрѕГ рЅ░рїѕріЮрЅ░рІЇ ріљрЅарѕГрЇб рЅарѕўрІхрѕеріЕ рЇц рЅЦрЇЂрІЋ рІѕрЅЁрІ▒рѕх ріарЅАріљ рѕЏрЅхрІФрѕх рЅђрІ│рѕЏрІі рЇЊрЅхрѕГрІФрѕГріГ рѕГріЦрѕ░ рѕірЅЃріљ рї│рї│рѕ│рЅх рІўрібрЅхрІ«рїхрІФ рѕірЅђ рї│рї│рѕх рІўріаріГрѕ▒рѕЮ рІѕріЦрїерїї рІўрѕўріЋрЅарѕе рЅ░ріГрѕѕрѕЃрІГрѕЏріќрЅх рѕўрѕЇрІЋріГрЅх ріарѕхрЅ░рѕІрѕЇрЇѕрІІрѕЇрЇб рЅЁрІ▒рѕхріљрЅ│рЅИрІЇ рѕЮріЋ ріарѕЅ ? (ріерѕўрѕЇрІЋріГрЅ│рЅИрІЇ рІерЅ░рІѕрѕ░рІ░) \" рѕ░рѕІрѕЮ рІерѕ░рІЇ рѕЇрїєрЅй рЇЇрѕІрїјрЅхрЇБ рІерЅЦрІЎ рѕЮріЋрІ▒рЅБріЋ рІерІерІЋрѕѕрЅх ріЊрЇЇрЅєрЅх ріљрІЇрЇб рІерЅарѕГріФрЅ│ рІўрѕўріЊрЅх рЅЁрѕГрѕХрЅйрЇц рїірІюрЇБ рїѕріЋрІўрЅЦ ріЦріЊ рІерѕ░рІЇ рїЅрѕЇрЅарЅх рІерЇѕрѕ░рѕ░рЅБрЅИрІЇ рїЇріЋрЅБрЅ│рІјрЅй рЅарѕ░рѕІрѕЮ рѕЏрїБрЅх рЅарЅЁрїйрЅарЅх рІГрЇѕрѕГрѕ│рѕЅрЇб рѕ░рѕІрѕЮ ріФрѕѕ рІерІЊрѕѕрѕЮ рѕђрЅЦрЅх рѕѕрѕЂрѕЅрѕЮ рЅарЅѓ ріљрІЇрЇб рѕ░рѕІрѕЮ рѕЏрїБрЅх рїЇріЋ рЅЦрІЎ рѕарѕФрІірЅхрЇБ рЅЦрІЎ рІерїдрѕГ рѕўрѕ│рѕфрІФ ріЦріЋрІ▓рІўрїІрїЁ ріЦрІФрІ░рѕерїѕ рѕђрЅЦрЅхріЋ рІФрІѕрІхрѕЏрѕЇрЇб рїдрѕГріљрЅх рѕЏрѕѕрЅх рѕђрЅЦрЅхріЊ рѕЋрІГрІѕрЅхріЋ рІѕрІ░рѕџріљрІх ріЦрѕ│рЅх рІЇрѕхрїЦ рѕўрїБрѕЇ ріљрІЇрЇб рІеріаріЋрІ░ріЏріЊ рІерѕЂрѕѕрЅ░ріЏ рІЊрѕѕрѕЮ рїдрѕГріљрЅхрЇБ рЅ│рѕфріГ рЅЦрЅ╗ рѕ│рІГрѕєріЋ рїарЅБрѕ│рІЇ ріарѕЂріЋрѕЮ рІерІЊрѕѕрѕЮріЋ рѕўрѕЇріГ ріарЅарѕІрѕйрЅХрЅ│рѕЇрЇб рѕ░рѕІрѕЮ рЅарІЇрѕхрїЦрІІ рїѕрѕФрѕЮріљрЅхрЇБ рЅхрІЋрїЇрѕЦрЅхрЇБ рЅ│рІЏрІЦріљрЅхріЊ рЅарЅхрѕЁрЅхріЊ рІЮрЅЁ рѕЏрѕѕрЅх рѕхрѕѕрѕџрїѕріЎ рѕўрѕФрѕФ рЅхрѕўрѕхрѕІрѕѕрЅйрЇц рЅарІЇрїцрЅи рїЇріЋ рѕђрїѕрѕГріЋ ріерїЦрЇІрЅхрЇБ рѕЋрІЮрЅЦріЋ ріерѕўріерѕФ рѕЏрЅхрѕерЇЇ рІерѕџрЅ╗рѕЇ рЅарѕўрѕєріЉ рІІрїІрІІ ріерЇЇ рІФрѕѕ ріљрІЇрЇб рЅЁрІхрѕхрЅх рЅцрЅ░ ріГрѕГрѕхрЅ▓рІФріЊрЅйріЋ рЇд ┬░ рѕ░рѕІрѕЮ рІерѕєріљрІЇ ріГрѕГрѕхрЅХрѕх рІерѕџрѕ░рЅаріГрЅБрЅхрЇБ ┬░ рІерѕ░рѕІрѕЮ рѕўрѕЇріЦріГрЅ░ріърЅй рЅарІЇрѕхрїЦрІІ рІерѕџрѕўрѕІрѕѕрѕ▒рЅБрЅхрЇБ ┬░ рЅарїЇрЅЦрѕе ріЃрїбріарЅх рІерІѕрІ░рЅЂрЅх рЅаріЋрѕхрѕЊ ріеріЦрїЇрІџріарЅЦрѕћрѕГ рїІрѕГ рІерѕџрЅ│рѕерЅЂрЅБрЅх рІерѕ░рѕІрѕЮ рІхрѕЇрІхрІГ рѕхрѕѕрѕєріљрЅй рЅарѕЦрѕГрІЊрЅ░ рЅЁрІ│рѕ┤рІІ рѕ░рѕІрѕЮріЋ рІ░рїІрїЇрѕЏ рЅ│рІЇрїЃрѕѕрЅйрЇц рЅарїИрѕјрЅи рѕѕрѕўрѕІрІЇ рІЊрѕѕрѕЮ рѕ░рѕІрѕЮріЋ рЅхрѕѕрѕЮріЊрѕѕрЅйрЇц рЅарїЅрѕЇрѕІрЅхрІІ рѕІрІГ рІерѕ░рЅђрѕѕрЅйрІЇ рѕўрѕхрЅђрѕЇрѕЮ рѕ░рѕІрѕЮріЋ рІерѕџрѕ░рЅЦріГ ріљрІЇрЇц рІерѕўрѕхрЅђрѕЅ рЅЁрѕГрЇЁ рІѕрІ░ рѕІрІГріЊ рІѕрІ░ рїјріЋ рѕўрѕєріЉрѕЮ ріеріЦрїЇрІџріарЅЦрѕћрѕГріЊ ріерѕ░рІЇ рїІрѕГ рѕ░рѕІрѕЮ рѕўрѕєріЋ ріЦріЋрІ│рѕѕрЅЦріЋ рІерѕџрІФрѕхрїѕріљрІЮрЅаріЋ ріљрІЇрЇб рЅ│рѕфріФрЅйріЋ ріЦріЋрІ░рѕџріљрїЇрѕеріЋ рІѕріЋрІхрѕЏрѕЏрЅЙрЅй рѕ▓рїІрІ░рѕЅрЇБ рЅарѕЋрІЮрЅЦ рѕўріФріерѕЇ рѕўрЅ░рѕІрѕѕрЅЁ рѕ▓рѕўрїБ рЅцрЅ░ ріГрѕГрѕхрЅ▓рІФріЋ рЅ│рЅдрЅх ріаріГрЅЦрѕФрЇБ рЅаріЦрѕ│рЅх рѕўріФріерѕЇ рїѕрЅЦрЅ│ рѕ░рѕІрѕЮріЋ рѕхрЅ│рІѕрѕГрІх рІеріќрѕерЅй ріЊрЅхрЇб рЅарѕђрїѕрѕГ рІЇрѕхрїЦ рїЇрїГрЅХрЅй рЅарЅ░рЇѕрїарѕЕрЅарЅх рїірІюрѕЮ рІерѕ░рѕІрѕЮ рїЦрѕфріЋ рІФрѕІрѕхрЅ░рѕІрѕѕрЇѕрЅйрЅарЅх рЅђріЋріЊ рѕ░рІЊрЅх ріарІГрїѕріЮрѕЮрЇАрЇА рѕ░рѕІрѕЮріЋ рІерѕўрІѕрІФрІФ рѕГріЦрѕх ріарІхрѕГрїѕріЋ рѕхріЋрѕ░рЅБрѕхрЅЦ рЅарїдрѕГріљрЅх рѕўріФріерѕЇ рІерЅ░рїеріљрЅЂ рѕЋрІЮрЅдрЅйрЇБ рѕФрѕ│рЅИрІЇріЋ рѕўріерѕІріерѕЇ рІерѕЏрІГрЅйрѕЅ ріЦріЊ рѕЮріЋ ріЦрІерЅ░рІ░рѕерїѕ ріЦріЋрІ│рѕѕ рЅарІЇрѕЇ рІерѕЏрІГрїѕріљрІўрЅА рІ░ріФрѕърЅй рЅ░рѕхрЇІ рІФрІ░рѕГрїЅріЊрѕЇрЇб рѕхрѕѕрІџрѕЁ рѕ░рѕІрѕЮ ріеріаріЋрїѕрЅх рЅарѕІрІГріЊ рІЮрѕЮ рѕІрѕѕрѕЏрѕѕрЅх рІФрѕЁрѕЇ рІерѕЮріЋріЊрїѕрѕерІЇ рѕ│рІГрѕєріЋ рІІрїІ ріерЇЇрѕѕріЋ рІерѕЮріЊрѕўрїБрІЇ рѕхрѕѕрѕєріљ рІГрѕЁ рїЅрЅБріц ріерІЇрІГрІГрЅх рЅБрѕ╗рїѕрѕГ рЅарЅ░рїЇрЅБрѕГ рїГрѕЮрѕГ рІерѕ░рѕІрѕЮ рЅ░рѕЮрѕ│рѕїрЅх рѕўрѕєріЋ ріЦріЋрІ░рѕџрїѕрЅБрІЇ рѕѕрѕЏрѕ│рѕ░рЅЦ ріЦріЋрІѕрІ│рѕѕріЋрЇб \" (рѕЎрѕЅ рѕўрѕЇрІЋріГрЅ│рЅИрІЇ ріерѕІрІГ рЅ░рІФрІГрІЪрѕЇ)\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Data at index 890 after tokenization:  [14, 302, 43245, 42, 9, 2331, 37977, 512, 470, 85965, 464, 253, 26895, 82905, 15, 14, 20, 13326, 57, 413, 98, 60, 260, 169, 4331, 6235, 150, 117, 820, 10, 8, 413, 125, 44, 20762, 41831, 7207, 33, 49, 2386, 175, 3828, 1077, 8591, 11239, 13264, 4076, 4, 1, 1914, 341, 640, 1, 78, 4405, 1, 1914, 33, 824, 750, 33, 1272, 741, 33116, 976, 1712, 150, 3598, 70, 2416, 3762, 1418, 4528, 4776, 3102, 3686, 4076, 4, 1, 1914, 8056, 1537, 4, 1, 78, 1420, 9140, 341, 1534, 9197, 337, 3875, 9931, 455, 3299, 13326, 107, 713, 135, 29, 330, 78447, 3799, 34, 14, 302, 550, 737, 461, 13, 7905, 4, 82566, 11, 57955, 23146, 35, 5899, 12034, 6409, 40, 38, 13, 242, 7, 550, 6417, 11, 26756, 277, 12721, 655, 5382, 6, 41824, 69299, 32, 302, 1205, 459, 1059, 2044, 722, 35, 302, 5382, 55, 228, 19167, 228, 472, 610, 18291, 998, 30048, 95209, 32, 334, 406, 17334, 25171, 16, 457, 26524, 1762, 23, 9930, 35, 5276, 9, 2199, 260, 334, 13, 458, 76, 464, 13472, 22, 382, 459, 5, 1552, 79770, 1910, 302, 6291, 353, 4, 12544, 8, 17829, 51736, 13, 4, 90352, 9, 12318, 2654, 406, 18296, 21698, 52377, 40, 6, 37647, 2381, 55, 162, 5, 17892, 13, 11472, 12, 36295, 6815, 13053, 317, 253, 353, 642, 182, 35, 3748, 586, 18878, 218, 4, 1, 302, 532, 5930, 20707, 204, 220, 36963, 4, 1, 413, 82240, 6291, 353, 51140, 36963, 4, 1, 26825, 4445, 1, 7663, 67691, 6, 5, 78, 1, 25914, 17, 352, 39806, 937, 413, 2805, 27495, 36749, 23030, 353, 4018, 36594, 15010, 2612, 36501, 6, 88354, 2902, 260, 4018, 4, 24, 6308, 9, 36501, 6, 94522, 353, 10, 11, 55032, 1431, 1064, 8, 4018, 74659, 1286, 77950, 10479, 16, 10, 9, 16, 1160, 3091, 25914, 9, 4908, 17, 302, 397, 12955, 81805, 204, 5, 35, 30394, 64, 73782, 10677, 210, 58238, 13, 5301, 97, 197, 81257, 4822, 586, 936, 17534, 66722, 13, 3045, 97, 12996, 4018, 3755, 9326, 44290, 3145, 1114, 23, 2523, 16629, 69, 38, 8, 413, 226, 5, 99213, 4793, 47, 9, 90, 21116, 84, 4018, 30556, 14552, 5314, 2957, 67967, 3232, 97, 75389, 4144, 13, 2334, 1113, 7594, 7, 107, 778, 856, 5061, 1593, 30894, 9460, 678, 17474, 5179, 1263, 302, 43245, 42, 9, 2331, 37977, 512, 470, 85965, 464, 253, 26895, 82905, 1149, 103, 552, 12, 41082, 2231, 4329, 976, 413, 10344, 397, 7504, 21800, 11085, 14, 29, 4545, 18864, 624, 1510, 1, 73, 34]\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Data at index 890 after detokenization: ['РќЂ\"', 'РќЂрѕ░рѕІрѕЮ', 'РќЂріеріаріЋрїѕрЅх', 'РќЂрЅарѕІрІГ', 'ріЊ', 'РќЂрІЮрѕЮ', 'РќЂрѕІрѕѕрѕЏ', 'рѕѕрЅх', 'РќЂрІФрѕЁрѕЇ', 'РќЂрІерѕЮріЋріЊрїѕрѕерІЇ', 'РќЂрѕ│рІГрѕєріЋ', 'РќЂрІІрїІ', 'РќЂріерЇЇрѕѕріЋ', 'РќЂрІерѕЮріЊрѕўрїБрІЇ', 'РќЂріљрІЇ', 'РќЂ\"', 'РќЂ-', 'РќЂрЅЁрІ▒рѕхріљрЅ│рЅИрІЇ', 'РќЂрІЏрѕг', 'РќЂрІерѕ░рѕІрѕЮ', 'РќЂрѕџріњрѕхрЅ┤рѕГ', 'РќЂріаріЋрІх', 'РќЂрІЊрѕѕрѕЮ', 'РќЂріарЅђрЇЇ', 'РќЂрі«ріЋрЇѕрѕеріЋрѕх', 'РќЂріарІўрїІрїЁрЅХ', 'РќЂріљрЅарѕГрЇб', 'РќЂрЅарІџрѕЁ', 'РќЂрѕўрІхрѕеріГ', 'РќЂрѕІрІГ', 'рѕЮ', 'РќЂрІерѕ░рѕІрѕЮ', 'РќЂрѕџріњрѕхрЅхрѕГ', 'РќЂріарЅХ', 'РќЂрЅЦріЊрѕЇрЇЇ', 'РќЂріарІ▒', 'рІЊрѕѕрѕЮ', 'РќЂрЇБ', 'РќЂрІерібрЅхрІ«рїхрІФ', 'РќЂрІерѕЃрІГрѕЏріќрЅх', 'РќЂрЅ░рЅІрѕЏрЅх', 'РќЂрІерЅарѕІрІГ', 'РќЂрїарЅБрЅѓ', 'РќЂріарЅБрЅХрЅйрЇБ', 'РќЂрЅЦрЇЂрІЊріЋ', 'РќЂріарЅарІЇ', 'РќЂрѕірЅЃріљ', 'РќЂ', 'рї│рї│', 'рѕ│рЅх', 'РќЂрІѕ', 'ріц', 'рї▓', 'рѕх', 'РќЂрЅє', 'рїХ', 'рѕ│рЅх', 'РќЂрЇБ', 'РќЂрІерѕўріЋрїЇрѕЦрЅх', 'РќЂрЅБрѕѕрѕхрѕЇрїБріЊрЅх', 'РќЂрЇБ', 'РќЂріарЅБ', 'рѕ│', 'рІ░рѕ«рЅй', 'РќЂрїГрѕЮрѕГ', 'РќЂрЅ░рїѕріЮрЅ░рІЇ', 'РќЂріљрЅарѕГрЇб', 'РќЂрЅарѕўрІхрѕеріЕ', 'РќЂрЇц', 'РќЂрЅЦрЇЂрІЋ', 'РќЂрІѕрЅЁрІ▒рѕх', 'РќЂріарЅАріљ', 'РќЂрѕЏрЅхрІФрѕх', 'РќЂрЅђрІ│рѕЏрІі', 'РќЂрЇЊрЅхрѕГрІФрѕГріГ', 'РќЂрѕГріЦрѕ░', 'РќЂрѕірЅЃріљ', 'РќЂ', 'рї│рї│', 'рѕ│рЅх', 'РќЂрІўрібрЅхрІ«рїхрІФ', 'РќЂрѕірЅђ', 'РќЂ', 'рї│рї│', 'рѕх', 'РќЂрІў', 'ріаріГрѕ▒рѕЮ', 'РќЂрІѕ', 'ріЦ', 'рїерїї', 'РќЂрІўрѕўріЋ', 'рЅарѕе', 'РќЂрЅ░ріГрѕѕрѕЃрІГрѕЏріќрЅх', 'РќЂрѕўрѕЇрІЋріГрЅх', 'РќЂріарѕхрЅ░рѕІрѕЇрЇѕрІІрѕЇрЇб', 'РќЂрЅЁрІ▒рѕхріљрЅ│рЅИрІЇ', 'РќЂрѕЮріЋ', 'РќЂріарѕЅ', 'РќЂ?', 'РќЂ(', 'ріе', 'рѕўрѕЇрІЋріГрЅ│рЅИрІЇ', 'РќЂрІерЅ░рІѕрѕ░рІ░', ')', 'РќЂ\"', 'РќЂрѕ░рѕІрѕЮ', 'РќЂрІерѕ░рІЇ', 'РќЂрѕЇрїєрЅй', 'РќЂрЇЇрѕІрїјрЅх', 'рЇБ', 'РќЂрІерЅЦрІЎ', 'РќЂ', 'рѕЮріЋрІ▒рЅБріЋ', 'РќЂрІе', 'рІерІЋрѕѕрЅх', 'РќЂріЊрЇЇрЅєрЅх', 'РќЂріљрІЇрЇб', 'РќЂрІерЅарѕГріФрЅ│', 'РќЂрІўрѕўріЊрЅх', 'РќЂрЅЁрѕГрѕХрЅй', 'рЇц', 'РќЂрїірІю', 'рЇБ', 'РќЂрїѕріЋрІўрЅЦ', 'РќЂріЦріЊ', 'РќЂрІерѕ░рІЇ', 'РќЂрїЅрѕЇрЅарЅх', 'РќЂрІе', 'рЇѕрѕ░рѕ░', 'рЅБрЅИрІЇ', 'РќЂрїЇріЋрЅБрЅ│рІјрЅй', 'РќЂрЅарѕ░рѕІрѕЮ', 'РќЂрѕЏрїБрЅх', 'РќЂрЅа', 'рЅЁрїйрЅарЅх', 'РќЂрІГрЇѕрѕГрѕ│рѕЅ', 'рЇб', 'РќЂрѕ░рѕІрѕЮ', 'РќЂріФрѕѕ', 'РќЂрІерІЊрѕѕрѕЮ', 'РќЂрѕђрЅЦрЅх', 'РќЂрѕѕрѕЂрѕЅрѕЮ', 'РќЂрЅарЅѓ', 'РќЂріљрІЇрЇб', 'РќЂрѕ░рѕІрѕЮ', 'РќЂрѕЏрїБрЅх', 'РќЂрїЇріЋ', 'РќЂрЅЦрІЎ', 'РќЂрѕарѕФрІірЅхрЇБ', 'РќЂрЅЦрІЎ', 'РќЂрІерїдрѕГ', 'РќЂрѕўрѕ│рѕфрІФ', 'РќЂріЦріЋрІ▓рІўрїІрїЁ', 'РќЂріЦрІФрІ░рѕерїѕ', 'РќЂрѕђрЅЦрЅхріЋ', 'РќЂрІФрІѕрІхрѕЏрѕЇ', 'рЇб', 'РќЂрїдрѕГріљрЅх', 'РќЂрѕЏрѕѕрЅх', 'РќЂрѕђрЅЦрЅхріЊ', 'РќЂрѕЋрІГрІѕрЅхріЋ', 'РќЂрІѕрІ░', 'рѕџ', 'ріљрІх', 'РќЂріЦрѕ│рЅх', 'РќЂрІЇрѕхрїЦ', 'РќЂрѕўрїБрѕЇ', 'РќЂріљрІЇрЇб', 'РќЂрІеріаріЋрІ░ріЏ', 'ріЊ', 'РќЂрІерѕЂрѕѕрЅ░ріЏ', 'РќЂрІЊрѕѕрѕЮ', 'РќЂрїдрѕГріљрЅх', 'рЇБ', 'РќЂрЅ│рѕфріГ', 'РќЂрЅЦрЅ╗', 'РќЂрѕ│рІГрѕєріЋ', 'РќЂрїарЅБрѕ│', 'рІЇ', 'РќЂріарѕЂріЋрѕЮ', 'РќЂрІерІЊрѕѕрѕЮ', 'ріЋ', 'РќЂрѕўрѕЇріГ', 'РќЂріарЅарѕІрѕйрЅХ', 'рЅ│рѕЇрЇб', 'РќЂрѕ░рѕІрѕЮ', 'РќЂрЅарІЇрѕхрїЦ', 'рІІ', 'РќЂ', 'рїѕрѕФ', 'рѕЮ', 'ріљрЅхрЇБ', 'РќЂрЅхрІЋрїЇрѕЦрЅх', 'рЇБ', 'РќЂ', 'рЅ│рІЏрІЦріљрЅх', 'ріЊ', 'РќЂрЅарЅхрѕЁрЅхріЊ', 'РќЂрІЮрЅЁ', 'РќЂрѕЏрѕѕрЅх', 'РќЂрѕхрѕѕрѕџрїѕріЎ', 'РќЂрѕўрѕФрѕФ', 'РќЂрЅхрѕўрѕхрѕІрѕѕрЅй', 'рЇц', 'РќЂрЅа', 'рІЇрїц', 'рЅи', 'РќЂрїЇріЋ', 'РќЂрѕђрїѕрѕГ', 'ріЋ', 'РќЂріерїЦрЇІрЅх', 'рЇБ', 'РќЂрѕЋрІЮрЅЦріЋ', 'РќЂріе', 'рѕўріерѕФ', 'РќЂрѕЏрЅхрѕерЇЇ', 'РќЂрІерѕџрЅ╗рѕЇ', 'РќЂрЅарѕўрѕєріЉ', 'РќЂрІІрїІ', 'рІІ', 'РќЂріерЇЇ', 'РќЂрІФрѕѕ', 'РќЂріљрІЇрЇб', 'РќЂрЅЁрІхрѕхрЅх', 'РќЂрЅцрЅ░', 'РќЂріГрѕГрѕхрЅ▓рІФріЊрЅйріЋ', 'РќЂрЇд', 'РќЂ', '┬░', 'РќЂрѕ░рѕІрѕЮ', 'РќЂрІерѕєріљрІЇ', 'РќЂріГрѕГрѕхрЅХрѕх', 'РќЂрІерѕџрѕ░', 'рЅа', 'ріГ', 'рЅБрЅхрЇБ', 'РќЂ', '┬░', 'РќЂрІерѕ░рѕІрѕЮ', 'РќЂрѕўрѕЇріЦріГрЅ░ріърЅй', 'РќЂрЅарІЇрѕхрїЦ', 'рІІ', 'РќЂрІерѕџрѕўрѕІрѕѕрѕ▒', 'рЅБрЅхрЇБ', 'РќЂ', '┬░', 'РќЂрЅарїЇрЅЦрѕе', 'РќЂріЃ', 'рїб', 'ріарЅх', 'РќЂрІерІѕрІ░рЅЂрЅх', 'РќЂрЅа', 'ріЋ', 'рѕх', 'рѕЊ', 'РќЂріеріЦрїЇрІџріарЅЦрѕћрѕГ', 'РќЂрїІрѕГ', 'РќЂрІерѕџ', 'рЅ│рѕерЅЂ', 'рЅБрЅх', 'РќЂрІерѕ░рѕІрѕЮ', 'РќЂрІхрѕЇрІхрІГ', 'РќЂрѕхрѕѕрѕєріљрЅй', 'РќЂрЅарѕЦрѕГрІЊрЅ░', 'РќЂрЅЁрІ│рѕ┤', 'рІІ', 'РќЂрѕ░рѕІрѕЮріЋ', 'РќЂрІ░рїІрїЇрѕЏ', 'РќЂрЅ│рІЇ', 'рїЃ', 'рѕѕрЅйрЇц', 'РќЂрЅа', 'рїИрѕјрЅи', 'РќЂрѕѕрѕўрѕІрІЇ', 'РќЂрІЊрѕѕрѕЮ', 'РќЂрѕ░рѕІрѕЮріЋ', 'РќЂ', 'рЅх', 'рѕѕрѕЮ', 'ріЊ', 'рѕѕрЅйрЇц', 'РќЂрЅа', 'рїЅрѕЇрѕІрЅх', 'рІІ', 'РќЂрѕІрІГ', 'РќЂрІе', 'рѕ░рЅђрѕѕ', 'рЅйрІЇ', 'РќЂрѕўрѕхрЅђрѕЇ', 'рѕЮ', 'РќЂрѕ░рѕІрѕЮріЋ', 'РќЂрІерѕџрѕ░рЅЦріГ', 'РќЂріљрІЇрЇц', 'РќЂрІерѕўрѕхрЅђрѕЅ', 'РќЂрЅЁрѕГрЇЁ', 'РќЂрІѕрІ░', 'РќЂрѕІрІГ', 'ріЊ', 'РќЂрІѕрІ░', 'РќЂрїјріЋ', 'РќЂрѕўрѕєріЉрѕЮ', 'РќЂріеріЦрїЇрІџріарЅЦрѕћрѕГ', 'ріЊ', 'РќЂріерѕ░рІЇ', 'РќЂрїІрѕГ', 'РќЂрѕ░рѕІрѕЮ', 'РќЂрѕўрѕєріЋ', 'РќЂріЦріЋрІ│рѕѕрЅЦріЋ', 'РќЂрІерѕџрІФрѕхрїѕріљрІЮ', 'рЅа', 'ріЋ', 'РќЂріљрІЇрЇб', 'РќЂрЅ│рѕфріФрЅйріЋ', 'РќЂріЦріЋрІ░', 'рѕџріљрїЇрѕеріЋ', 'РќЂрІѕріЋрІхрѕЏрѕЏрЅЙрЅй', 'РќЂрѕ▓', 'рїІрІ░рѕЅ', 'рЇБ', 'РќЂрЅарѕЋрІЮрЅЦ', 'РќЂрѕўріФріерѕЇ', 'РќЂрѕў', 'рЅ░рѕІрѕѕрЅЁ', 'РќЂрѕ▓рѕўрїБ', 'РќЂрЅцрЅ░', 'РќЂріГрѕГрѕхрЅ▓рІФріЋ', 'РќЂрЅ│рЅдрЅх', 'РќЂріаріГрЅЦрѕФ', 'рЇБ', 'РќЂрЅаріЦрѕ│рЅх', 'РќЂрѕўріФріерѕЇ', 'РќЂрїѕрЅЦрЅ│', 'РќЂрѕ░рѕІрѕЮріЋ', 'РќЂрѕхрЅ│', 'рІѕрѕГрІх', 'РќЂрІеріќрѕерЅй', 'РќЂріЊрЅхрЇб', 'РќЂрЅарѕђрїѕрѕГ', 'РќЂрІЇрѕхрїЦ', 'РќЂрїЇрїГрЅХрЅй', 'РќЂрЅарЅ░рЇѕрїарѕЕ', 'рЅарЅх', 'РќЂрїірІю', 'рѕЮ', 'РќЂрІерѕ░рѕІрѕЮ', 'РќЂрїЦрѕф', 'ріЋ', 'РќЂрІФрѕІрѕхрЅ░рѕІрѕѕрЇѕ', 'рЅйрЅарЅх', 'РќЂрЅђріЋ', 'ріЊ', 'РќЂрѕ░рІЊрЅх', 'РќЂріарІГрїѕріЮрѕЮ', 'рЇАрЇА', 'РќЂрѕ░рѕІрѕЮріЋ', 'РќЂрІерѕўрІѕрІФрІФ', 'РќЂрѕГріЦрѕх', 'РќЂріарІхрѕГрїѕріЋ', 'РќЂрѕхріЋ', 'рѕ░рЅБрѕхрЅЦ', 'РќЂрЅарїдрѕГріљрЅх', 'РќЂрѕўріФріерѕЇ', 'РќЂрІерЅ░рїеріљрЅЂ', 'РќЂрѕЋрІЮрЅдрЅй', 'рЇБ', 'РќЂрѕФрѕ│рЅИрІЇріЋ', 'РќЂрѕўріерѕІріерѕЇ', 'РќЂрІерѕЏрІГрЅйрѕЅ', 'РќЂріЦріЊ', 'РќЂрѕЮріЋ', 'РќЂріЦрІерЅ░рІ░рѕерїѕ', 'РќЂріЦріЋрІ│рѕѕ', 'РќЂрЅарІЇрѕЇ', 'РќЂрІерѕЏрІГ', 'рїѕріљрІўрЅА', 'РќЂрІ░ріФрѕърЅй', 'РќЂрЅ░рѕхрЇІ', 'РќЂрІФрІ░рѕГрїЅ', 'ріЊрѕЇрЇб', 'РќЂрѕхрѕѕрІџрѕЁ', 'РќЂрѕ░рѕІрѕЮ', 'РќЂріеріаріЋрїѕрЅх', 'РќЂрЅарѕІрІГ', 'ріЊ', 'РќЂрІЮрѕЮ', 'РќЂрѕІрѕѕрѕЏ', 'рѕѕрЅх', 'РќЂрІФрѕЁрѕЇ', 'РќЂрІерѕЮріЋріЊрїѕрѕерІЇ', 'РќЂрѕ│рІГрѕєріЋ', 'РќЂрІІрїІ', 'РќЂріерЇЇрѕѕріЋ', 'РќЂрІерѕЮріЊрѕўрїБрІЇ', 'РќЂрѕхрѕѕрѕєріљ', 'РќЂрІГрѕЁ', 'РќЂрїЅрЅБріц', 'РќЂріе', 'рІЇрІГрІГрЅх', 'РќЂрЅБрѕ╗рїѕрѕГ', 'РќЂрЅарЅ░рїЇрЅБрѕГ', 'РќЂрїГрѕЮрѕГ', 'РќЂрІерѕ░рѕІрѕЮ', 'РќЂрЅ░рѕЮрѕ│рѕїрЅх', 'РќЂрѕўрѕєріЋ', 'РќЂріЦріЋрІ░рѕџрїѕрЅБрІЇ', 'РќЂрѕѕрѕЏрѕ│рѕ░рЅЦ', 'РќЂріЦріЋрІѕрІ│рѕѕріЋрЇб', 'РќЂ\"', 'РќЂ(', 'рѕЎрѕЅ', 'РќЂрѕўрѕЇрІЋріГрЅ│рЅИрІЇ', 'РќЂріерѕІрІГ', 'РќЂрЅ░рІФрІГ', 'рІЪ', 'рѕЇ', ')']\n"
     ]
    }
   ],
   "source": [
    "index=890\n",
    "print(f\"Data at index {index} before tokenization:\", cleaned_data[index])\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(f\"Data at index {index} after tokenization: \", tokenizer.encode(cleaned_data[index]))\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(f\"Data at index {index} after detokenization:\", tokenizer.encode_as_pieces(cleaned_data[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pretrain our Transformer network, we will use the Masked Language Model (MLM) approach. This technique involves randomly masking a percentage of words in a sentence and replacing them with special tokens. The model then attempts to predict these masked words, enabling it to learn contextual and semantic representations effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be implementing the Masked language model (MLM) as shown in the following image. \n",
    "\n",
    "<img src = \"images/losses.png\" width=\"600\" height = \"400\">\n",
    "\n",
    "Assume you have the following text: <span style = \"color:blue\"> **рѕ░рѕІрѕЮ <span style = \"color:red\">рІерѕ░рІЇ рѕЇрїєрЅй </span> рЇЇрѕІрїјрЅхрЇБ рІерЅЦрІЎ рѕЮріЋрІ▒рЅБріЋ рІерІерІЋрѕѕрЅх <span style = \"color:red\">ріЊрЇЇрЅєрЅх</span>  ріљрІЇрЇб** </span>     \n",
    "\n",
    "\n",
    "Now as input you will mask the words in red in the text: \n",
    "\n",
    "<span style = \"color:blue\"> **Input:**</span> рѕ░рѕІрѕЮ  **X** рЇЇрѕІрїјрЅхрЇБ рІерЅЦрІЎ рѕЮріЋрІ▒рЅБріЋ рІерІерІЋрѕѕрЅх **Y** ріљрІЇрЇб\n",
    "\n",
    "<span style = \"color:blue\">**Output:**</span> The model should predict the words(s) for **X** and **Y**. \n",
    "\n",
    "**[EOS]** will be used to mark the end of the target sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, I were able to take a piece of string and tokenize it. \n",
    "\n",
    "Now I will create `input` and `target` pairs that will allow me to pre-train the model. The model uses the ids at the end of the vocab file as sentinels. For example, it will replace: \n",
    "   - `vocab_size - 1` by `<Z>`\n",
    "   - `vocab_size - 2` by `<Y>`\n",
    "   - and so forth. \n",
    "   \n",
    "It assigns every word a `chr`.\n",
    "\n",
    "The `pretty_decode` function below, which I will use in a bit, helps in handling the type when decoding. \n",
    "\n",
    "Notice that:\n",
    "```python\n",
    "string.ascii_letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentinels(tokenizer, display=False):\n",
    "    sentinels = {}\n",
    "    vocab_size = tokenizer.vocab_size()\n",
    "    for i, char in enumerate(reversed(string.ascii_letters), 1):\n",
    "        decoded_text = tokenizer.detokenize([vocab_size - i])\n",
    "        \n",
    "        # Sentinels, ex: <Z> - <a>\n",
    "        sentinels[decoded_text] = f'<{char}>'    \n",
    "    \n",
    "        if display:\n",
    "            print(f'The sentinel is <{char}> and the decoded token is:', decoded_text)\n",
    "\n",
    "    return sentinels\n",
    "\n",
    "\n",
    "def pretty_decode(encoded_str_list, sentinels, tokenizer):\n",
    "    # If already a string, just do the replacements.\n",
    "    if isinstance(encoded_str_list, str):\n",
    "        for token, char in sentinels.items():\n",
    "            encoded_str_list = re.sub(re.escape(token), char, encoded_str_list)\n",
    "        return encoded_str_list\n",
    "  \n",
    "    # We need to decode and then prettyfy it.\n",
    "    return pretty_decode(tokenizer.detokenize(encoded_str_list), sentinels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentinel is <Z> and the decoded token is: рїѓрІ«рЇќрѕірЅ│ріЋ\n",
      "The sentinel is <Y> and the decoded token is: рі«рѕїрїЃрЅйріЋ\n",
      "The sentinel is <X> and the decoded token is: рЅЦрѕ«ріЋрІЮ\n",
      "The sentinel is <W> and the decoded token is: рі«ріЋрЅ▓ріћ\n",
      "The sentinel is <V> and the decoded token is: ріарЇѕрїЇрЇЇрїѕ\n",
      "The sentinel is <U> and the decoded token is: ріарІГрѕЮрѕ«ріарЅйріЋ\n",
      "The sentinel is <T> and the decoded token is: рІерЅ░рІѕрїарѕе\n",
      "The sentinel is <S> and the decoded token is: рІФріФрЅЦрЅ▒\n",
      "The sentinel is <R> and the decoded token is: рІГрЅарѕерЅ│рѕЇ\n",
      "The sentinel is <Q> and the decoded token is: рїѓріаріЋрїЇ\n",
      "The sentinel is <P> and the decoded token is: рІФрѕхрЅйрѕЅрЅ│рѕЇ\n",
      "The sentinel is <O> and the decoded token is: ,744\n",
      "The sentinel is <N> and the decoded token is: рІѕрїцрЅ│рѕЏ\n",
      "The sentinel is <M> and the decoded token is: ріЦрѕ░рїБрЅйріІрѕѕрѕЂ\n",
      "The sentinel is <L> and the decoded token is: рІФрѕ│рѕЮріљріЮ\n",
      "The sentinel is <K> and the decoded token is: рЅарЅхрѕЮрѕГрЅх\n",
      "The sentinel is <J> and the decoded token is: рІФрѕхрЇѕрѕЇрїІрЅйріІрѕЇ\n",
      "The sentinel is <I> and the decoded token is: ріарѕЇрѕєріљрѕЇріЮрѕЮ\n",
      "The sentinel is <H> and the decoded token is: рЅхрїарѕЇрЅЁ\n",
      "The sentinel is <G> and the decoded token is: рїарЇѕрїарЇЇ\n",
      "The sentinel is <F> and the decoded token is: ріарїйрѕЏрЅИрІЇ\n",
      "The sentinel is <E> and the decoded token is: рЅЦрЅхрІѕрІхрЅЁ\n",
      "The sentinel is <D> and the decoded token is: рѕЏрѕхрЇѕрЅ░ріЊрЅИрІЇ\n",
      "The sentinel is <C> and the decoded token is: рїІрѕерІ░рЅй\n",
      "The sentinel is <B> and the decoded token is: рі«ріЋрЅ▓ріљрѕГ\n",
      "The sentinel is <A> and the decoded token is: ріарѕхрЅ░ріЮрЅ│\n",
      "The sentinel is <z> and the decoded token is: ріЦріЋрІхрЅ│ріерЅЦрѕЕ\n",
      "The sentinel is <y> and the decoded token is: рѕЦрѕЇрїц\n",
      "The sentinel is <x> and the decoded token is: рІерѕѕрѕЙрЅй\n",
      "The sentinel is <w> and the decoded token is: рІерѕ░рЅарѕ░рЅарЅйрІЇ\n",
      "The sentinel is <v> and the decoded token is: рїЇрѕГрЇІрЅ▒\n",
      "The sentinel is <u> and the decoded token is: рЅђрѕІрЅЁрѕѕрІІрѕЇ\n",
      "The sentinel is <t> and the decoded token is: рѕўрѕѕрїарїЦ\n",
      "The sentinel is <s> and the decoded token is: ріарѕЇрЅђрѕГрѕЮ\n",
      "The sentinel is <r> and the decoded token is: рІЇрїЦріќрЅй\n",
      "The sentinel is <q> and the decoded token is: рЅарѕџрІФрѕхрігрІ░\n",
      "The sentinel is <p> and the decoded token is: ріЦрІерЅђрѕерЇѕ\n",
      "The sentinel is <o> and the decoded token is: рѕ░рЅарѕ░рЅДрЅИ\n",
      "The sentinel is <n> and the decoded token is: рІФріГрЅЦрѕГрѕЇріЋ\n",
      "The sentinel is <m> and the decoded token is: рІерІ░рЅарІ░рЅарІЇ\n",
      "The sentinel is <l> and the decoded token is: рІ░рЅБрѕѕрЅЂ\n",
      "The sentinel is <k> and the decoded token is: рѕўрЅЦрЅХрЅ╗рЅйріЋ\n",
      "The sentinel is <j> and the decoded token is: рЅхрѕЁрѕЮрѕГрЅх\n",
      "The sentinel is <i> and the decoded token is: ріФрѕЇрѕ░рЅа\n",
      "The sentinel is <h> and the decoded token is: рІ▓рЇЋрѕгрѕйріЋ\n",
      "The sentinel is <g> and the decoded token is: рЅ░рЅєрѕерѕ░\n",
      "The sentinel is <f> and the decoded token is: рЅарѕЏрїБрЅ┤\n",
      "The sentinel is <e> and the decoded token is: 60.88\n",
      "The sentinel is <d> and the decoded token is: рѕџрІФрІІрїАрЅх\n",
      "The sentinel is <c> and the decoded token is: рЇЅрѕѕрѕГ\n",
      "The sentinel is <b> and the decoded token is: рі«рѕфрІ░рѕЕ\n",
      "The sentinel is <a> and the decoded token is: ріЦрІерЅ░рЅђріљрѕ░\n"
     ]
    }
   ],
   "source": [
    "sentinels = get_sentinels(tokenizer, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the `pretty_decode` function in the following sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-5'></a>\n",
    "### 3.4 - Tokenizing and Masking\n",
    "\n",
    "In this task, I will implement the `tokenize_and_mask` function, which tokenizes and masks input words based on a given probability. The probability is controlled by the `noise` parameter, typically set to mask around `15%` of the words in the input text. The function will generate two lists of tokenized sequences following the algorithm outlined below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  tokenize_and_mask\n",
    "\n",
    "- Start with two empty lists: `inps` and `targs`\n",
    "- Tokenize the input text using the given tokenizer.\n",
    "- For each `token` in the tokenized sequence:\n",
    "  - Generate a random number(simulating a weighted coin toss)\n",
    "  - If the random value is greater than the given threshold(noise):\n",
    "    - Add the current token to the `inps` list\n",
    "  - Else:\n",
    "    - If a new sentinel must be included:\n",
    "      - Compute the next sentinel ID using a progression.\n",
    "      - Add a sentinel into the `inps` and `targs` to mark the position of the masked element.\n",
    "    - Add the current token to the `targs` list.\n",
    "\n",
    "** There's a special case to consider. If two or more consecutive tokens get masked during the process, no need to add a new sentinel to the sequences. To account for this, use the `prev_no_mask` flag, which starts as `True` but is turned to `False` each time I mask a new element. The code that adds sentinels will only be executed if, before masking the token, the flag was in the `True` state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_mask(text, \n",
    "                      noise=0.15, \n",
    "                      randomizer=np.random.uniform, \n",
    "                      tokenizer=None):\n",
    "    \"\"\"Tokenizes and masks a given input.\n",
    "\n",
    "    Args:\n",
    "        text (str or bytes): Text input.\n",
    "        noise (float, optional): Probability of masking a token. Defaults to 0.15.\n",
    "        randomizer (function, optional): Function that generates random values. Defaults to np.random.uniform.\n",
    "        tokenizer (function, optional): Tokenizer function. Defaults to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        inps, targs: Lists of integers associated to inputs and targets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Current sentinel number (starts at 0)\n",
    "    cur_sentinel_num = 0\n",
    "    \n",
    "    # Inputs and targets\n",
    "    inps, targs = [], []\n",
    "\n",
    "    # Vocab_size\n",
    "    vocab_size = int(tokenizer.vocab_size())\n",
    "    \n",
    "    # EOS token id \n",
    "    # Must be at the end of each target!\n",
    "    eos = tokenizer.piece_to_id(\"</s>\")\n",
    "    \n",
    "\n",
    "    \n",
    "    # prev_no_mask is True if the previous token was NOT masked, False otherwise\n",
    "    # set prev_no_mask to True\n",
    "    prev_no_mask = True\n",
    "    \n",
    "    # Loop over the tokenized text\n",
    "    for token in tokenizer.tokenize(text):\n",
    "        \n",
    "        # Generate a random value between 0 and 1\n",
    "        rnd_val = randomizer() \n",
    "        \n",
    "        # Check if the noise is greater than a random value (weighted coin flip)\n",
    "        if noise > rnd_val:\n",
    "            \n",
    "            # Check if previous token was NOT masked\n",
    "            if prev_no_mask:\n",
    "                \n",
    "                # Current sentinel increases by 1\n",
    "                cur_sentinel_num += 1\n",
    "                \n",
    "                # Compute end_id by subtracting current sentinel value out of the total vocabulary size\n",
    "                end_id = vocab_size - cur_sentinel_num\n",
    "                \n",
    "                # Append end_id at the end of the targets\n",
    "                targs.append(end_id)\n",
    "                \n",
    "                # Append end_id at the end of the inputs\n",
    "                inps.append(end_id)\n",
    "                \n",
    "            # Append token at the end of the targets\n",
    "            targs.append(token)\n",
    "            \n",
    "            # set prev_no_mask accordingly\n",
    "            prev_no_mask = False\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Append token at the end of the inputs\n",
    "            inps.append(token)\n",
    "            \n",
    "            # Set prev_no_mask accordingly\n",
    "            prev_no_mask = True\n",
    "    \n",
    "    \n",
    "    # Add EOS token to the end of the targets\n",
    "    targs.append(eos)\n",
    "    \n",
    "\n",
    "    \n",
    "    return inps, targs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now take random value from the cleaned_data and pass it to `tokenize_and_mask` function and see how it randomly masks and separate inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random data before Masking : \n",
      "\n",
      " рІерЇђрїЦрЅ│ рѕЂріћрЅ│рІЇ ріарѕхрЅ░рѕЏрѕЏріЮ ріљрІЇ~рїЁрїЇрїЁрїІ! . . рѕхрѕЮріЋрЅ░ріЏрІЇріЋ рІеріерЅ░рѕърЅй рЇјрѕерѕЮ рѕѕрѕЏріФрѕёрІх рІЮрїЇрїЁрЅ▒ рѕўрїаріЊрЅђрЅЂріЋріЊ рѕЮріЋрѕЮ ріарІГріљрЅх рІерЇђрїЦрЅ│ рЅйрїЇрѕГ ріарѕѕрѕўріќрѕЕріЋ рІеріерЅ░рѕЏ рѕЇрѕЏрЅхріЊ рі«ріЋрѕхрЅхрѕФріГрѕйріЋ рѕџріњрѕхрЅ┤рѕГ рѕџріњрѕхрЅхрѕГ рІ┤ріцрЅ│ ріарЅХ ріФрѕ│рѕЂріЋ рїјрЇї рЅ░ріЊрїѕрѕЕрЇб рІЮрїЇрїЁрЅ▒ріЋ ріарѕхрѕўрѕЇріГрЅХ рЅарїЁрїЇрїЁрїІ рѕўрїЇрѕѕрїФ рІерѕ░рїАрЅх рѕџріњрѕхрЅхрѕГ рІ┤ріцрЅ│рІЇ рїЁрїЁрїІ рѕІрІГ рІерЇђрїЦрЅ│ рѕЂріћрЅ│рІЇ ріарѕхрЅ░рѕЏрѕЏріЮ ріљрІЇ\"рЇц рІеріГрѕЇрѕЅ рЇќрѕірѕх ріерЇїрІ┤рѕФрѕЇ рІерЇђрїЦрЅ│ ріаріФрѕІрЅх рїІрѕГрѕЮ рЅарЅЁріЋрїЁрЅх ріЦрІерѕ░рѕФ ріљрІЇ рЅЦрѕѕрІІрѕЇрЇб рЅаріерЅ░рѕЏрІІ ріарѕхрЅ░рѕЏрѕЏріЮ рІерЇђрїЦрЅ│ рѕЂріћрЅ│ рѕўріќрѕЕріЋ рІерїѕрѕѕрЇЂрЅх рѕџріњрѕхрЅхрѕГ рІ┤ріцрЅ│рІЇ рЅ░рѕ│рЅ│рЇі ріерЅ░рѕърЅй рІѕрІ░рїЁрїЇрїЁрїІ рїѕрЅЦрЅ░рІЇ рІЮрїЇрїЁрЅ│рЅИрІЇріЋ рїђрѕЮрѕерІІрѕЇ рЅЦрѕѕрІІрѕЇрЇб рІЮрїЇрїЁрЅХрЅйріЋ рЅарЅ░рѕўрѕѕріерЅ░ ріеріљрїѕ рїђрѕЮрѕ« ріЦрѕхріе рѕерЅАрІЋ ріерѕџріќрѕЕ рІЮрїЇрїЁрЅХрЅй рѕўріФріерѕЇ рЅарїЁрїЇрїЁрїІ рѕхрЅ┤рІ▓рІерѕЮ рІерѕўріГрЇѕрЅ╗ рѕхріљрѕхрѕГрІљрЅхрЇБ рЅаріерЅ░рѕърЅй рІерѕхрѕФ рІЋрІхрѕЇ рЇѕрїарѕФ рібріЋрЅ░рѕГрЇЋрѕФрІГрІърЅй рѕЇрѕЏрЅхрЇБ рЅарѕўрѕгрЅх рѕЇрѕЏрЅх рѕЏріћрїЁрѕўріЋрЅхрЇБ рЅарІўрѕГрЇЅ ріарїђріЋрІ│рІјрЅй рѕЏрѕѕрЅхрѕЮ рЅаріерЅ░рѕЏ рѕЇрѕЏрЅхрЇБ ріарѕеріЋрїЊрІ┤ рѕЇрѕЏрЅхріЊ ріаріФрЅБрЅб рїЦрЅарЅЃрЇБ рЅарЅцрЅХрЅйріЊ рі«ріЋрѕхрЅхрѕФріГрѕйріЋ ріЦріЋрІ▓рѕЂрѕЮ рЅ░рІФрІФрІЦ рІўрѕГрЇјрЅй рІЎрѕфрІФ рїЦріЊрЅ│рІі рЇЁрѕЂрЇјрЅй рІГрЅђрѕГрЅБрѕЅрЇб ріарЅХ ріФрѕ│рѕЂріЋ ріерЅ░рѕърЅй рѕФрѕ│рЅИрІЇріЋ рІерѕџрІФрѕхрЅ░рІІрІЇрЅЂрЅарЅх ріцрїЇрІџрЅбрѕйріЋ рІерѕџріФрѕёрІх рѕ▓рѕєріЋ ріерІџрѕЁ рЅарЇірЅх рЅарЅ░рІ░рѕерїЅ рЇјрѕерѕърЅй рІерЅ░рѕхрЅ░рІІрѕѕрІЇ рІерІхрѕЮрЇЁ рЅЦріГрѕѕрЅх рЅарІџрѕЁ ріарѕўрЅх ріЦріЋрІ│рІГріќрѕГ ріеріерЅ░рѕърЅй рїІрѕГ рѕўрїЇрЅБрЅБрЅх рѕІрІГ рЅ░рІ░рѕГрѕирѕЇ рЅЦрѕѕрІІрѕЇрЇб рѕђрѕЎрѕх рЅарѕџріќрѕерІЇ рІерѕЏрїарЅЃрѕѕрІФ рѕхрѕГрІЊрЅх рѕѕрѕърІ┤рѕЇ рібріЋрЅ░рѕГрЇЋрѕФрІГрІърЅйрЇБ рѕѕрѕ┤рЅх рѕхрѕФ рЇѕрїБрѕфрІјрЅйрЇБ рЅарѕЂрѕЅрѕЮ ріГрѕЇрѕјрЅй ріФрѕЅ рІерѕ┤ріГрЅ░рѕГ рЅ░рЅІрѕЏрЅх рЅаріарЇѕрЇЃрЇђрѕЮ рЅЦрѕЇрїФ рѕІрїѕріЎ ріЦріЋрІ▓рѕЂрѕЮ рѕѕрІЕріњрЅерѕГрѕ▓рЅ▓ рЅ░рѕўрѕФрЅѓ рѕхрѕФ рЇѕрїБрѕфрІјрЅй ріЦрІЇрЅЁріЊріЊ рѕйрѕЇрѕЏрЅх рІГрѕ░рїБрѕЇ рЅ░рЅЦрѕЈрѕЇрЇб рЅарЅ░рѕўрѕ│рѕ│рІГ рІерІўрїаріљріЏрІЇ рІерібрЅхрІ«рїхрІФ ріерЅ░рѕърЅй рЇјрѕерѕЮ ріарІўрїІрїЁ рЅаріЦрѕѕрЅ▒ рІГрЇІ рІГрІ░рѕерїІрѕЇрЇц рІерІІріЋрїФ рѕГріГріГрЅЦрѕЮ рІГріќрѕФрѕЇ рЅЦрѕѕрІІрѕЇ рѕџріњрѕхрЅхрѕГ рІ┤ріцрЅ│рІЇрЇб рѕхрѕЮріЋрЅ░ріЏрІЇріЋ рІеріерЅ░рѕърЅй рЇјрѕерѕЮ рІерЅ░рѕѕрІе рѕѕрѕЏрІхрѕерїЇ ріерѕЏрѕїрІЦрІФ ріарѕѕрѕЮріарЅђрЇЇ рІеріерЅ░рѕърЅй рЇјрѕерѕЮ рѕЇрѕЮрІх рЅ░рІѕрѕхрІирѕЇ рІФрѕЅрЅх ріарЅХ ріФрѕ│рѕЂріЋ рЇјрѕерѕЎ рѕѕрѕўрїђрѕўрѕфрІФ рїірІю рЅарЅ│рІ│рїі ріГрѕЇрѕЇ рѕўріФрѕёрІ▒рѕЮ рѕЇрІЕ рІФрІ░рѕГрїѕрІІрѕЇ рЅЦрѕѕрІІрѕЇрЇб рІерѕ▒рѕЏрѕї ріГрѕЇрѕЇ ріерЅ░рѕЏ рѕЇрѕЏрЅхріЊ рі«ріЋрѕхрЅхрѕФріГрѕйріЋ рЅбрѕ« рѕђрѕІрЇі рІХ/рѕГ рЅарЅаріЕрѕІрЅИрІЇ ріГрѕЇрѕЅ рЇјрѕерѕЎріЋ рѕѕрѕЏрѕхрЅ░ріЊрїѕрІх рІЮрїЇрїЁрЅ▒ріЋ ріарїаріЊрЅІрѕЇрЇц рЅ░рѕ│рЅ│рЇі ріерЅ░рѕърЅйрѕЮ рІѕрІ░ рїЁрїЇрїЁрїІ ріЦрІерїѕрЅА ріљрІЇ рЅЦрѕѕрІІрѕЇрЇб рЅбрѕ« рѕђрѕІрЇірІЇ рЅарїЦрЅѓрЅх рїЇрѕѕрѕ░рЅдрЅй ріерІѕрѕФрЅх рЅарЇірЅх рЅ░ріерѕхрЅХ рІеріљрЅарѕерІЇ рЅйрїЇрѕГ рїѕрЇЁрЅ│рЅйріЋріЋ ріарЅарѕІрѕйрЅХ рІеріљрЅарѕе рЅбрѕєріЋрѕЮ ріарѕЂріЋ рїЇріЋ рѕЮріЋрѕЮ рІерЇђрїЦрЅ│рѕЮ рІГрѕЂріЋ рІерІ░рѕЁріЋріљрЅх рЅйрїЇрѕГ рІерѕѕрѕЮ рЅЦрѕѕрІІрѕЇрЇб рІХ/рѕГ ріарЅЦрІ▒рѕЇрЇѕрЅ│рѕЁ рѕхрѕЮріЋрЅ░ріЏрІЇ рІеріерЅ░рѕърЅй рЇјрѕерѕЮ рІеріГрѕЇрѕІрЅйріЋріЋ рЅЦрѕјрѕЮ рІеріерЅ░рѕЏрЅйріЋріЋ ріарѕхрЅ░рѕЏрѕЏріЮ рѕ░рѕІрѕЮ рІерѕЮріЊрѕерїІрїЇрїЦрЅарЅхріЊ рЅарЅ░рїЇрЅБрѕГрѕЮ рІерѕЮріЊрѕ│рІГрЅарЅх ріЦріЋрІ▓рѕєріЋ рѕ░рЇі рѕхрѕФ рѕ░рѕГрЅ░ріЊрѕЇ рІЇрїцрЅ▒ріЋрѕЮ ріЦрІФрІеріЋ ріљрІЇ рЅЦрѕѕрІІрѕЇрЇб рѕхрѕЮріЋрЅ░ріЏрІЇ рІерібрЅхрІ«рїхрІФ ріерЅ░рѕърЅй рЇјрѕерѕЮ ріерІеріФрЅ▓рЅх 9-14/2011 ''рѕўрІ░рѕўрѕГ рѕѕрібрЅхрІ«рїхрІФ ріерЅ░рѕърЅй рЅЦрѕЇрЇЁрїЇріЊ'' рЅарѕџрѕЇ рѕўрѕф рЅЃрѕЇ рЅарїЁрїЇрїЁрїІ рІГріФрѕёрІ│рѕЇрЇб рѕЮріЋрїГрЇд\n"
     ]
    }
   ],
   "source": [
    "random_data=cleaned_data[32000]\n",
    "print(\"Random data before Masking : \\n\\n\", random_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps_sample,targs_sample=tokenize_and_mask(random_data,noise=0.15,randomizer=np.random.uniform,tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "\n",
      " <Z> рѕЂріћрЅ│рІЇ ріарѕхрЅ░рѕЏрѕЏріЮ ріљрІЇ<Y>! . . рѕхрѕЮріЋрЅ░ріЏрІЇріЋ рІеріерЅ░рѕърЅй рЇјрѕерѕЮ рѕѕрѕЏріФрѕёрІх рІЮрїЇрїЁрЅ▒ рѕўрїаріЊрЅђрЅЂріЋріЊ рѕЮріЋрѕЮ ріарІГріљрЅх рІерЇђрїЦрЅ│ рЅйрїЇрѕГ<X> рІеріерЅ░рѕЏ рѕЇрѕЏрЅхріЊ <W> рѕџріњрѕхрЅ┤рѕГ рѕџріњрѕхрЅхрѕГ рІ┤ріцрЅ│ ріарЅХ ріФрѕ│рѕЂріЋ рїјрЇї рЅ░ріЊрїѕрѕЕрЇб рІЮрїЇрїЁрЅ▒ріЋ ріарѕхрѕўрѕЇріГрЅХ рЅарїЁрїЇрїЁрїІ рѕўрїЇрѕѕрїФ рІерѕ░рїАрЅх рѕџріњрѕхрЅхрѕГ рІ┤ріцрЅ│рІЇ рїЁрїЁрїІ рѕІрІГ рІерЇђрїЦрЅ│ <V> ріарѕхрЅ░рѕЏрѕЏріЮ ріљрІЇ<U>рЇц <T> рІерЇђрїЦрЅ│<S> рїІрѕГрѕЮ рЅарЅЁріЋрїЁрЅх ріЦрІерѕ░рѕФ ріљрІЇ рЅЦрѕѕрІІрѕЇрЇб <R> ріарѕхрЅ░рѕЏрѕЏріЮ рІерЇђрїЦрЅ│ рѕЂріћрЅ│ рѕўріќрѕЕріЋ рІерїѕрѕѕрЇЂрЅх рѕџріњрѕхрЅхрѕГ рІ┤ріцрЅ│рІЇ рЅ░рѕ│рЅ│рЇі ріерЅ░рѕърЅй рІѕрІ░рїЁрїЇрїЁрїІ рїѕрЅЦрЅ░рІЇ рІЮрїЇрїЁрЅ│рЅИрІЇріЋ<Q> рЅЦрѕѕрІІрѕЇрЇб рІЮрїЇрїЁрЅХрЅйріЋ <P> рїђрѕЮрѕ«<O> рѕерЅАрІЋ ріерѕџріќрѕЕ рІЮрїЇрїЁрЅХрЅй<N> рЅарїЁрїЇрїЁрїІ <M> рІерѕўріГрЇѕрЅ╗ рѕхріљрѕхрѕГрІљрЅхрЇБ рЅаріерЅ░рѕърЅй рІерѕхрѕФ рІЋрІхрѕЇ рЇѕрїарѕФ рібріЋрЅ░рѕГрЇЋрѕФрІГрІърЅй рѕЇрѕЏрЅхрЇБ<L> рѕЇрѕЏрЅх рѕЏріћрїЁрѕўріЋрЅхрЇБ <K> ріарїђріЋрІ│рІјрЅй рѕЏрѕѕрЅхрѕЮ рЅаріерЅ░рѕЏ рѕЇрѕЏрЅхрЇБ ріарѕеріЋрїЊрІ┤ <J>ріЊ ріаріФрЅБрЅб <I> рЅарЅцрЅХрЅйріЊ рі«ріЋрѕхрЅхрѕФріГрѕйріЋ ріЦріЋрІ▓рѕЂрѕЮ рЅ░рІФрІФрІЦ рІўрѕГрЇјрЅй рІЎрѕфрІФ рїЦріЊрЅ│рІі рЇЁрѕЂрЇјрЅй<H> ріарЅХ ріФрѕ│рѕЂріЋ ріерЅ░рѕърЅй рѕФрѕ│рЅИрІЇріЋ рІерѕџрІФрѕхрЅ░рІІрІЇрЅЂрЅарЅх<G> рІерѕџріФрѕёрІх рѕ▓рѕєріЋ ріерІџрѕЁ рЅарЇірЅх рЅарЅ░рІ░рѕерїЅ рЇјрѕерѕърЅй <F> рІерІхрѕЮрЇЁ рЅЦріГрѕѕрЅх рЅарІџрѕЁ ріарѕўрЅх ріЦріЋрІ│рІГріќрѕГ ріеріерЅ░рѕърЅй рїІрѕГ рѕўрїЇрЅБрЅБрЅх рѕІрІГ <E> рЅЦрѕѕрІІрѕЇрЇб рѕђрѕЎрѕх <D>рѕџріќрѕерІЇ рІерѕЏрїарЅЃрѕѕрІФ рѕхрѕГрІЊрЅх рѕѕрѕърІ┤рѕЇ рібріЋрЅ░рѕГрЇЋрѕФрІГрІърЅйрЇБ рѕѕрѕ┤рЅх рѕхрѕФ рЇѕрїБрѕфрІјрЅйрЇБ рЅарѕЂрѕЅрѕЮ ріГрѕЇрѕјрЅй ріФрѕЅ<C>рѕ┤ріГрЅ░рѕГ рЅ░рЅІрѕЏрЅх рЅаріарЇѕрЇЃрЇђрѕЮ рЅЦрѕЇрїФ<B> ріЦріЋрІ▓рѕЂрѕЮ рѕѕрІЕріњрЅерѕГрѕ▓рЅ▓ рЅ░рѕўрѕФрЅѓ рѕхрѕФ рЇѕрїБрѕфрІјрЅй ріЦрІЇрЅЁріЊріЊ <A> рЅ░рЅЦрѕЈрѕЇрЇб <z>рІўрїаріљріЏрІЇ рІерібрЅхрІ«рїхрІФ ріерЅ░рѕърЅй рЇјрѕерѕЮ<y> рЅаріЦрѕѕрЅ▒ рІГрЇІ рІГрІ░рѕерїІрѕЇрЇц рІерІІріЋрїФ рѕГріГріГрЅЦрѕЮ<x> рЅЦрѕѕрІІрѕЇ рѕџріњрѕхрЅхрѕГ <w>рЇб рѕхрѕЮріЋрЅ░ріЏрІЇріЋ рІеріерЅ░рѕърЅй<v> рІерЅ░рѕѕрІе рѕѕрѕЏрІхрѕерїЇ <u> ріарѕѕрѕЮріарЅђрЇЇ рІеріерЅ░рѕърЅй рЇјрѕерѕЮ рѕЇрѕЮрІх рЅ░рІѕрѕх РЂЄ рѕЇ рІФрѕЅрЅх<t> ріФрѕ│рѕЂріЋ рЇјрѕерѕЎ рѕѕрѕўрїђрѕўрѕфрІФ рїірІю рЅарЅ│рІ│рїі ріГрѕЇрѕЇ <s>рѕЮ рѕЇрІЕ <r> рЅЦрѕѕрІІрѕЇрЇб рІерѕ▒рѕЏрѕї ріГрѕЇрѕЇ ріерЅ░рѕЏ рѕЇрѕЏрЅхріЊ рі«ріЋрѕхрЅхрѕФріГрѕйріЋ рЅбрѕ« рѕђрѕІрЇі рІХ/рѕГ рЅарЅаріЕрѕІрЅИрІЇ ріГрѕЇрѕЅ рЇјрѕерѕЎріЋ <q> рІЮрїЇрїЁрЅ▒ріЋ ріарїаріЊрЅІрѕЇрЇц <p> ріерЅ░рѕърЅйрѕЮ рІѕрІ░ рїЁрїЇрїЁрїІ ріЦрІерїѕрЅА<o> рЅЦрѕѕрІІрѕЇрЇб рЅбрѕ« рѕђрѕІрЇірІЇ рЅарїЦрЅѓрЅх рїЇрѕѕрѕ░рЅдрЅй ріерІѕрѕФрЅх рЅарЇірЅх рЅ░ріерѕхрЅХ <n> рЅйрїЇрѕГ рїѕрЇЁрЅ│рЅйріЋріЋ ріарЅарѕІрѕйрЅХ рІеріљрЅарѕе рЅбрѕєріЋрѕЮ ріарѕЂріЋ рїЇріЋ рѕЮріЋрѕЮ рІерЇђрїЦрЅ│рѕЮ рІГрѕЂріЋ рІерІ░рѕЁріЋріљрЅх рЅйрїЇрѕГ рІерѕѕрѕЮ рЅЦрѕѕрІІрѕЇрЇб рІХ/рѕГ ріарЅЦрІ▒рѕЇрЇѕрЅ│рѕЁ рѕхрѕЮріЋрЅ░ріЏрІЇ рІеріерЅ░рѕърЅй рЇјрѕерѕЮ <m>ріЋ<l> рІеріерЅ░рѕЏрЅйріЋріЋ ріарѕхрЅ░рѕЏрѕЏріЮ рѕ░рѕІрѕЮ рІерѕЮріЊрѕерїІрїЇрїЦрЅарЅхріЊ <k>рѕЮ рІерѕЮріЊрѕ│рІГрЅарЅх ріЦріЋрІ▓рѕєріЋ рѕ░рЇі<j> рІЇрїцрЅ▒ріЋрѕЮ ріЦрІФрІеріЋ ріљрІЇ рЅЦрѕѕрІІрѕЇрЇб <i> рІерібрЅхрІ«рїхрІФ ріерЅ░рѕърЅй<h> ріерІеріФрЅ▓рЅх 9-<g>/2011 ''рѕўрІ░рѕўрѕГ рѕѕрібрЅхрІ«рїхрІФ ріерЅ░рѕърЅй рЅЦрѕЇрЇЁрїЇріЊ'' рЅарѕџрѕЇ рѕўрѕф <f> рѕЮріЋрїГрЇд\n",
      "\n",
      "Targets: \n",
      "\n",
      " <Z> рІерЇђрїЦрЅ│<Y> РЂЄ рїЁрїЇрїЁрїІ<X> ріарѕѕрѕўріќрѕЕріЋ <W> рі«ріЋрѕхрЅхрѕФріГрѕйріЋ <V> рѕЂріћрЅ│рІЇ<U>\" <T> рІеріГрѕЇрѕЅ рЇќрѕірѕх ріерЇїрІ┤рѕФрѕЇ<S> ріаріФрѕІрЅх <R> рЅаріерЅ░рѕЏрІІ<Q> рїђрѕЮрѕерІІрѕЇ <P> рЅарЅ░рѕўрѕѕріерЅ░ ріеріљрїѕ<O> ріЦрѕхріе<N> рѕўріФріерѕЇ <M> рѕхрЅ┤рІ▓рІерѕЮ<L> рЅарѕўрѕгрЅх <K> рЅарІўрѕГрЇЅ <J> рѕЇрѕЏрЅх <I> рїЦрЅарЅЃрЇБ<H> рІГрЅђрѕГрЅБрѕЅрЇб<G> ріцрїЇрІџрЅбрѕйріЋ <F> рІерЅ░рѕхрЅ░рІІрѕѕрІЇ <E> рЅ░рІ░рѕГрѕирѕЇ <D> рЅа<C> рІе<B> рѕІрїѕріЎ <A> рѕйрѕЇрѕЏрЅх рІГрѕ░рїБрѕЇ <z> рЅарЅ░рѕўрѕ│рѕ│рІГ рІе<y> ріарІўрїІрїЁ<x> рІГріќрѕФрѕЇ <w> рІ┤ріцрЅ│рІЇ<v> рЇјрѕерѕЮ <u> ріерѕЏрѕїрІЦрІФ<t> ріарЅХ <s> рѕўріФрѕёрІ▒ <r> рІФрІ░рѕГрїѕрІІрѕЇ <q> рѕѕрѕЏрѕхрЅ░ріЊрїѕрІх <p> рЅ░рѕ│рЅ│рЇі<o> ріљрІЇ <n> рІеріљрЅарѕерІЇ <m> рІеріГрѕЇрѕІрЅйріЋ<l> рЅЦрѕјрѕЮ <k> рЅарЅ░рїЇрЅБрѕГ<j> рѕхрѕФ рѕ░рѕГрЅ░ріЊрѕЇ <i> рѕхрѕЮріЋрЅ░ріЏрІЇ<h> рЇјрѕерѕЮ<g>14 <f> рЅЃрѕЇ рЅарїЁрїЇрїЁрїІ рІГріФрѕёрІ│рѕЇрЇб\n"
     ]
    }
   ],
   "source": [
    "print('Inputs: \\n\\n', pretty_decode(inps_sample, sentinels, tokenizer))\n",
    "print('\\nTargets: \\n\\n', pretty_decode(targs_sample, sentinels, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ріарѕхрЅђрІхрѕю рїЦрІФрЅёрІг рЅарїерІІріљрЅх рЅарІЇрѕхрїЦ рѕўрѕхрѕўрѕГ ріЦріЋрІ▓рІ░рѕГрѕхрІј ріарІхрѕГрїї рЇЇрЅхрѕЁріЋ рѕѕрѕЏрїѕрІЮ рЅЦрѕъріГрѕГ ріЦрѕГрѕхрІј рІерѕўрЇЁрѕђрЇЇ рЅЁрІ▒рѕ▒ рІерІ│рІірЅх рідрѕ«рІ«ріЋріЋ рѕўріЋрїѕрІх рЅарѕўрѕЮрѕерїЦрІј рЅарІГрЇІ рѕѕрѕўрЇЃрЇЇ рЅ░рїѕрІхрїЃрѕѕрѕЂ ріарѕЂріЋрѕЮ рѕўрѕерїЃрІЇ ріЦріЋрІ┤рЅх ріЦрѕ▒ рїІрѕГ рІ░рѕерѕ░ рІерѕџрѕѕрІЇріЋ рІЇрЅхрІѕрЅ│ рЅхрЅ░рІЇ рЅарѕЏріЋріЏрІЇрѕЮ рѕўріЋрїѕрІх рЅђрїЦрЅ░ріЏ рѕЮрѕІрѕй рІГрѕхрїАріЮ ріерѕўрѕхріерѕерѕЮ рІѕрІ▓рІФ ріарѕЂріЋ рІФрѕѕрѕЂ рѕўріЋрїЇрѕхрЅх рѕЁрїІрІі рІерѕхрѕФ рІўрѕўріЉ рѕхрѕѕрѕџрІФрЅарЅЃ рІерЅБрѕѕріарІ░рѕФ рѕўріЋрїЇрѕхрЅх ріЦріЋрІ▓рЅІрЅІрѕЮ рѕєріќрѕЮ ріарѕЂріЋ рІФрѕѕрІЇ рІеріарЅЦрІГ рѕўріЋрїЇрѕхрЅх ріарѕхрЇѕрЇЃрѕџ ріаріФрѕЇ рІерѕўріЋрїЇрѕхрЅхріЋ рІеріЦрѕѕ рЅ░ріЦрѕѕрЅх рЅ░рїЇрЅБрѕФрЅхріЋ ріЦрІеріерІѕріљ рѕЮрѕГрїФ ріЦрѕхріфрІ░рѕерїЇ рѕѕ ріарѕўрЅх ріЦріЦріљрІџрѕЁріЋ рІѕрѕ│ріЮ рїЅрІ│рІ«рЅйріЋ рІерѕџрІФрѕхрЇѕрЇЁрѕЮ ріаріФрѕЇ ріЦріЋрІ▓рЅІрЅІрѕЮріЊ ріГрЅхрЅхрѕЇ ріЦріЋрІ▓рІ░рѕерїЇ рЅарѕўрїЇрѕѕрїФрІЇ рїарІГрЅђрІІрѕЇ рІеріарѕЏрѕФ рѕЁрІЮрЅЦ рЅаріаріЦрѕЮрѕ« ріГріЋрЇЅ рІФрѕЇрЅарѕерѕерЅарЅх рїЦрЅарЅЦріЊ рЇЇрѕЇрѕхрЇЇріЊ рІФрѕЇріерЇѕрЅ░рІЇ рІеріЦрІЇрЅђрЅх рїјрІ│ріЊ ріарІГріЉ рІФрѕІрІерІЇ рїєрѕ«рІЇ рІФрѕЇрѕ░рѕЏрІЇ рѕЇрЅА рІФрѕІрѕ░рЅарІЇ ріЦрІЇрЅђрЅхріЊ рЅЦрѕЇрѕђрЅх рІерѕѕрѕЮріеріарѕЏрѕФ рѕЁрІЮрЅЦ рІерѕђрїѕрѕфрЅ▒ рІўрѕГрЇѕ рЅЦрІЎ ріЦрІЇрЅђрЅх рѕўріЋрїГрЅХ рІерѕърѕІрЅарЅхріерѕЎрѕІрЅ▒рѕЮ рЅарѕўрѕЇріГ рЅарѕўрѕЇріЕ рѕ▓рЅђрІ│рЅарЅх рІеріќрѕе ріерібрЅхрІ«рїхрІФ ріарѕЇрЇј рѕѕріарѕѕрѕЮ рѕ▓рЅ│рІ░рѕЇ рІеріќрѕерІЇріЊ рІерѕџріќрѕерІЇ ріЦрІЇрЅђрЅх рІерЅ░рїѕріўрЅарЅх рЅ│рѕІрЅЁ ріљрїѕрІх ріљрІЇ рІЏрѕг рЅарІерЅхріЏрІЇрѕЮ рѕўрѕѕріфрІФ рІГрѕЂріЋ рѕўрѕўрІўріЏ рібрЅхрІ«рїхрІФрІіріљрЅх рІерѕџріЋрЇђрЅБрѕерЅђрІЇ рЅаріарѕЏрѕФ рѕЁрІЮрЅЦ рѕІрІГ рЅЦрЅ╗ ріљрІЇрѕїрѕІрІЇ рІерЅхрѕЁріљрїЇріЋ рЅБріЋрІ▓рѕФ рѕѕрЅЦрѕХрІерідріљрїЇріЋ рЅБріЋрІ▓рѕФ ріЦрІФрІЇрѕѕрЅарѕѕрЅа рІерібрЅхрІ«рїхрІФріЋ рѕ░ріЋрІ░рЅЁріарѕІрѕЏ рЅаріЦрїЇрѕЕ рѕерїЇрїдрЅ│рѕЇрїерѕГрЅЁ ріљрІЇ рЅЦрѕѕрІЇ ріарЅЃрїЦрѕѕрІЇрЅ│рѕЇрЅђрІ│рІ░рІЇ рїЦрѕѕрІЇрЅ│рѕЇ'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.detokenize(tokenizer.tokenize(\"ріарѕхрЅђрІхрѕю рїЦрІФрЅёрІг рЅарїерІІріљрЅх рЅарІЇрѕхрїЦ рѕўрѕхрѕўрѕГ ріЦріЋрІ▓рІ░рѕГрѕхрІј ріарІхрѕГрїї рЇЇрЅхрѕЁріЋ рѕѕрѕЏрїѕрІЮ рЅЦрѕъріГрѕГ  ріЦрѕГрѕхрІј рІерѕўрЇЁрѕђрЇЇ рЅЁрІ▒рѕ▒ рІерІ│рІірЅх  рідрѕ«рІ«ріЋріЋ рѕўріЋрїѕрІх рЅарѕўрѕЮрѕерїЦрІј рЅарІГрЇІ рѕѕрѕўрЇЃрЇЇ рЅ░рїѕрІхрїЃрѕѕрѕЂ  ріарѕЂріЋрѕЮ рѕўрѕерїЃрІЇ ріЦріЋрІ┤рЅх ріЦрѕ▒ рїІрѕГ рІ░рѕерѕ░ рІерѕџрѕѕрІЇріЋ рІЇрЅхрІѕрЅ│ рЅхрЅ░рІЇ рЅарѕЏріЋріЏрІЇрѕЮ рѕўріЋрїѕрІх рЅђрїЦрЅ░ріЏ рѕЮрѕІрѕй рІГрѕхрїАріЮ   ріерѕўрѕхріерѕерѕЮ   рІѕрІ▓рІФ ріарѕЂріЋ рІФрѕѕрѕЂ рѕўріЋрїЇрѕхрЅх рѕЁрїІрІі рІерѕхрѕФ рІўрѕўріЉ рѕхрѕѕрѕџрІФрЅарЅЃ рІерЅБрѕѕріарІ░рѕФ рѕўріЋрїЇрѕхрЅх ріЦріЋрІ▓рЅІрЅІрѕЮ рѕєріќрѕЮ ріарѕЂріЋ рІФрѕѕрІЇ рІеріарЅЦрІГ рѕўріЋрїЇрѕхрЅх ріарѕхрЇѕрЇЃрѕџ ріаріФрѕЇ рІерѕўріЋрїЇрѕхрЅхріЋ рІеріЦрѕѕ рЅ░ріЦрѕѕрЅх рЅ░рїЇрЅБрѕФрЅхріЋ ріЦрІеріерІѕріљ рѕЮрѕГрїФ ріЦрѕхріфрІ░рѕерїЇ рѕѕ ріарѕўрЅх ріЦріЦріљрІџрѕЁріЋ рІѕрѕ│ріЮ рїЅрІ│рІ«рЅйріЋ рІерѕџрІФрѕхрЇѕрЇЁрѕЮ ріаріФрѕЇ ріЦріЋрІ▓рЅІрЅІрѕЮріЊ ріГрЅхрЅхрѕЇ ріЦріЋрІ▓рІ░рѕерїЇ рЅарѕўрїЇрѕѕрїФрІЇ рїарІГрЅђрІІрѕЇ рІеріарѕЏрѕФ рѕЁрІЮрЅЦ рЅаріаріЦрѕЮрѕ« ріГріЋрЇЅ рІФрѕЇрЅарѕерѕерЅарЅх рїЦрЅарЅЦріЊ рЇЇрѕЇрѕхрЇЇріЊ рІФрѕЇріерЇѕрЅ░рІЇ рІеріЦрІЇрЅђрЅх рїјрІ│ріЊ ріарІГріЉ рІФрѕІрІерІЇ рїєрѕ«рІЇ рІФрѕЇрѕ░рѕЏрІЇ рѕЇрЅА рІФрѕІрѕ░рЅарІЇ ріЦрІЇрЅђрЅхріЊ рЅЦрѕЇрѕђрЅх рІерѕѕрѕЮріеріарѕЏрѕФ рѕЁрІЮрЅЦ рІерѕђрїѕрѕфрЅ▒ рІўрѕГрЇѕ рЅЦрІЎ ріЦрІЇрЅђрЅх рѕўріЋрїГрЅХ рІерѕърѕІрЅарЅхріерѕЎрѕІрЅ▒рѕЮ рЅарѕўрѕЇріГ рЅарѕўрѕЇріЕ рѕ▓рЅђрІ│рЅарЅх рІеріќрѕе ріерібрЅхрІ«рїхрІФ ріарѕЇрЇј рѕѕріарѕѕрѕЮ рѕ▓рЅ│рІ░рѕЇ рІеріќрѕерІЇріЊ рІерѕџріќрѕерІЇ ріЦрІЇрЅђрЅх рІерЅ░рїѕріўрЅарЅх рЅ│рѕІрЅЁ ріљрїѕрІх ріљрІЇ рІЏрѕг рЅарІерЅхріЏрІЇрѕЮ рѕўрѕѕріфрІФ рІГрѕЂріЋ рѕўрѕўрІўріЏ рібрЅхрІ«рїхрІФрІіріљрЅх рІерѕџріЋрЇђрЅБрѕерЅђрІЇ рЅаріарѕЏрѕФ рѕЁрІЮрЅЦ рѕІрІГ рЅЦрЅ╗ ріљрІЇрѕїрѕІрІЇ рІерЅхрѕЁріљрїЇріЋ рЅБріЋрІ▓рѕФ рѕѕрЅЦрѕХрІерідріљрїЇріЋ рЅБріЋрІ▓рѕФ ріЦрІФрІЇрѕѕрЅарѕѕрЅа рІерібрЅхрІ«рїхрІФріЋ рѕ░ріЋрІ░рЅЁріарѕІрѕЏ рЅаріЦрїЇрѕЕ рѕерїЇрїдрЅ│рѕЇрїерѕГрЅЁ ріљрІЇ рЅЦрѕѕрІЇ ріарЅЃрїЦрѕѕрІЇрЅ│рѕЇрЅђрІ│рІ░рІЇ рїЦрѕѕрІЇрЅ│рѕЇ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Creating the training data pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will create pairs using the cleaned_data by iterating over the data and create(inp,targ) pairs using the function I defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing data preprocessing and defining the dataset, I encountered an issue while training the model: the maximum sequence length is around 1300 tokens, causing the model to run out of memory and crash. This happens because transformer models, when trained using a masked language model (MLM) objective, have quadratic complexity (O(n┬▓)) concerning sequence length, leading to excessive memory usage for long sequences. To resolve this issue, we need to reduce the size of the input sequences by spliting a single news into multiple small news(max_size words per news). this process will reduce the size of each news but will increase the number of training datas as we split single data into multiple datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of contents before reduced: 193419\n",
      "Number of contents after reduced: 408671\n",
      "maximum size before reduced: 836\n",
      "maximum size after reduced: 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_size=50\n",
    "\n",
    "def reduce_size(data, max_size):\n",
    "    for element in data:  # Loop over each sub-array in the main array\n",
    "        sub_array = element.split()\n",
    "        for i in range(0, len(sub_array), max_size):  # Split each sub-array into chunks\n",
    "            yield  ' '.join(sub_array[i:i + max_size])\n",
    "            \n",
    "cleaned_data_reduced=list(reduce_size(cleaned_data,max_size))\n",
    "print(f'Number of contents before reduced: {len(cleaned_data)}')\n",
    "print(f'Number of contents after reduced: {len(cleaned_data_reduced)}')\n",
    "print(f\"maximum size before reduced: {max([len(content.split()) for content in cleaned_data])}\")\n",
    "print(f\"maximum size after reduced: {max([len(content.split()) for content in cleaned_data_reduced])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us see sample the news before and after reducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before reduced: \n",
      "\n",
      " рѕ░рІЇріЋ рѕѕрѕўрѕГрІ│рЅх рѕ░рІЇ рѕўрѕєріЋ рЅарЅѓ ріљрІЇ ! рЅхрѕІріЋрЅх рІеріФрЅ▓рЅх 1/2017 рІЊ/рѕЮ рЅарїђрѕўрѕерІЇ рІерѕўрЅёрІХріЋрІФ рІеріарѕерїІрІірІФріЋ ріЦріЊ рІеріаріЦрѕЮрѕ« рѕЁрѕЎрѕЏріЋ рѕўрѕГрїЃ рѕЏрІЋріерѕЇ рІерІхрїІрЇЇ рѕЏрѕ░рЅБрѕ░рЅЦ рІўрѕўрЅ╗ ріЦрѕхріЕріЋ 120,000,000 рЅЦрѕГ рЅ░рѕ░рЅЦрѕхрЅДрѕЇрЇб рѕўрЅёрІХріЋрІФ рЅарѕџрІФрѕхрїѕріљрЅБрІЇ рѕєрѕхрЇњрЅ│рѕЇ рїГрѕЮрѕГ рІФрѕѕрІЇ рѕЁріЋрЇЃ рѕѕрѕЏрїаріЊрЅђрЅЁ рІерїѕріЋрІўрЅЦ ріЦрїЦрѕерЅх ріарїІрїЦрѕърЅ│рѕЇрЇб рѕЁріЋрЇЃрІЇ рѕѕрѕЏрїаріЊрЅђрЅЁ рїѕріЋрІўрЅЦ рЅ░рЅИрїЇрѕеріЊрѕЇрЇб рѕѕрѕЏрїаріЊрЅђрЅЁ рІѕрІ░ 5 рЅбрѕірІ«ріЋ рЅЦрѕГ рІФрѕхрЇѕрѕЇрїІрѕЇрЇб рЅарЅђрїЦрЅ│ рІГріерЅ│рЅ░рѕЅ рІерѕЮрЅхрЅйрѕЅрЅхріЋ рѕЂрѕЅ рІхрїІрЇЇ ріарІхрѕГрїЅрЇб\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Data after reduced: \n",
      "\n",
      " рѕ░рІЇріЋ рѕѕрѕўрѕГрІ│рЅх рѕ░рІЇ рѕўрѕєріЋ рЅарЅѓ ріљрІЇ ! рЅхрѕІріЋрЅх рІеріФрЅ▓рЅх 1/2017 рІЊ/рѕЮ рЅарїђрѕўрѕерІЇ рІерѕўрЅёрІХріЋрІФ рІеріарѕерїІрІірІФріЋ ріЦріЊ рІеріаріЦрѕЮрѕ« рѕЁрѕЎрѕЏріЋ рѕўрѕГрїЃ рѕЏрІЋріерѕЇ рІерІхрїІрЇЇ рѕЏрѕ░рЅБрѕ░рЅЦ рІўрѕўрЅ╗ ріЦрѕхріЕріЋ 120,000,000 рЅЦрѕГ рЅ░рѕ░рЅЦрѕхрЅДрѕЇрЇб рѕўрЅёрІХріЋрІФ рЅарѕџрІФрѕхрїѕріљрЅБрІЇ рѕєрѕхрЇњрЅ│рѕЇ рїГрѕЮрѕГ рІФрѕѕрІЇ рѕЁріЋрЇЃ рѕѕрѕЏрїаріЊрЅђрЅЁ рІерїѕріЋрІўрЅЦ ріЦрїЦрѕерЅх ріарїІрїЦрѕърЅ│рѕЇрЇб рѕЁріЋрЇЃрІЇ рѕѕрѕЏрїаріЊрЅђрЅЁ рїѕріЋрІўрЅЦ рЅ░рЅИрїЇрѕеріЊрѕЇрЇб рѕѕрѕЏрїаріЊрЅђрЅЁ рІѕрІ░ 5 рЅбрѕірІ«ріЋ рЅЦрѕГ рІФрѕхрЇѕрѕЇрїІрѕЇрЇб рЅарЅђрїЦрЅ│ рІГріерЅ│рЅ░рѕЅ рІерѕЮрЅхрЅйрѕЅрЅхріЋ рѕЂрѕЅ\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Data after reduced: \n",
      "\n",
      " рІхрїІрЇЇ ріарІхрѕГрїЅрЇб\n"
     ]
    }
   ],
   "source": [
    "print(\"Data before reduced: \\n\\n\", cleaned_data[0])\n",
    "print(\"\\n---------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"Data after reduced: \\n\\n\", cleaned_data_reduced[0])\n",
    "print(\"\\n---------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"Data after reduced: \\n\\n\", cleaned_data_reduced[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_targets_pairs=[tokenize_and_mask(text.encode('utf-8', errors=\"ignore\").decode('utf-8'),tokenizer=tokenizer) for text in cleaned_data_reduced]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "\n",
      " рѕ░рІЇріЋ рѕѕрѕўрѕГрІ│рЅх<Z> рѕўрѕєріЋ рЅарЅѓ ріљрІЇ ! рЅхрѕІріЋрЅх рІеріФрЅ▓рЅх 1/2017 рІЊ/рѕЮ рЅарїђрѕўрѕерІЇ рІерѕўрЅёрІХріЋрІФ рІеріарѕерїІрІірІФріЋ ріЦріЊ рІеріаріЦрѕЮрѕ« рѕЁрѕЎрѕЏріЋ<Y> рѕЏрІЋріерѕЇ<X> рѕЏрѕ░рЅБрѕ░рЅЦ рІўрѕўрЅ╗ ріЦрѕхріЕріЋ 120,000,000 рЅЦрѕГ рЅ░рѕ░рЅЦрѕхрЅДрѕЇрЇб рѕўрЅёрІХріЋрІФ <W> рїГрѕЮрѕГ рІФрѕѕрІЇ <V> рІерїѕріЋрІўрЅЦ ріЦрїЦрѕерЅх ріарїІрїЦрѕърЅ│рѕЇрЇб рѕЁріЋрЇЃрІЇ рѕѕрѕЏрїаріЊрЅђрЅЁ рїѕріЋрІўрЅЦ рЅ░рЅИрїЇрѕеріЊрѕЇрЇб<U> рІѕрІ░ 5 рЅбрѕірІ«ріЋ рЅЦрѕГ рІФрѕхрЇѕрѕЇрїІрѕЇрЇб рЅарЅђрїЦрЅ│ рІГріерЅ│рЅ░рѕЅ рІерѕЮрЅхрЅйрѕЅрЅхріЋ рѕЂрѕЅ\n",
      "\n",
      "Targets: \n",
      "\n",
      " <Z> рѕ░рІЇ<Y> рѕўрѕГрїЃ<X> рІерІхрїІрЇЇ <W> рЅарѕџрІФрѕхрїѕріљрЅБрІЇ рѕєрѕхрЇњрЅ│рѕЇ <V> рѕЁріЋрЇЃ рѕѕрѕЏрїаріЊрЅђрЅЁ<U> рѕѕрѕЏрїаріЊрЅђрЅЁ\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Inputs: \n",
      "\n",
      " рІхрїІрЇЇ ріарІхрѕГрїЅрЇб\n",
      "\n",
      "Targets: \n",
      "\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pairs in inputs_targets_pairs[:2]:\n",
    "    print('Inputs: \\n\\n', pretty_decode(pairs[0], sentinels, tokenizer))\n",
    "    print('\\nTargets: \\n\\n', pretty_decode(pairs[1], sentinels, tokenizer))\n",
    "    print(\"\\n---------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of inputs and targets pairs:  408671\n",
      "Training data size: 326936\n",
      "Validation data size: 81735\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of inputs and targets pairs: \",len(inputs_targets_pairs))\n",
    "training_size=int(len(inputs_targets_pairs)*0.8)\n",
    "training_data=inputs_targets_pairs[:training_size]\n",
    "validation_data=inputs_targets_pairs[training_size:]\n",
    "print(f\"Training data size: {len(training_data)}\")\n",
    "print(f\"Validation data size: {len(validation_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a Tensorflow model we need to arrange the data into datasets. Now, I will get the `inputs` and the `targets` for the transformer model from the `training_data and validation_data`. Before creating the dataset, I need to be sure that all `inputs` have the same length by truncating the longer sequences and padding the shorter ones with `0`. The same must be done for the targets. The function `tf.keras.preprocessing.sequence.pad_sequences` will help us here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 12:25:12.919476: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-02-22 12:25:12.919645: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-02-22 12:25:12.919657: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "2025-02-22 12:25:12.920111: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-22 12:25:12.920436: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "training_data_inputs_padded=tf.keras.utils.pad_sequences(\n",
    "    [pairs[0] for pairs in training_data],\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    ")\n",
    "training_data_targets_padded=tf.keras.utils.pad_sequences(\n",
    "    [pairs[1] for pairs in training_data],\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    ")\n",
    "\n",
    "validation_data_inputs_padded=tf.keras.utils.pad_sequences(\n",
    "    [pairs[0] for pairs in validation_data],\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    ")\n",
    "\n",
    "validation_data_targets_padded=tf.keras.utils.pad_sequences(\n",
    "    [pairs[1] for pairs in validation_data],\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    ")\n",
    "BUFFER_SIZE = 12000\n",
    "BATCH_SIZE = 32\n",
    "training_dataset_final=tf.data.Dataset.from_tensor_slices((training_data_inputs_padded,training_data_targets_padded))\n",
    "validation_dataset_final=tf.data.Dataset.from_tensor_slices((validation_data_inputs_padded,validation_data_targets_padded))\n",
    "training_dataset_final=training_dataset_final.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset_final=validation_dataset_final.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum size of training data inputs: 201\n",
      "maximum size of training data targets: 63\n",
      "maximum size of validation data inputs: 138\n",
      "maximum size of validation data targets: 49\n",
      "seems good size for small memory devices\n"
     ]
    }
   ],
   "source": [
    "print(f'maximum size of training data inputs: {training_data_inputs_padded.shape[1]}')\n",
    "print(f'maximum size of training data targets: {training_data_targets_padded.shape[1]}')\n",
    "print(f'maximum size of validation data inputs: {validation_data_inputs_padded.shape[1]}')\n",
    "print(f'maximum size of validation data targets: {validation_data_targets_padded.shape[1]}')\n",
    "print(\"seems good size for small memory devices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Tokenize both the training and validation sets using our tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pretraining the Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to define the structure of the transformer network and pretrain it on the dataset given above. The general structure of the transformer model we will build is shown in the figure below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"images/fulltransformer.png\" width=\"500\" height=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "As you can see in the figure, the input embeddings are added with positional embedding vectors to capture the position of words in a sentence. The following function creates positional encoding given the embedding vectors.\n",
    "\n",
    "In sequence-to-sequence tasks, the relative order of your data is extremely important to its meaning. When you were training sequential neural networks such as RNNs, you fed your inputs into the network in order. Information about the order of your data was automatically fed into your model. However, when you train a Transformer network using multi-head attention, you feed your data into the model all at once. While this dramatically reduces training time, there is no information about the order of your data. This is where positional encoding is useful - you can specifically encode the positions of your inputs and pass them into the network using these sine and cosine formulas:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "*   `d` is the dimension of the word embedding and positional encoding.\n",
    "*   `pos` is the position of the word.\n",
    "*   `i` refers to each of the different dimensions in the positional encodings, where `i = k // 2`.\n",
    "\n",
    "To develop some intuition about positional encodings, you can think of them broadly as a feature that contains the information about the relative positions of words. The sum of the positional encoding and word embedding is ultimately what is fed into the model.  If you just hard code the positions in, say by adding a matrix of 1's or whole numbers to the word embedding, the semantic meaning is distorted. Conversely, the values of the sine and cosine equations are small enough (between -1 and 1) that when you add the positional encoding to a word embedding, the word embedding is not significantly distorted, and is instead enriched with positional information. Using a combination of these two equations helps your Transformer network attend to the relative positions of your input data.\n",
    "\n",
    "### Sine and Cosine Angles\n",
    "\n",
    "Notice that even though the sine and cosine positional encoding equations take in different arguments (`2i` versus `2i+1`, or even versus odd numbers) the inner terms for both equations are the same:\n",
    "\n",
    "$$\\theta(pos, i, d) = \\frac{pos}{10000^{\\frac{2i}{d}}}$$\n",
    "\n",
    "Consider the inner term as you calculate the positional encoding for a word in a sequence:\n",
    "\n",
    "$PE_{(pos, 0)}= sin\\left(\\frac{pos}{{10000}^{\\frac{0}{d}}}\\right)$, since solving `2i = 0` gives `i = 0`\n",
    "\n",
    "$PE_{(pos, 1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{0}{d}}}\\right)$, since solving `2i + 1 = 1` gives `i = 0`\n",
    "\n",
    "The angle is the same for both! The angles for $PE_{(pos, 2)}$ and $PE_{(pos, 3)}$ are the same as well, since for both, `i = 1` and therefore the inner term is $\\left(\\frac{pos}{{10000}^{\\frac{2}{d}}}\\right)$. This relationship holds true for all paired sine and cosine curves:\n",
    "\n",
    "| k             | 0                         | 1                         | 2                         | 3                         | ... | d - 2                     | d - 1                     |\n",
    "| ------------- | ------------------------- | ------------------------- | ------------------------- | ------------------------- | --- | ------------------------- | ------------------------- |\n",
    "| encoding(0) = | [sin(╬И(0, 0, d))         | cos(╬И(0, 0, d))         | sin(╬И(0, 1, d))         | cos(╬И(0, 1, d))         | ... | sin(╬И(0, d//2, d))        | cos(╬И(0, d//2, d))        |\n",
    "| encoding(1) = | [sin(╬И(1, 0, d))         | cos(╬И(1, 0, d))         | sin(╬И(1, 1, d))         | cos(╬И(1, 1, d))         | ... | sin(╬И(1, d//2, d))        | cos(╬И(1, d//2, d))        |\n",
    "| ...           | ...                       | ...                       | ...                       | ...                       | ... | ...                       | ...                       |\n",
    "| encoding(pos) =| [sin(╬И(pos, 0, d))        | cos(╬И(pos, 0, d))        | sin(╬И(pos, 1, d))        | cos(╬И(pos, 1, d))        | ... | sin(╬И(pos, d//2, d))       | cos(╬И(pos, d//2, d))]      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d_model):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int): Maximum number of positions to be encoded \n",
    "        d_model (int): Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding (tf.Tensor): A matrix of shape (1, position, d_model) with the positional encodings\n",
    "    \"\"\"\n",
    "    \n",
    "    position = np.arange(positions)[:, np.newaxis]\n",
    "    k = np.arange(d_model)[np.newaxis, :]\n",
    "    i = k // 2\n",
    "    \n",
    "    # initialize a matrix angle_rads of all the angles \n",
    "    angle_rates = 1 / np.power(10000, (2 * i) / np.float32(d_model))\n",
    "    angle_rads = position * angle_rates\n",
    "  \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4.2 Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The masking we will define here is different from the masking we used while preparing the data for Masked Language modeling. \n",
    "\n",
    "\n",
    "There are two types of masks that are useful when building your Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in your input sentence. \n",
    "\n",
    "### 1.1 - Padding Mask\n",
    "\n",
    "Oftentimes your input sequence will exceed the maximum length of a sequence your network can process. Let's say the maximum length of your model is five, it is fed the following sequences:\n",
    "\n",
    "    [[\"Do\", \"you\", \"know\", \"when\", \"Jane\", \"is\", \"going\", \"to\", \"visit\", \"Africa\"], \n",
    "     [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\" ],\n",
    "     [\"Exciting\", \"!\"]\n",
    "    ]\n",
    "\n",
    "which might get vectorized as:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600]\n",
    "    ]\n",
    "    \n",
    "When passing sequences into a transformer model, it is important that they are of uniform length. You can achieve this by padding the sequence with zeros, and truncating sentences that exceed the maximum length of your model:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99],\n",
    "     [ 2344, 345, 1284, 15, 0],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600, 0, 0, 0],\n",
    "    ]\n",
    "    \n",
    "Sequences longer than the maximum length of five will be truncated, and zeros will be added to the truncated sequence to achieve uniform length. Similarly, for sequences shorter than the maximum length, zeros will also be added for padding.\n",
    "\n",
    "When pasing these vectors through the attention layers, the zeros will typically disappear  (you will get completely new vectors given the mathematical operations that happen in the attention block). However, you still want the network to attend only to the first few numbers in that vector (given by the sentence length) and this is when a padding mask comes in handy. You will need to define a boolean mask that specifies to which elements you must attend (1) and which elements you must ignore (0) and you do this by looking at all the zeros in the sequence. Then you use the mask to set the values of the vectors (corresponding to the zeros in the initial vector) close to negative infinity (-1e9).\n",
    "\n",
    "Imagine your input vector is `[87, 600, 0, 0, 0]`. This would give you a mask of `[1, 1, 0, 0, 0]`. When your vector passes through the attention mechanism, you get another (randomly looking) vector, let's say `[1, 2, 3, 4, 5]`, which after masking becomes `[1, 2, -1e9, -1e9, -1e9]`, so that when you take the softmax, the last three elements (where there were zeros in the input) don't affect the score.\n",
    "\n",
    "The [MultiheadAttention](https://keras.io/api/layers/attention_layers/multi_head_attention/) layer implemented in Keras, uses this masking logic.\n",
    "\n",
    "**Note:** The below functions create the masking of both types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(decoder_token_ids):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    \n",
    "    Arguments:\n",
    "        decoder_token_ids (matrix like): matrix of size (n, m)\n",
    "    \n",
    "    Returns:\n",
    "        mask (tf.Tensor): binary tensor of size (n, 1, m)\n",
    "    \"\"\"    \n",
    "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding to the attention logits. \n",
    "    # this will allow for broadcasting later when comparing sequences\n",
    "    return seq[:, tf.newaxis, :] \n",
    "\n",
    "\n",
    "def create_look_ahead_mask(sequence_length):\n",
    "    \"\"\"\n",
    "    Returns a lower triangular matrix filled with ones\n",
    "    \n",
    "    Arguments:\n",
    "        sequence_length (int): matrix size\n",
    "    \n",
    "    Returns:\n",
    "        mask (tf.Tensor): binary tensor of size (sequence_length, sequence_length)\n",
    "    \"\"\"\n",
    "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.3 - Self-Attention\n",
    "\n",
    "As the authors of the Transformers paper state, \"Attention is All You Need\". \n",
    "\n",
    "<center><img src=\"images/attention.png\" alt=\"Encoder\" width=\"600\"/></center>\n",
    "\n",
    "<center><caption><font color='purple'><b>Figure 1: Self-Attention calculation visualization</font></</caption></center>\n",
    "    \n",
    "\n",
    "The use of self-attention paired with traditional convolutional networks allows for parallelization which speeds up training. we will implement **scaled dot product attention** which takes in a query, key, value, and a mask as inputs to return rich, attention-based vector representations of the words in your sequence. This type of self-attention can be mathematically expressed as:\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{4}\\\n",
    "$$\n",
    "\n",
    "* $Q$ is the matrix of queries \n",
    "* $K$ is the matrix of keys\n",
    "* $V$ is the matrix of values\n",
    "* $M$ is the optional mask you choose to apply \n",
    "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode\n",
    "\n",
    "\n",
    "This will be handled by Tensorlfow so we will not searately define a function to handel self-attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the encoder part of the transformer using multi-head attention and feed forward. \n",
    "The structure of the model we will implement will look like the following figure.\n",
    "\n",
    "<center><img src=\"images/encoders.png\" alt=\"Encoder\" width=\"400\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above figure inside the Encoder there is a feed forward layer. Here we will use 2 Dense layers as part of the Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    \"\"\"\n",
    "    Returns a sequential model consisting of two dense layers. The first dense layer has\n",
    "    fully_connected_dim neurons and is activated by relu. The second dense layer has\n",
    "    embedding_dim and no activation.\n",
    "\n",
    "    Arguments:\n",
    "        embedding_dim (int): output dimension\n",
    "        fully_connected_dim (int): dimension of the hidden layer\n",
    "\n",
    "    Returns:\n",
    "        _ (tf.keras.Model): sequential model\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, d_model)\n",
    "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define the encoder layer class that contains both the multi-head attention and the FullyConnected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network. \n",
    "    This architecture includes a residual connection around each of the two \n",
    "    sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FullyConnected(\n",
    "            embedding_dim=embedding_dim,\n",
    "            fully_connected_dim=fully_connected_dim\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask (tf.Tensor): Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            encoder_layer_out (tf.Tensor): Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Dropout is added by Keras automatically if the dropout parameter is non-zero during training\n",
    "        self_mha_output = self.mha(x, x, x, mask)  # Self attention (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # skip connection\n",
    "        # apply layer normalization on sum of the input and the attention output to get the  \n",
    "        # output of the multi-head attention layer\n",
    "        skip_x_attention = self.layernorm1(x + self_mha_output)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "\n",
    "        # pass the output of the multi-head attention layer through a ffn\n",
    "        ffn_output = self.ffn(skip_x_attention)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # apply dropout layer to ffn output during training\n",
    "        # use `training=training`\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of the output from multi-head attention (skip connection) and ffn output\n",
    "        # to get the output of the encoder layer\n",
    "        encoder_layer_out = self.layernorm2(skip_x_attention + ffn_output)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        return encoder_layer_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the full Encoder Layer including the Embedding of the tokens and addition of positional embedding with Dropout layer before entering the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "        \n",
    "    \"\"\"  \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.embedding_dim)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, seq_len)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask (tf.Tensor): Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "\n",
    "        Returns:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, seq_len, embedding dim)\n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Pass input through the Embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        # Add the position encoding to embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        # Pass the encoded embedding through a dropout layer\n",
    "        # use `training=training`\n",
    "        x = self.dropout(x, training=training)\n",
    "        # Pass the output through the stack of encoding layers \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the decoder part of the transformer using masked multi-head attention, multi-head attention and feed forward. \n",
    "\n",
    "<b>N.B  pre-training transformer with both Encoder and decoder with MLM is not common task as most models like BERT which are pretrained using masked language modeling(MLM) need only encoders. But here I used both encoder and decoders so that the model can be further pre-trained or fine-tuned for tasks that need both encoder and decoder, like Neural Machine translation as well.</b>\n",
    "\n",
    "The structure of the decoder layer we will implement will look like the following figure.\n",
    "\n",
    "<center><img src=\"images/decoders.png\" alt=\"Encoder\" width=\"400\" /> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The decoder layer is composed by two multi-head attention blocks, \n",
    "    one that takes the new input and uses self-attention, and the other \n",
    "    one that combines it with the output of the encoder, followed by a\n",
    "    fully connected block. \n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.mha2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FullyConnected(\n",
    "            embedding_dim=embedding_dim,\n",
    "            fully_connected_dim=fully_connected_dim\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            enc_output (tf.Tensor): Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            out3 (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            attn_weights_block1 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "            attn_weights_block2 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "  \n",
    "        # enc_output.shape == (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # BLOCK 1\n",
    "        # calculate self-attention and return attention scores as attn_weights_block1.\n",
    "        # Dropout will be applied during training \n",
    "        mult_attn_out1, attn_weights_block1 = self.mha1(x,x,x,look_ahead_mask, return_attention_scores=True)\n",
    "        \n",
    "        # apply layer normalization (layernorm1) to the sum of the attention output and the input \n",
    "        Q1 = self.layernorm1(mult_attn_out1 + x)\n",
    "\n",
    "        # BLOCK 2\n",
    "        # calculate self-attention using the Q from the first block and K and V from the encoder output. \n",
    "        # Dropout will be applied during training\n",
    "        # Return attention scores as attn_weights_block2 \n",
    "        mult_attn_out2, attn_weights_block2 = self.mha2(Q1,enc_output,enc_output, padding_mask, return_attention_scores=True)\n",
    "        \n",
    "        # # apply layer normalization (layernorm2) to the sum of the attention output and the Q from the first block \n",
    "        mult_attn_out2 = self.layernorm2(mult_attn_out2+Q1)\n",
    "                \n",
    "        #BLOCK 3\n",
    "        # pass the output of the second block through a ffn\n",
    "        ffn_output = self.ffn(mult_attn_out2)\n",
    "        \n",
    "        # apply a dropout layer to the ffn output\n",
    "        # use `training=training`\n",
    "        ffn_output =self.dropout_ffn(ffn_output)\n",
    "        \n",
    "        # apply layer normalization (layernorm3) to the sum of the ffn output and the output of the second block\n",
    "        out3 =self.layernorm3(ffn_output+mult_attn_out2)\n",
    "      \n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the full Decoder Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the target input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    decoder Layers\n",
    "        \n",
    "    \"\"\" \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward  pass for the Decoder\n",
    "        \n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len)\n",
    "            enc_output (tf.Tensor):  Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "\n",
    "        # create word embeddings \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # scale embeddings by multiplying by the square root of their dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        \n",
    "        # add positional encodings to word embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        # apply a dropout layer to x\n",
    "        # use `training=training`\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # use a for loop to pass x through a stack of decoder layers and update attention_weights \n",
    "        for i in range(self.num_layers):\n",
    "            # pass x and the encoder output through a stack of decoder layers and save the attention weights\n",
    "            # of block 1 and 2 \n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                   look_ahead_mask=look_ahead_mask,\n",
    "                                                   padding_mask=padding_mask)\n",
    "\n",
    "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
    "            attention_weights[f'decoder_layer{i+1}_block1_self_att'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2_decenc_att'] = block2\n",
    "\n",
    "        \n",
    "        # x.shape == (batch_size, target_seq_len, fully_connected_dim)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine both the encoder and the decoder layers defined above into a single transformer model. The full structure of the model we will form is depicted below. In the code, in addition to encoder and decoder we will add a final Dense layer with softmax  activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/transformer.png\" width=\"3\" height=\"2\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Complete transformer with an Encoder and a Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n",
    "               target_vocab_size, max_positional_encoding_input,\n",
    "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               input_vocab_size=input_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding_input,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, \n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               target_vocab_size=target_vocab_size, \n",
    "                               maximum_position_encoding=max_positional_encoding_target,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire Transformer\n",
    "        Arguments:\n",
    "            input_sentence (tf.Tensor): Tensor of shape (batch_size, input_seq_len)\n",
    "                              An array of the indexes of the words in the input sentence\n",
    "            output_sentence (tf.Tensor): Tensor of shape (batch_size, target_seq_len)\n",
    "                              An array of the indexes of the words in the output sentence\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            enc_padding_mask (tf.Tensor): Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            dec_padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            final_output (tf.Tensor): The final output of the model\n",
    "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights for the decoder\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \n",
    "        \"\"\"\n",
    "       \n",
    "\n",
    "        enc_output = self.encoder(input_sentence, training, enc_padding_mask)        \n",
    "        # dec_output.shape == (batch_size, tar_seq_len, fully_connected_dim)\n",
    "        dec_output, attention_weights = self.decoder(output_sentence, enc_output, training, \n",
    "           look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)\n",
    " \n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the total number of Parameters of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m dec_padding_mask \u001b[38;5;241m=\u001b[39m create_padding_mask(sentence_a)\n\u001b[1;32m      8\u001b[0m look_ahead_mask \u001b[38;5;241m=\u001b[39m create_look_ahead_mask(sentence_a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 10\u001b[0m test_summary, att_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m(\n\u001b[1;32m     11\u001b[0m     sentence_a,\n\u001b[1;32m     12\u001b[0m     sentence_b,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     enc_padding_mask,\n\u001b[1;32m     15\u001b[0m     look_ahead_mask,\n\u001b[1;32m     16\u001b[0m     dec_padding_mask\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m transformer\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "# 0 is the padding value\n",
    "sentence_a = np.array([[2, 3, 1, 3, 0, 0, 0]])\n",
    "sentence_b = np.array([[1, 3, 4, 0, 0, 0, 0]])\n",
    "\n",
    "enc_padding_mask = create_padding_mask(sentence_a)\n",
    "dec_padding_mask = create_padding_mask(sentence_a)\n",
    "\n",
    "look_ahead_mask = create_look_ahead_mask(sentence_a.shape[1])\n",
    "\n",
    "test_summary, att_weights = transformer(\n",
    "    sentence_a,\n",
    "    sentence_b,\n",
    "    False,\n",
    "    enc_padding_mask,\n",
    "    look_ahead_mask,\n",
    "    dec_padding_mask\n",
    ")\n",
    "\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Intialize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will intialize our model to pre-train it on the data we defined. Most of the parameter values we will use here are taken form the <a href=\"https://arxiv.org/abs/1706.03762\">Attention is All You Need</a> paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum positional encoding length for input: 201\n",
      "Maximum positional encoding length for target: 63\n"
     ]
    }
   ],
   "source": [
    "#let's find the maximum length in our training data to set it as the maximum positional encoding\n",
    "POSITIONAL_ENCODING_INPUT_LENGTH =training_data_inputs_padded.shape[1]\n",
    "POSITIONAL_ENCODING_TARGET_LENGTH =training_data_targets_padded.shape[1]\n",
    "print(f\"Maximum positional encoding length for input: {POSITIONAL_ENCODING_INPUT_LENGTH}\")\n",
    "print(f\"Maximum positional encoding length for target: {POSITIONAL_ENCODING_TARGET_LENGTH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "NUM_LAYERS = 6\n",
    "EMBEDDING_DIM = 512\n",
    "FULLY_CONNECTED_DIM = 2048\n",
    "NUM_HEADS= 8\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "transformer = Transformer(\n",
    "    NUM_LAYERS,\n",
    "    EMBEDDING_DIM,\n",
    "    NUM_HEADS,\n",
    "    FULLY_CONNECTED_DIM,\n",
    "    vocab_size,\n",
    "    vocab_size,\n",
    "    POSITIONAL_ENCODING_INPUT_LENGTH,\n",
    "    POSITIONAL_ENCODING_TARGET_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if TensorFlow can access the GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have finished defining our model and processing our traning and validation datas. The next step will be training !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EMBEDDING_DIM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m         arg2 \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarmup_steps \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.5\u001b[39m)\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mrsqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim) \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mminimum(arg1, arg2)\n\u001b[0;32m---> 14\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m CustomSchedule(\u001b[43mEMBEDDING_DIM\u001b[49m)\n\u001b[1;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate, beta_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, beta_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.98\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Define metrics to track loss and accuracy\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EMBEDDING_DIM' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_object = SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embedding_dim, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = tf.cast(embedding_dim, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embedding_dim) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(EMBEDDING_DIM)\n",
    "optimizer = Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# Define metrics to track loss and accuracy\n",
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy = SparseCategoricalAccuracy(name='train_accuracy')\n",
    "val_loss = Mean(name='val_loss')\n",
    "val_accuracy = SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "# Function to calculate the loss\n",
    "def loss_function(real, pred, mask):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        real (tf.Tensor): Ground truth labels\n",
    "        pred (tf.Tensor): Model predictions\n",
    "        mask (tf.Tensor): Mask to ignore padding tokens\n",
    "    Returns:\n",
    "        loss (tf.Tensor): Computed loss\n",
    "    \"\"\"\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "# Function to create masks for the input and target sequences\n",
    "def create_masks(inp, tar):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        inp (tf.Tensor): Input sequence\n",
    "        tar (tf.Tensor): Target sequence\n",
    "    Returns:\n",
    "        enc_padding_mask (tf.Tensor): Padding mask for encoder\n",
    "        look_ahead_mask (tf.Tensor): Look-ahead mask for decoder\n",
    "        dec_padding_mask (tf.Tensor): Padding mask for decoder\n",
    "    \"\"\"\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    return enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
    "\n",
    "# Training step function\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        inp (tf.Tensor): Input sequence\n",
    "        tar (tf.Tensor): Target sequence\n",
    "    \"\"\"\n",
    "    tar_inp = tar[:, :-1]  # Shifted target input for teacher forcing\n",
    "    tar_real = tar[:, 1:]  # Actual target output\n",
    "\n",
    "    # Create masks\n",
    "    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
    "        # Compute loss\n",
    "        loss = loss_function(tar_real, predictions, create_padding_mask(tar_real))\n",
    "\n",
    "    # Compute gradients and update weights\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    # Update metrics\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)\n",
    "\n",
    "# Validation step function\n",
    "@tf.function\n",
    "def val_step(inp, tar):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        inp (tf.Tensor): Input sequence\n",
    "        tar (tf.Tensor): Target sequence\n",
    "    \"\"\"\n",
    "    tar_inp = tar[:, :-1]  # Shifted target input for teacher forcing\n",
    "    tar_real = tar[:, 1:]  # Actual target output\n",
    "\n",
    "    # Create masks\n",
    "    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    # Forward pass\n",
    "    predictions, _ = transformer(inp, tar_inp, False, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
    "    # Compute loss\n",
    "    loss = loss_function(tar_real, predictions, create_padding_mask(tar_real))\n",
    "\n",
    "    # Update metrics\n",
    "    val_loss(loss)\n",
    "    val_accuracy(tar_real, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's baby sit the training with 3 epoch each time and save the weights then continue for 3 steps and so until we reach good accuracy and low loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 22:10:12.023298: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 367.2421875, Accuracy: 0.00046641789958812296\n",
      "Epoch 1, Batch 2, Loss: 367.2628173828125, Accuracy: 0.000155472633196041\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (batch, (inp, tar)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_dataset_final):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#First training\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "EPOCHS = 1\n",
    "step=1\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset metrics at the start of each epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "\n",
    "    # Training loop\n",
    "    for (batch, (inp, tar)) in enumerate(training_dataset_final):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 2 == 0:\n",
    "            print(f'Epoch {epoch + 1}, Batch {batch}, Loss: {train_loss.result()}, Accuracy: {train_accuracy.result()}')\n",
    "\n",
    "    # Validation loop\n",
    "    for (batch, (inp, tar)) in enumerate(validation_dataset_final):\n",
    "        val_step(inp, tar)\n",
    "\n",
    "    # Append loss values to the lists\n",
    "    train_losses.append(train_loss.result())\n",
    "    val_losses.append(val_loss.result())\n",
    "\n",
    "    # Print metrics at the end of each epoch\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss.result()}, Train Accuracy: {train_accuracy.result()}')\n",
    "    print(f'Epoch {epoch + 1}, Val Loss: {val_loss.result()}, Val Accuracy: {val_accuracy.result()}')\n",
    "    transformer.save_weights(f\"model_weights/model_weights_{epoch}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig(f'loss_images/training_validation_loss{step}.png')\n",
    "\n",
    "# Optionally, display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next steps\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "EPOCHS = 1\n",
    "step=1\n",
    "transformer.load_weights(f\"model_weights/model_weights_{step-1}.h5\")\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset metrics at the start of each epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "\n",
    "    # Training loop\n",
    "    for (batch, (inp, tar)) in enumerate(training_dataset_final):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}, Batch {batch}, Loss: {train_loss.result()}, Accuracy: {train_accuracy.result()}')\n",
    "\n",
    "    # Validation loop\n",
    "    for (batch, (inp, tar)) in enumerate(validation_dataset_final):\n",
    "        val_step(inp, tar)\n",
    "\n",
    "    # Append loss values to the lists\n",
    "    train_losses.append(train_loss.result())\n",
    "    val_losses.append(val_loss.result())\n",
    "\n",
    "    # Print metrics at the end of each epoch\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss.result()}, Train Accuracy: {train_accuracy.result()}')\n",
    "    print(f'Epoch {epoch + 1}, Val Loss: {val_loss.result()}, Val Accuracy: {val_accuracy.result()}')\n",
    "    transformer.save_weights(f\"model_weights/model_weights_{epoch}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig(f'loss_images/training_validation_loss{step}.png')\n",
    "\n",
    "# Optionally, display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Fine-Tuning for Hate Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning is the process of adapting a pre-trained model to a specific task, in this case, hate speech recognition. This involves taking the pre-trained transformer model and training it further on a smaller, task-specific dataset. Fine-tuning allows the model to leverage the general language understanding it gained during pre-training while specializing in the nuances of hate speech detection.  \n",
    "\n",
    "We have pre-trained the model using **Masked Language Modeling (MLM)**, a self-supervised task that enables the model to capture semantic meanings for Amharic text. The next step is to take the checkpoints of our pre-trained model, remove the final layer, and lock all the remaining layers. We will then add new layers and train only the newly added layers on our labeled hate speech dataset.  \n",
    "\n",
    "If this approach does not yield satisfactory results, we will unlock some of the pre-trained layers and fine-tune them alongside the new layers. This allows the model to adapt more effectively to the specific task while retaining the knowledge it gained during pre-training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Preparing the Fine-Tuning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, instead of collecting and labeling a hate speech recognition dataset myself, I will utilize an existing dataset to fine-tune the model. The dataset I will use is available at [https://data.mendeley.com/datasets/ymtmxx385m/1](https://data.mendeley.com/datasets/ymtmxx385m/1). This dataset contains labeled examples of hate speech, making it suitable for fine-tuning the pre-trained model for the specific task of hate speech recognition.  \n",
    "\n",
    "By leveraging this dataset, I can save time and resources while ensuring the model is trained on high-quality, annotated data. The dataset will be preprocessed to align with the input format required by the model.\n",
    "\n",
    "This approach allows me to focus on adapting the pre-trained model to the task of hate speech detection without the overhead of data collection and labeling. The fine-tuning process will involve training the model on this dataset, evaluating its performance, and iterating as needed to achieve the desired results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset from the source is inside the folder /data/original_hate_speech_data. lets see what the datas look like and process it for our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of post datas: 30000\n",
      "Total number of label datas: 30000\n"
     ]
    }
   ],
   "source": [
    "with open(\"datas/original_hate_speech_data/Posts.txt\", \"r\") as file:\n",
    "    post_data = file.readlines()\n",
    "with open(\"datas/original_hate_speech_data/Labels.txt\", \"r\") as file:\n",
    "    labels = file.readlines()\n",
    "\n",
    "print(f\"Total number of post datas: {len(post_data)}\")\n",
    "print(f\"Total number of label datas: {len(labels)}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post 1: ріаріЋрЅх ріЦріЋріерЇЇ рѕўрѕГрїарѕЁ ріарѕЏрѕФ рѕєріљрѕЁ рЅ░рІѕрѕЇрІ░рѕђрѕЇ\n",
      "\n",
      "Label 1: Hate\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 2: рЇѓрѕІ рЅарІЮрЅирѕЇ ріЦрѕ│ рі«рѕюріЋрЅх рѕІрІГ\n",
      "\n",
      "Label 2: Hate\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 3: ріарЅЦріЋ рІЏрѕг рЅБрІѕрїБрІЇ рѕўрїЇрѕѕрїФ рІеріарЅЦрІГ ріарѕЁрѕўрІх рѕўріЋрїЇрѕхрЅх рІФрѕѕріарїЇрЅБрЅЦ рІФрѕ░рѕФрЅИрІЇріЋ ріарѕўрѕФрѕ«рЅ╣ріЋ ріЦріЊ ріарЅБрѕІрЅХрЅ╣ріЋ ріЦріЋрІ▓рѕѕрЅЁрЅЁ рїарІГрЅІрѕЇ\n",
      "\n",
      "Label 3: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 4: ріерЅарїЇ рІЮрѕГрЇЇрІФ рІѕрІ░ рЅ│рѕфріГ рІўрѕерЇІ\n",
      "\n",
      "Label 4: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 5: ріарѕЏрѕФ рІерібрЅхрІ«рї▓рІФ рѕ▓рѕЇ ріљрІЇ рѕџрІФрѕЮрѕГрЅарЅх\n",
      "\n",
      "Label 5: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 6: ріИрѕе рѕ╝рѕЮ ріљрІЇ рІерЅёрѕ│рѕГріЋ рѕѕрЅёрѕ│рѕГ\n",
      "\n",
      "Label 6: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 7: ріГрѕГрѕхрЅ▓рѕх рѕ░рѕЁрѕѕріљріГрѕГрѕхрЅХрѕх рѕўрѕђрѕеріЊрІГрЅЁрѕГ рЅарѕѕріЊ\n",
      "\n",
      "Label 7: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 8: рІѕрїѕріЋ ріЦріЋрІ┤рЅх ріЊрЅйрѕЂ ріарІ▓рѕх рѕЁрІГрІѕрЅх ріЦріЊ рібріЋрЅФрІГрѕ«ріЋрѕўріЋрЅх рІерѕўрѕѕрѕЏрѕўрІх рѕЂріћрЅ│ рѕІрІГ ріљрЅарѕГріЕ ріљрїѕрѕ«рЅйріЋ ріарѕхрЅ░ріФріГрІе рЅЦрЅЁ рЅЦрІФрѕѕрѕЂ\n",
      "\n",
      "Label 8: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 9: ріерЅхрЅѓрЅх рІѕрѕФрЅХрЅй рЅарЇірЅх рІеріарѕЏрѕФ рѕЁрІЮрЅЦ ріЦріЊ рЇќрѕѕрЅ▓ріеріЏ рЅхрѕЇрЅЂ рѕхрѕФрІЅ рЅарЅБрѕѕрЇЅрЅх  ріарѕўрЅ│рЅх рІерЅ░рїѕріљрЅА рІерЇќрѕѕрЅ▓ріФ ріЦріЊ рІерібрі«ріќрѕџ рЅ░рЅІрѕЏрЅх     рЅарѕЏрЇѕрѕФрѕерѕх ріарІ│рІ▓рѕх рЅ░рЅІрѕЏрЅхріЋ рІеріарѕЏрѕФріЋ рѕЁрІЮрЅЦрѕЮ рѕєріљ рѕїрѕІрІЅріЋ рЅаріЦріЕрѕЇріљрЅх рЅарѕџрїарЅЁрѕЎ ріЦріЊ рЅарѕџрІФрѕхрЅ░ріЊрїЇрІ▒рЅарЅх рѕўріЋрїѕрІх рѕўрїѕріЋрЅБрЅх ріЦріЋрІ│рѕѕрЅБрЅИрІЅ ріЦріЊ рЅхрѕЇрЅЂ рІерЅцрЅх рѕхрѕФрЅйріЋ рІГрѕЁ рѕўрѕєріЉріЋ рЅарѕ░рЇірІЅ рїѕрѕЇрїерІІрѕѕрІЅ\n",
      "\n",
      "Label 9: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 10:  рІеріарѕЏрѕФріЋ рѕЁрІЮрЅЦ рЅ░рЅІрѕЏрЅХрЅй ріљрІЅ рІерїјрІ▒рЅх рЅарІѕрЅЁрЅ▒ рЇЊрІѕрѕГ рІеріљрЅарѕерІЅ рЅАрІхріЋ рЅ░рЅІрѕЏрЅХрЅйріЋ рѕ▓рїѕріљрЅБ рЅарѕЏріЋріЏрІЅрѕЮ рѕўріЋрїѕрІх рІеріарѕЏрѕФріЋ рѕЁрІЮрЅЦ ріГрЇЅріЏ рЅарібрі«ріќрѕџ рЅарЇќрѕѕрЅ▓ріФ ріЦріЊ рѕїрѕјрЅй рѕЏрѕЁрЅарѕФрІі рІўрѕГрЇјрЅй рЅарѕџрІФрѕйрѕўрІ░рѕЮрІх рѕўрѕЇріЕ ріљрІЅ \n",
      "\n",
      "Label 10: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start=1800\n",
    "end=1810\n",
    "sample_post_data = post_data[start:end]\n",
    "sample_label_data = labels[start:end]\n",
    "for i,(post,label) in enumerate(zip(sample_post_data,sample_label_data)):\n",
    "    print(f\"Post {i+1}: {post}\")\n",
    "    print(f\"Label {i+1}: {label}\")\n",
    "    print(\"\\n---------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the labels into binary. 1 mean Free and 0 mean hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All labels either hate or free\n"
     ]
    }
   ],
   "source": [
    "#first lates make sure the labels are only Hate and Free\n",
    "labels_array=[label.strip() for label in labels]\n",
    "total_labels=0\n",
    "for label in labels_array:\n",
    "    if label.lower() in [\"hate\",\"free\"]:\n",
    "        total_labels+=1\n",
    "if total_labels==len(labels):\n",
    "    print(\"All labels either hate or free\")\n",
    "else:\n",
    "    print(\"Not all labels are hate or free, the total number of correct labels is: \",total_labels)\n",
    "    print(\"Please check the labels\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let as change the labels into binary and save. Uncomment the cell only if labels_binary.txt data is not available inside datas/original_hate_speech_data. Or you can  delete the file labels_binary.txt inside datas/original_hate_speech_data, uncomment the code and run it. Otherwise it will duplicate the labels data  2 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for label in labels_array:\n",
    "#     with open(\"/Users/mahder/Real Projects/Mahder AI/Training_models/Transformer_classifier_app_1/datas/original_hate_speech_data/labels_binary.txt\", \"a\") as file:\n",
    "#         if label.lower()==\"hate\":\n",
    "#             file.write(\"1\\n\")\n",
    "#         else:\n",
    "#             file.write(\"0\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All labels are correctly changed into binary\n"
     ]
    }
   ],
   "source": [
    "# let's check if it is correctly changed into binary\n",
    "with open(\"/Users/mahder/Real Projects/Mahder AI/Training_models/Transformer_classifier_app_1/datas/original_hate_speech_data/labels_binary.txt\", \"r\") as file:\n",
    "    binary_labels = file.readlines()\n",
    "binary_labels_array=[int(label.strip()) for label in binary_labels]\n",
    "for binary, label in zip(binary_labels_array,labels_array):\n",
    "    if label.lower()==\"hate\":\n",
    "        assert binary==1\n",
    "    else:\n",
    "        assert binary==0\n",
    "print(\"All labels are correctly changed into binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the datas into a form suitable for our model and tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#tokenize the post data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m post_data_array\u001b[38;5;241m=\u001b[39m[post\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m post \u001b[38;5;129;01min\u001b[39;00m post_data]\n\u001b[0;32m----> 3\u001b[0m tokenized_hate_speech_data\u001b[38;5;241m=\u001b[39m[tokenizer\u001b[38;5;241m.\u001b[39mtokenize(post) \u001b[38;5;28;01mfor\u001b[39;00m post \u001b[38;5;129;01min\u001b[39;00m post_data_array]\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#tokenize the post data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m post_data_array\u001b[38;5;241m=\u001b[39m[post\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m post \u001b[38;5;129;01min\u001b[39;00m post_data]\n\u001b[0;32m----> 3\u001b[0m tokenized_hate_speech_data\u001b[38;5;241m=\u001b[39m[\u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize(post) \u001b[38;5;28;01mfor\u001b[39;00m post \u001b[38;5;129;01min\u001b[39;00m post_data_array]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#tokenize the post data\n",
    "post_data_array=[post.strip() for post in post_data]\n",
    "tokenized_hate_speech_data=[tokenizer.tokenize(post) for post in post_data_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique data: 25097\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFC', text.strip())\n",
    "\n",
    "# Initialize a set to store unique data\n",
    "unique_data = set()\n",
    "\n",
    "# Iterate through the data array\n",
    "for data in post_data_array:\n",
    "    normalized_data = normalize_text(data)  # Normalize the text\n",
    "    if normalized_data not in unique_data:  # Check if the normalized data is not empty\n",
    "        unique_data.add(normalized_data)  # Add to the set if not already present\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Print the total number of unique data\n",
    "print(f\"Total number of unique data: {len(unique_data)}\")\n",
    "for data in list(unique_data):\n",
    "    with open(\"unique.txt\", \"a\") as file:\n",
    "        file.write(data+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original post data: \n",
      "\n",
      " ріарѕЏрѕГріЏріЋ ріЦріЊ ріарѕЏрѕФрІЇ рІерѕџрЅБрѕѕрІЇ рЅарІхрѕ« рїірІю рѕЮріЋ ріарїѕріЊріўрІЇ\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Tokenized post data: \n",
      "\n",
      " [1649, 5, 7, 745, 22, 3638, 45221, 38, 107, 81333, 22]\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Tokenized data as subwords: \n",
      "\n",
      " ['РќЂріарѕЏрѕГріЏ', 'ріЋ', 'РќЂріЦріЊ', 'РќЂріарѕЏрѕФ', 'рІЇ', 'РќЂрІерѕџрЅБрѕѕрІЇ', 'РќЂрЅарІхрѕ«', 'РќЂрїірІю', 'РќЂрѕЮріЋ', 'РќЂріарїѕріЊріў', 'рІЇ']\n"
     ]
    }
   ],
   "source": [
    "#let's visialize what the tokenization looks like\n",
    "print(\"Original post data: \\n\\n\", post_data_array[1742])\n",
    "print(\"\\n---------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"Tokenized post data: \\n\\n\", tokenized_hate_speech_data[1742])\n",
    "print(\"\\n---------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"Tokenized data as subwords: \\n\\n\", tokenizer.id_to_piece(tokenized_hate_speech_data[1742]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_hate_speech_data_padded=tf.keras.utils.pad_sequences(\n",
    "    tokenized_hate_speech_data,\n",
    "    maxlen=200,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_hate_final_input=tokenized_hate_speech_data_padded[:28000]\n",
    "training_hate_final_output=binary_labels_array[:28000]\n",
    "validation_hate_final_input=tokenized_hate_speech_data_padded[28000:]\n",
    "validation_hate_final_output=binary_labels_array[28000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "HATE_BUFFER_SIZE = 3000\n",
    "HATE_BATCH_SIZE = 32\n",
    "training_final=tf.data.Dataset.from_tensor_slices((training_hate_final_input,training_hate_final_output))\n",
    "validation_final=tf.data.Dataset.from_tensor_slices((validation_hate_final_input,validation_hate_final_output))\n",
    "training_final_dataset=training_final.shuffle(HATE_BUFFER_SIZE).batch(HATE_BATCH_SIZE).prefetch(tf.data.AUTOTUNE).cache()\n",
    "validation_final_dataset=validation_final.batch(HATE_BATCH_SIZE).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Adjusting the Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre trained model is inside /Users/mahder/Real Projects/Mahder AI/Training_models/Transformer_classifier_app_1/model_weights  we will load it and fine tune it in on the above processed dataset. The pretrained model has both encoder and decoder. but for this binary classification task we need only the ecncoder part so we need to take only the encoder part of the transformer and fine tune it ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "NUM_LAYERS = 6\n",
    "EMBEDDING_DIM = 512\n",
    "FULLY_CONNECTED_DIM = 2048\n",
    "NUM_HEADS= 8\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "transformer_pretrained = Transformer(\n",
    "    NUM_LAYERS,\n",
    "    EMBEDDING_DIM,\n",
    "    NUM_HEADS,\n",
    "    FULLY_CONNECTED_DIM,\n",
    "    vocab_size,\n",
    "    vocab_size,\n",
    "    POSITIONAL_ENCODING_INPUT_LENGTH,\n",
    "    POSITIONAL_ENCODING_TARGET_LENGTH,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_1 (Encoder)         multiple                  114219008 \n",
      "                                                                 \n",
      " decoder_1 (Decoder)         multiple                  164633600 \n",
      "                                                                 \n",
      " dense_49 (Dense)            multiple                  51300000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 330152608 (1.23 GB)\n",
      "Trainable params: 330152608 (1.23 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build the model for the hate speech classification\n",
    "sentence_a = np.array([[2, 3, 1, 3, 0, 0, 0]])\n",
    "sentence_b = np.array([[1, 3, 4, 0, 0, 0, 0]])\n",
    "\n",
    "enc_padding_mask = create_padding_mask(sentence_a)\n",
    "dec_padding_mask = create_padding_mask(sentence_a)\n",
    "\n",
    "look_ahead_mask = create_look_ahead_mask(sentence_a.shape[1])\n",
    "\n",
    "test_summary, att_weights = transformer_pretrained(\n",
    "    sentence_a,\n",
    "    sentence_b,\n",
    "    False,\n",
    "    enc_padding_mask,\n",
    "    look_ahead_mask,\n",
    "    dec_padding_mask\n",
    ")\n",
    "\n",
    "transformer_pretrained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pretrained model weights\n",
    "transformer_pretrained.load_weights(\"/Users/mahder/Real Projects/Mahder AI/Training_models/Transformer_classifier_app_1/model_weights/model_weights_version4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForBinaryClassification(tf.keras.Model):\n",
    "    def __init__(self, transformer, dropout_rate=0.1):\n",
    "        super(TransformerForBinaryClassification, self).__init__()\n",
    "        self.encoder = transformer.encoder\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.classifier = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, input_sentence, training):\n",
    "        # Create padding mask inside the call method\n",
    "        enc_padding_mask = create_padding_mask(input_sentence)\n",
    "        \n",
    "        # Pass the input through the encoder\n",
    "        enc_output = self.encoder(input_sentence, training, enc_padding_mask)\n",
    "        \n",
    "        # Extract the [CLS] token representation\n",
    "        cls_output = enc_output[:, 0, :]\n",
    "        \n",
    "        # Apply dropout\n",
    "        cls_output = self.dropout(cls_output, training=training)\n",
    "        \n",
    "        # Pass through the classification head\n",
    "        final_output = self.classifier(cls_output)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the binary classification model\n",
    "binary_classifier = TransformerForBinaryClassification(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 12:31:32.686224: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9/875 [..............................] - ETA: 1:23:50 - loss: 0.5618 - accuracy: 0.7674"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m binary_classifier\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      2\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-7\u001b[39m),\n\u001b[1;32m      3\u001b[0m     loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(),\n\u001b[1;32m      4\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_final_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_final_dataset\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m binary_classifier\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/mahder/Real Projects/Mahder AI/Training_models/Transformer_classifier_app_1/model_weights/finetune_model_weights/binary_classifier_weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "binary_classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-7),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# Fine-tune the model\n",
    "history = binary_classifier.fit(\n",
    "    training_final_dataset,\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    validation_data=validation_final_dataset\n",
    ")\n",
    "binary_classifier.save_weights(\"/Users/mahder/Real Projects/Mahder AI/Training_models/Transformer_classifier_app_1/model_weights/finetune_model_weights/binary_classifier_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MlnewEnv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
