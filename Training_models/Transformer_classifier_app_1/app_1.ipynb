{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1><b>Pretraining a Transformer Model from Scratch on an Amharic Dataset and Fine-Tuning for Amharic Hate Speech Recognition Task</b></h1>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents  \n",
    "1. [Introduction](#introduction)  \n",
    "2. [Importing Packages](#importing-packages)  \n",
    "3. [Dataset Collection & Preprocessing](#dataset-collection--preprocessing)  \n",
    "   - 3.1 [Data Collection](#data-collection)  \n",
    "   - 3.2 [Data Cleaning](#data-cleaning)  \n",
    "   - 3.3 [Tokenization](#tokenization)  \n",
    "   - 3.4 [Tokenizing and Masking](#tokenizing-and-masking)  \n",
    "4. [Pretraining the Transformer Model](#pretraining-the-transformer-model)  \n",
    "5. [Fine-Tuning for Hate Speech Recognition](#fine-tuning-for-hate-speech-recognition)  \n",
    "6. [Evaluation](#evaluation)  \n",
    "7. [Deployment on Mahder AI App](#deployment-on-mahder-ai-app)  \n",
    "8. [Conclusion](#conclusion)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction  \n",
    "\n",
    "In this notebook, I will pretrain a Transformer network on an Amharic dataset collected from a variety of Telegram channels, using the Masked Language Model (MLM). The primary objective of pretraining is to enable the model to learn contextualized word and phrase representations, thereby enhancing its understanding of language semantics. The Transformer’s self-attention mechanism plays a crucial role by allowing the model to dynamically weigh different parts of the input sequence, effectively capturing long-range dependencies in the data.  \n",
    "\n",
    "After pretraining, I will fine-tune the model on a labeled dataset of hate speech and deploy the resulting model in the **Mahder AI** app.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Importing the Packages\n",
    "\n",
    "Let's start by importing all the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import emoji\n",
    "import sentencepiece as spm\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Collection & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Collection  \n",
    "\n",
    "In order to pretrain the Transformer network from scratch, we will use **self-supervised learning**, which requires a large corpus of unlabeled text. We will apply a **Masked Language Model (MLM)** to pre-train the model.  \n",
    "\n",
    "#### **Why Telegram Channels?**  \n",
    "Telegram is the most widely used platform for information storage in Ethiopia. For this reason, I have chosen **Telegram channels** as the primary data source. Most of the selected channels are news channels, ensuring a diverse and rich dataset.  \n",
    "\n",
    "#### **Data Collection Method**  \n",
    "To collect the data, I used the **Telethon Python library** and the **Telegram API** to scrape text from selected channels.  \n",
    "\n",
    "#### **Selected Telegram Channels**  \n",
    "The dataset has been collected from the following Telegram channels:  \n",
    "\n",
    "- [Tikvah Ethiopia](https://t.me/tikvahethiopia)  \n",
    "- [Addis Standard Amharic](https://t.me/AddisstandardAmh)  \n",
    "- [Tarikn Wedehuala](https://t.me/TariknWedehuala)  \n",
    "- [Addis News](https://t.me/Addis_News)  \n",
    "- [Zena 24 Now](https://t.me/zena24now)  \n",
    "- [Tikvah University](https://t.me/TikvahUniversity)  \n",
    "- [Tikvah Ethiopia Magazine](https://t.me/tikvahethmagazine)  \n",
    "- [Tikvah Ethiopia Sport](https://t.me/tikvahethsport)  \n",
    "- [Philosophy Thoughts](https://t.me/Philosophy_Thoughts1)  \n",
    "- [Mudenyaz](https://t.me/Mudenyaz)  \n",
    "- [Yemeri Terekoch](https://t.me/yemeri_terekoch)  \n",
    "- [Bemnet Library](https://t.me/Bemnet_Library)  \n",
    "- [Amazing Fact](https://t.me/amazing_fact_433)  \n",
    "- [Zephilosophy](https://t.me/Zephilosophy)  \n",
    "- [Huluezih](https://t.me/huluezih)  \n",
    "\n",
    "#### **Accessing Collected Data**  \n",
    "To access the code and all the raw data collected from each channel, visit the following GitHub repository:  \n",
    "[GitHub Repository Link](https://github.com/your-repo-link-here).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a function that will load data from a JSON file as an array of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    return_data=[]\n",
    "    with open (filepath, \"r\") as file:\n",
    "        datas=json.load(file)\n",
    "        for data in datas:\n",
    "            return_data.append(data[\"text\"])\n",
    "            \n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows how the news look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#መቄዶንያ\n",
      "\n",
      "ሰውን ለመርዳት ሰው መሆን በቂ ነው !\n",
      "\n",
      "ትላንት የካቲት 1/2017 ዓ/ም በጀመረው የመቄዶንያ የአረጋዊያን እና የአእምሮ ህሙማን መርጃ ማዕከል የድጋፍ ማሰባሰብ ዘመቻ እስኩን 120,000,000 ብር ተሰብስቧል።\n",
      "\n",
      "መቄዶንያ በሚያስገነባው ሆስፒታል ጭምር ያለው ህንፃ ለማጠናቀቅ የገንዘብ እጥረት አጋጥሞታል። ህንፃው ለማጠናቀቅ ገንዘብ ተቸግረናል። ለማጠናቀቅ ወደ 5 ቢሊዮን ብር ያስፈልጋል።\n",
      "\n",
      "በቀጥታ ይከታተሉ 👇\n",
      "https://www.youtube.com/live/q0bMjwt9PvM?feature=shared\n",
      "\n",
      "የምትችሉትን ሁሉ ድጋፍ አድርጉ።\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "🔊 #የሠራተኞችድምጽ\n",
      "\n",
      "\" ቋሚ ሠራተኞች ሆነን ሳለ በደሞዝ ማሻሻያው አልተካተትንም \" - የሀዋሳ ዙሪያ ወረዳ መንግስት ሠራተኞች\n",
      "\n",
      "የማክሮ ኢኮኖሚ ማሻሻያ ሪፎርሙን ተከትሎ የሚከሰቱ የኑሮ ዉድነትና ተያያዥ ጉዳዮችን ታሳቢ በማድረግ የመንግስት ሠራተኞች ደሞዝ ማሻሻያ ተደርጎ ከጥቅምት ወር 2017 ዓ/ም ጀምሮ ተግባራዊ የተደረገ መሆኑ ይታወቃል።\n",
      "\n",
      "በሲዳማ ክልል፤ ሰሜናዊ ሲዳማ ዞን፤ ሀዋሳ ዙሪያ ወረዳ በተለያዩ የመንግስት መስሪያ ቤቶች የሚሰሩ የመንግስት ሠራተኞች ግን \" ከ2012 ዓ/ም ጀምሮ በቋሚነት ተቀጥረን እየሰራን ያለን ቢሆንም በአዲሱ የመንግስት ሠራተኞች የደመወዝ ማሻሻያ አልተካተትንም \" ሲሉ ቅሬታቸዉን ለቲክቫህ ኢትዮጵያ አስገብተዋል።\n",
      "\n",
      "ቅሬታቸዉን ካደረሱን መካከል ፦\n",
      "- በከተማ ልማትና ኮንስትራክሽን፣\n",
      "- ማዘጋጃ ቤቶች፣\n",
      "- በትምህርት ዘርፍ ፣\n",
      "- በሴቶችና ሕፃናት እንዲሁም በሕብረት ስራ ጽ/ቤቶች የሚሰሩ ሠራተኞች ናቸው።\n",
      "\n",
      "\" በወቅቱ በአግባቡ ማስታወቂያ ወጥቶ ተመዝግበንና ተወዳድረን ማለፋችን ተረጋግጦ የቋሚነት ደብዳቤ ተሰጥቶን ላለፉት አምስትና ስድስት ዓመታት ደሞዝ ሲከፈለን በቆየንባቸው መደቦች ላይ እየሰራን ባለንበት በአዲሱ የደሞዝ ማሻሻያ አለመካተታችን ለዘርፈ ብዙ ችግሮች ዳርጎናል \" ብለዋል።\n",
      "\n",
      "\" ለወረዳዉ ፐብሊክ ሰርቪስና የሰዉ ሃብት ልማት ጽ/ቤት እና ለክልሉ ፐብሊክ ሰርቪስ ቢሮ ቅሬታችንን በአካልና በፅሁፍ ብናቀርብም ተገቢዉ ምላሽ አልተሰጠንም ጉዳዩን ለኢትዮጵያ እምባ ጠባቂ ተቋም ለማቅረብ መረጃ እያደራጀን ነው \" ሲሉ ተናግረዋል።\n",
      "\n",
      "ቃላቸውን ለቲክቫህ ኢትዮጵያ የሰጡት ፤ የሀዋሳ ዙሪያ ወረዳ ፐብሊክ ሰርቪስ እና የሰዉ ሃብት ልማት ጽ/ቤት ኃላፊ አቶ ሃይሉ አቢኖ ፥ \" በወረዳዉ በ2012 ዓ/ም የነበረው አግባብነት በሌለው ቅጥር በአንድ መደብ ሶስትና አራት ሰዎችን በተደራራቢነት የመቅጠር ሁኔታዎች አሁን ለተፈጠረው ችግር ዋነኛ ምክንያት ሆኗል \" ሲሉ ገልጸዋል።\n",
      "\n",
      "ከዞኑና የክልሉ ፐብልክ ሰርቪስ ጋር በመናበብ መፍትሔ እያፈላለጉ ስለመሆኑም ጠቁመዋል።\n",
      "\n",
      "በወቅቱ ይህን ተግባር የፈፀሙ አመራሮች እና የሰዉ ሃብት ልማት ኃላፊዎች ላይ እርምጃ መወሰዱን የሚናገሩት ኃላፊዉ በወረዳዉ በዚህ መልክ ተጠቀጥረዉ በአዲሱ የደመወዝ ማሻሻያ ያልካተቱና በቀጣይ መፍትሔ የሚፈለግላቸዉ 470 በተለያዩ መስሪያ ቤቶች ዉስጥ የተለዩ ሰራተኞች ስለመኖራቸዉ አክለዋል።\n",
      "\n",
      "የሰሜናዊ ሲዳማ ዞን ፐብልክ ሰርቪስና የሰዉ ሃይል ልማት መምሪያ ኃላፊ አቶ በዛብህ ባርሶ በበኩላቸው በ2011 እና 2012 በአከባቢው ሕገወጥ ቅጥሮች መፈፀማቸውን ገልጸዋል።\n",
      "\n",
      "በወረዳዉ አጣሪ ቡድን ተቋቁሞ በአዲሱ ደሞዝ ያልተካተቱንና በወረዳው ቅጥር ያልተፈፀመባቸዉ ክፍት መደቦችን የመለየት ስራ መከናወኑን አንስተዉ በሀዋሳ ዙሪያ ወረዳ ብቻ 407 ክፍት መደቦች መኖራቸዉን ለማወቅ መቻሉን ገልፀዋል።\n",
      "\n",
      "የክልሉ የበላይ አመራሮች በሚያስቀምጡት አቅጣጫ መሰረት እነዚህን ሠራተኞች በነዚህ ክፍት መደቦች የመደልደልና ሌሎችም ሕጋዊ አመራጮች በመፈለግ በአጭር ጊዜ ዉስጥ እልባት ለመስጠት እየተሰራ መሆኑን አስታውቀዋል።\n",
      "\n",
      "ቲክቫህ ኢትዮጵያ ጉዳዩን እስከመጨረሻ ተከታትሎ መረጃውን ይልካል።\n",
      "\n",
      "#TikvahEthiopiaFamilyHW\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "የIMF ማኔጂንግ ዳይሬክተሯ ምን አሉ ?\n",
      "\n",
      "የዓለም አቀፍ የገንዘብ ተቋም (IMF) ማኔጂንግ ዳይሬክተር ክሪስታሊና ጆርጂዬቫ በኢትዮጵያ የስራ ቆይታ አድርገዋል።\n",
      "\n",
      "በዚህም ወቅት ከጠ/ሚ ዐቢይ አህመድ (ዶ/ር) ጋር ጨምሮ ከፌዴራል ከፍተኛ ባለስልጣናት ጋር መክረዋል።\n",
      "\n",
      "የነበራቸውን ቆይታ በተመለከተ ከገንዘብ ሚኒስትሩ አቶ አህመድ ሽዴ ጋር በጋራ መግለጫ ሰጥተው ነበር።\n",
      "\n",
      "ምን አሉ ?\n",
      "\n",
      "ዳይሬክተሯ ፤ \" የኢትዮጵያ ሪፎርም ከባድ እና ጊዜ የሚወስድ ነው ፤ እባካችሁ ታገሱ \" የሚል ጥሪ አቅርበዋል።\n",
      "\n",
      "ኢትዮጵያውያን ለትዕግስት እንዲያሳዩ እና ከመንግስት የኢኮኖሚ ማሻሻያ ጥረቶች ጎን እንዲቆሙ ጠይቀዋል።\n",
      "\n",
      "ጆርጂዬቫ ፥ \" የሪፎርሙን ግቦች ለማሳካት የአንድነት አስፈላጊ ነው \" ሲሉ አፅንኦት ሰጥተዋል።\n",
      "\n",
      "\" ኢትዮጵያ የተቀበለችው ሪፎርም ከባድ እና ጊዜ የሚወስድ ቢሆንም እጅግ ትልቅ ውጤት ያስገኛል \" ሲሉ ተናግረዋል።\n",
      "\n",
      "\" ህዝቡ በትዕግስት እንዲጠብቅ ጥሪዬን አቀርባለሁ \" ያሉት ማኔጂንግ ዳይሬክተሯ \" ህብረተሰቡ ከሪፎርሙ ጀርባ በመሰባሰብ በአንድነት ድጋፍ ማድረግ አለበት \" ብለዋል።\n",
      "\n",
      "ጆርጂዬቫ ፥ ኢኮኖሚውን የበለጠ አጥጋቢና ብቁ ለማድረግ ብዙ የሚሠራ ሥራ አለ \" ብለው \" እባካችሁ መንግሥት ሥራውን እንዲያጠናቅቅ ድጋፍ አድርጉ \" የሚል ጥሪ አቅርበዋል።\n",
      "\n",
      "የዋጋ ንረትን ለመፍታት የሚሰራው ስራ ውስብስብ መሆኑን ያልሸሸጉት ዳይሬክተራ \" የዋጋ ንረትን ወደ ታች ለማውረድ ጠንካራ የገንዘብና የፊስካል ፖሊሲዎች፣ የኢኮኖሚውን የማምረት አቅም ማስፋት፣ የወጪ ንግድና የውጭ ምንዛሪ ገቢን ማሳደግ እና የግሉ ሴክተርን ማብቃት ይጠይቃል \" ብለዋል።\n",
      "\n",
      "ሌላው ያነሱት ጉዳይ በG20 የጋራ ማዕቀፍ ኢትዮጵያ እያካሄደች ያለችውን የዕዳ መልሶ ማደራጀት ድርድር በተመለከተ ነው።\n",
      "\n",
      "ጆርጂዬቫ ፤ \" የዕዳ መልሶ ማዋቀር ሂደት የመጨረሻ ደረጃ ላይ ይገኛል ፤ ከኢትዮጵያ አበዳሪዎች ጋር ባለኝ ግንኙነት ይህ ቅድሚያ የሚሰጠው ጉዳይ ነው \" ሲሉ ገልጸዋል። \n",
      "\n",
      "የIMF ፕሮግራም አካል ሆነውን የታክስ እርምጃዎችን በተመለከተም ፤ የኢትዮጵያ ባለስልጣናት ለብሄራዊ በጀቱ ድጋፍ ለማድረግ ወሳኝ የሆኑ የታክስ አቅሞችን መለየታቸውን ጠቁመዋል። \n",
      "\n",
      "ጆርጂዬቫ ፥ የኢትዮጵያ አጠቃላይ የሀገር ውስጥ ምርት ዕድገት ከIMF የመጀመሪያ ትንበያዎች መብለጡን ማብራራታቸውን ዘሪፖርተር አስነብቧል።\n",
      "\n",
      "የማኔጂንግ ዳይሬክተራ ንግግር ተከትሎ \" መሬት ላይ ካለው እውነታ ጋር የሚገናኝ አይደለም \" የሚሉ አስተያየቶች ሲሰጡም ተመልክተናል።\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "ልታስመርቀዉ ከቤተሰቦቿ ተደብቃ የመጣች እጮኛዉን ጭካኔ በተሞላበት ሁኔታ የገደለዉ ግለሰብ በ20 ዓመት ጽኑ እስራት ተቀጣ።\n",
      "\n",
      "በደቡብ ኢትዮጵያ ክልል ጋሞ ዞን አርባምንጭ ከተማ አስተዳደር ጉርባ በሚባል ቀበሌ የገዛ እጮኛዉን ጭካኔ በተሞላበት መልኩ በአሰቃቂ ሁኔታ በስለት አንገቷን በመቁረጥ ሕይወቷ እንዲያልፍ ያደረገዉ ወጣት በፅኑ እስራት መቀጣቱን የአርባ ምንጭ ከተማ አስተዳደር ፖሊስ መምሪያ አዛዥ ም/ኢንስፔክተር ጋፋሮ ቶማስ ለቲክቫህ ኢትዮጵያ ተናግረዋል።\n",
      "\n",
      "የወንጀል ድርጊቱ የተፈፀመዉ ነሐሴ 16/2016 ዓ/ም በአርባምንጭ ከተማ ጉርባ ቀበሌ ነው።\n",
      "\n",
      "ተከሳሽ ዮናስ ጫፊቄ የተባለው ግለሰብ የጂንካ ዩኒቨርሲቲ 1ኛ  ዓመት ተማሪ የሆነችዉን ሟች ሊዲያ ዮሐንስ እሱን ለማስመረቅ ወደ አርባ ምንጭ ከተማ በመጣችበት ተከራይቶ በሚኖርበት ቤት አሰቃቂ ድርጊቱን መፈፀሙን የምርመራ መዝገቡ ያስረዳል።\n",
      "\n",
      "ወጣቷ \" እጮኛዬ ይመረቅልኛል \" በሚል ደስታ ከቤተሰቦቿ ተደብቃ ተከሳሽ ተከራይቶ ወደ ሚማርበት  ቤት መጥታ በዋዜማዉ ለምረቃዉ የሚሆኑ የዲኮር፣ የዳቦና ለስላሳ መጠጦችና በቡና ዝግጅት ቤቱን አሰማምራ በምሽቱም ግቢ ዉስጥ ያሉ ተከራዮችን ጠርተዉ ከሸኙ በኋላ ሟች ሀገር ሰላም ብላ በተኛችበት ከሌሊቱ 7 ሰዓት ገደማ እራሷን መከላከል በማትችልበት ሁኔታ በቢላዋ አንገቷን አርዶ መግደሉን የምርመራ መዝገቡን ዋቢ አድርገው ፖሊስ አዛዡ ገልፀዋል።\n",
      "\n",
      "ፖሊስ አዛዡ አክለው እንደገለጹት ፥ በወቅቱ በተደረገዉ ማጣራትም ሆነ በክስ መዝገቡ ላይ እንደሰፈረዉ ወንጀለኛው  \" ወደ ዩኒቨርሲቲ በሄድሽበት ሌላ የወንድ ጓደኛ ይዘሻል \" በሚል ነው በር ዘግቶ አሰቃቂ የወንጀል ድርጊቱን የፈጸመው።\n",
      "\n",
      "ፖሊስ በዚህ ዘግናኝ ወንጀል ዙሪያ ተገቢዉን ማጣራትና ምርመራ አድርጎ ለዐቃቤ ሕግ ማቅረቡን አስታውቀዋል።\n",
      "\n",
      "ዐቃቤ ሕግም ክስ በመመስረት ለፍርድ ቤቱ ተከሳሽ የወንጀል ድርጊቱን መፈፀሙን የሰዉ፣ የሰነድና የህክምና ማስረጃዎችን በማቅረብ አስረድቷል።\n",
      "\n",
      "በቀረቡ ማስረጃዎች እና ምስክሮች ግራ ቀኙን ሲያጣራ የቆየዉ የጋሞ ዞን ከፍተኛዉ ፍርድ ቤት በቀን 29/5/2017 ዓም በዋለዉ ችሎት ተከሳሽ ዮናስ ጨፊቄ በተከሰሰበት በአሰቃቂ ሁኔታ ነብስ የማጥፋት ወንጀል ጥፋተኛ መሆኑን በማረጋገጥ በ20 ዓመት ፅኑ እስራት እንዲቀጣ መወሰኑንም ኢንስፔክተር ጋፋሮ ቶማስ ለቲክቫህ ኢትዮጵያ ተናግረዋል።\n",
      "\n",
      "#TikvahEthiopiaFamilyHW\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "#መቄዶንያ\n",
      "\n",
      "\" ሰውን ለመርዳት ሰው መሆን በቂ ነው !! \"\n",
      "\n",
      "መቄዶንያ የአረጋዊያንና የአእምሮ ህሙማን መርጃ ማዕከል በሚያስገነባው ሆስፒታል ጭምር ያለው ህንፃ ለማጠናቀቅ የገንዘብ እጥረት ስለገጠመው እግዛ እንዲደረግ ለመላው ኢትዮጵያውያን ጥሪ መቅረቡ ይታወሳል።\n",
      "\n",
      "ዛሬ “ በሰይፉ ኢቢኤስ የዩቱብ ቻነል ” ድጋፍ ማድረጊያ መርሀ ግብር እየተካሄደ ነው።\n",
      "\n",
      "ደጋጎች ሁሉ ድጋፍ እንድታደርጉ ጥሪ እናቀርባለን።\n",
      "\n",
      "የድጋፍ ማድረጊያ አማራጮች ከላይ ተያይዘዋል።\n",
      "\n",
      "መቄዶንያ “ ህንፃው ለማጠናቀቅ ገንዘብ ተቸግረናል። ለማጠናቀቅ ወደ 5 ቢሊዮን ብር ያስፈልገናል ” ማለቱ አይዘነጋም።\n",
      "\n",
      "በቀጥታ ይከታተሉ 👇\n",
      "https://www.youtube.com/live/q0bMjwt9PvM?feature=shared\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample=load_data(\"datas/sample.json\")\n",
    "for news in sample[:5]:\n",
    "    print(news)\n",
    "    \n",
    "    print(\"---------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with a Telegram dataset, we aim to clean the text by removing substrings that are commonly used on the platform, such as hashtagged entities, usernames, hyperlinks, and emojis. To achieve this, we will use Python's re library to perform regular expression operations. We will define specific search patterns and use the sub() method to remove matches by replacing them with an empty string ('').\n",
    "\n",
    "However, we will retain some rarely occurring English words. This decision is based on the observation that Amharic texts on social media are often mixed with English words, and completely pure Amharic text is difficult to find in informal digital communication. These rarely occurring English words often carry meaningful context, so preserving them ensures that the cleaned dataset remains representative of real-world usage while still achieving our goal of removing platform-specific clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    text=re.sub(r'@\\S+', '', text)\n",
    "    text=emoji.replace_emoji(text,\" \")\n",
    "   \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the above function on sample data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's load our training data and see how many contents we have and what the first 5 contents look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of contents: 193419\n",
      "\n",
      "First 5 contents: \n",
      "\n",
      "#መቄዶንያ\n",
      "\n",
      "ሰውን ለመርዳት ሰው መሆን በቂ ነው !\n",
      "\n",
      "ትላንት የካቲት 1/2017 ዓ/ም በጀመረው የመቄዶንያ የአረጋዊያን እና የአእምሮ ህሙማን መርጃ ማዕከል የድጋፍ ማሰባሰብ ዘመቻ እስኩን 120,000,000 ብር ተሰብስቧል።\n",
      "\n",
      "መቄዶንያ በሚያስገነባው ሆስፒታል ጭምር ያለው ህንፃ ለማጠናቀቅ የገንዘብ እጥረት አጋጥሞታል። ህንፃው ለማጠናቀቅ ገንዘብ ተቸግረናል። ለማጠናቀቅ ወደ 5 ቢሊዮን ብር ያስፈልጋል።\n",
      "\n",
      "በቀጥታ ይከታተሉ 👇\n",
      "https://www.youtube.com/live/q0bMjwt9PvM?feature=shared\n",
      "\n",
      "የምትችሉትን ሁሉ ድጋፍ አድርጉ።\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "🔊 #የሠራተኞችድምጽ\n",
      "\n",
      "\" ቋሚ ሠራተኞች ሆነን ሳለ በደሞዝ ማሻሻያው አልተካተትንም \" - የሀዋሳ ዙሪያ ወረዳ መንግስት ሠራተኞች\n",
      "\n",
      "የማክሮ ኢኮኖሚ ማሻሻያ ሪፎርሙን ተከትሎ የሚከሰቱ የኑሮ ዉድነትና ተያያዥ ጉዳዮችን ታሳቢ በማድረግ የመንግስት ሠራተኞች ደሞዝ ማሻሻያ ተደርጎ ከጥቅምት ወር 2017 ዓ/ም ጀምሮ ተግባራዊ የተደረገ መሆኑ ይታወቃል።\n",
      "\n",
      "በሲዳማ ክልል፤ ሰሜናዊ ሲዳማ ዞን፤ ሀዋሳ ዙሪያ ወረዳ በተለያዩ የመንግስት መስሪያ ቤቶች የሚሰሩ የመንግስት ሠራተኞች ግን \" ከ2012 ዓ/ም ጀምሮ በቋሚነት ተቀጥረን እየሰራን ያለን ቢሆንም በአዲሱ የመንግስት ሠራተኞች የደመወዝ ማሻሻያ አልተካተትንም \" ሲሉ ቅሬታቸዉን ለቲክቫህ ኢትዮጵያ አስገብተዋል።\n",
      "\n",
      "ቅሬታቸዉን ካደረሱን መካከል ፦\n",
      "- በከተማ ልማትና ኮንስትራክሽን፣\n",
      "- ማዘጋጃ ቤቶች፣\n",
      "- በትምህርት ዘርፍ ፣\n",
      "- በሴቶችና ሕፃናት እንዲሁም በሕብረት ስራ ጽ/ቤቶች የሚሰሩ ሠራተኞች ናቸው።\n",
      "\n",
      "\" በወቅቱ በአግባቡ ማስታወቂያ ወጥቶ ተመዝግበንና ተወዳድረን ማለፋችን ተረጋግጦ የቋሚነት ደብዳቤ ተሰጥቶን ላለፉት አምስትና ስድስት ዓመታት ደሞዝ ሲከፈለን በቆየንባቸው መደቦች ላይ እየሰራን ባለንበት በአዲሱ የደሞዝ ማሻሻያ አለመካተታችን ለዘርፈ ብዙ ችግሮች ዳርጎናል \" ብለዋል።\n",
      "\n",
      "\" ለወረዳዉ ፐብሊክ ሰርቪስና የሰዉ ሃብት ልማት ጽ/ቤት እና ለክልሉ ፐብሊክ ሰርቪስ ቢሮ ቅሬታችንን በአካልና በፅሁፍ ብናቀርብም ተገቢዉ ምላሽ አልተሰጠንም ጉዳዩን ለኢትዮጵያ እምባ ጠባቂ ተቋም ለማቅረብ መረጃ እያደራጀን ነው \" ሲሉ ተናግረዋል።\n",
      "\n",
      "ቃላቸውን ለቲክቫህ ኢትዮጵያ የሰጡት ፤ የሀዋሳ ዙሪያ ወረዳ ፐብሊክ ሰርቪስ እና የሰዉ ሃብት ልማት ጽ/ቤት ኃላፊ አቶ ሃይሉ አቢኖ ፥ \" በወረዳዉ በ2012 ዓ/ም የነበረው አግባብነት በሌለው ቅጥር በአንድ መደብ ሶስትና አራት ሰዎችን በተደራራቢነት የመቅጠር ሁኔታዎች አሁን ለተፈጠረው ችግር ዋነኛ ምክንያት ሆኗል \" ሲሉ ገልጸዋል።\n",
      "\n",
      "ከዞኑና የክልሉ ፐብልክ ሰርቪስ ጋር በመናበብ መፍትሔ እያፈላለጉ ስለመሆኑም ጠቁመዋል።\n",
      "\n",
      "በወቅቱ ይህን ተግባር የፈፀሙ አመራሮች እና የሰዉ ሃብት ልማት ኃላፊዎች ላይ እርምጃ መወሰዱን የሚናገሩት ኃላፊዉ በወረዳዉ በዚህ መልክ ተጠቀጥረዉ በአዲሱ የደመወዝ ማሻሻያ ያልካተቱና በቀጣይ መፍትሔ የሚፈለግላቸዉ 470 በተለያዩ መስሪያ ቤቶች ዉስጥ የተለዩ ሰራተኞች ስለመኖራቸዉ አክለዋል።\n",
      "\n",
      "የሰሜናዊ ሲዳማ ዞን ፐብልክ ሰርቪስና የሰዉ ሃይል ልማት መምሪያ ኃላፊ አቶ በዛብህ ባርሶ በበኩላቸው በ2011 እና 2012 በአከባቢው ሕገወጥ ቅጥሮች መፈፀማቸውን ገልጸዋል።\n",
      "\n",
      "በወረዳዉ አጣሪ ቡድን ተቋቁሞ በአዲሱ ደሞዝ ያልተካተቱንና በወረዳው ቅጥር ያልተፈፀመባቸዉ ክፍት መደቦችን የመለየት ስራ መከናወኑን አንስተዉ በሀዋሳ ዙሪያ ወረዳ ብቻ 407 ክፍት መደቦች መኖራቸዉን ለማወቅ መቻሉን ገልፀዋል።\n",
      "\n",
      "የክልሉ የበላይ አመራሮች በሚያስቀምጡት አቅጣጫ መሰረት እነዚህን ሠራተኞች በነዚህ ክፍት መደቦች የመደልደልና ሌሎችም ሕጋዊ አመራጮች በመፈለግ በአጭር ጊዜ ዉስጥ እልባት ለመስጠት እየተሰራ መሆኑን አስታውቀዋል።\n",
      "\n",
      "ቲክቫህ ኢትዮጵያ ጉዳዩን እስከመጨረሻ ተከታትሎ መረጃውን ይልካል።\n",
      "\n",
      "#TikvahEthiopiaFamilyHW\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "የIMF ማኔጂንግ ዳይሬክተሯ ምን አሉ ?\n",
      "\n",
      "የዓለም አቀፍ የገንዘብ ተቋም (IMF) ማኔጂንግ ዳይሬክተር ክሪስታሊና ጆርጂዬቫ በኢትዮጵያ የስራ ቆይታ አድርገዋል።\n",
      "\n",
      "በዚህም ወቅት ከጠ/ሚ ዐቢይ አህመድ (ዶ/ር) ጋር ጨምሮ ከፌዴራል ከፍተኛ ባለስልጣናት ጋር መክረዋል።\n",
      "\n",
      "የነበራቸውን ቆይታ በተመለከተ ከገንዘብ ሚኒስትሩ አቶ አህመድ ሽዴ ጋር በጋራ መግለጫ ሰጥተው ነበር።\n",
      "\n",
      "ምን አሉ ?\n",
      "\n",
      "ዳይሬክተሯ ፤ \" የኢትዮጵያ ሪፎርም ከባድ እና ጊዜ የሚወስድ ነው ፤ እባካችሁ ታገሱ \" የሚል ጥሪ አቅርበዋል።\n",
      "\n",
      "ኢትዮጵያውያን ለትዕግስት እንዲያሳዩ እና ከመንግስት የኢኮኖሚ ማሻሻያ ጥረቶች ጎን እንዲቆሙ ጠይቀዋል።\n",
      "\n",
      "ጆርጂዬቫ ፥ \" የሪፎርሙን ግቦች ለማሳካት የአንድነት አስፈላጊ ነው \" ሲሉ አፅንኦት ሰጥተዋል።\n",
      "\n",
      "\" ኢትዮጵያ የተቀበለችው ሪፎርም ከባድ እና ጊዜ የሚወስድ ቢሆንም እጅግ ትልቅ ውጤት ያስገኛል \" ሲሉ ተናግረዋል።\n",
      "\n",
      "\" ህዝቡ በትዕግስት እንዲጠብቅ ጥሪዬን አቀርባለሁ \" ያሉት ማኔጂንግ ዳይሬክተሯ \" ህብረተሰቡ ከሪፎርሙ ጀርባ በመሰባሰብ በአንድነት ድጋፍ ማድረግ አለበት \" ብለዋል።\n",
      "\n",
      "ጆርጂዬቫ ፥ ኢኮኖሚውን የበለጠ አጥጋቢና ብቁ ለማድረግ ብዙ የሚሠራ ሥራ አለ \" ብለው \" እባካችሁ መንግሥት ሥራውን እንዲያጠናቅቅ ድጋፍ አድርጉ \" የሚል ጥሪ አቅርበዋል።\n",
      "\n",
      "የዋጋ ንረትን ለመፍታት የሚሰራው ስራ ውስብስብ መሆኑን ያልሸሸጉት ዳይሬክተራ \" የዋጋ ንረትን ወደ ታች ለማውረድ ጠንካራ የገንዘብና የፊስካል ፖሊሲዎች፣ የኢኮኖሚውን የማምረት አቅም ማስፋት፣ የወጪ ንግድና የውጭ ምንዛሪ ገቢን ማሳደግ እና የግሉ ሴክተርን ማብቃት ይጠይቃል \" ብለዋል።\n",
      "\n",
      "ሌላው ያነሱት ጉዳይ በG20 የጋራ ማዕቀፍ ኢትዮጵያ እያካሄደች ያለችውን የዕዳ መልሶ ማደራጀት ድርድር በተመለከተ ነው።\n",
      "\n",
      "ጆርጂዬቫ ፤ \" የዕዳ መልሶ ማዋቀር ሂደት የመጨረሻ ደረጃ ላይ ይገኛል ፤ ከኢትዮጵያ አበዳሪዎች ጋር ባለኝ ግንኙነት ይህ ቅድሚያ የሚሰጠው ጉዳይ ነው \" ሲሉ ገልጸዋል። \n",
      "\n",
      "የIMF ፕሮግራም አካል ሆነውን የታክስ እርምጃዎችን በተመለከተም ፤ የኢትዮጵያ ባለስልጣናት ለብሄራዊ በጀቱ ድጋፍ ለማድረግ ወሳኝ የሆኑ የታክስ አቅሞችን መለየታቸውን ጠቁመዋል። \n",
      "\n",
      "ጆርጂዬቫ ፥ የኢትዮጵያ አጠቃላይ የሀገር ውስጥ ምርት ዕድገት ከIMF የመጀመሪያ ትንበያዎች መብለጡን ማብራራታቸውን ዘሪፖርተር አስነብቧል።\n",
      "\n",
      "የማኔጂንግ ዳይሬክተራ ንግግር ተከትሎ \" መሬት ላይ ካለው እውነታ ጋር የሚገናኝ አይደለም \" የሚሉ አስተያየቶች ሲሰጡም ተመልክተናል።\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "ልታስመርቀዉ ከቤተሰቦቿ ተደብቃ የመጣች እጮኛዉን ጭካኔ በተሞላበት ሁኔታ የገደለዉ ግለሰብ በ20 ዓመት ጽኑ እስራት ተቀጣ።\n",
      "\n",
      "በደቡብ ኢትዮጵያ ክልል ጋሞ ዞን አርባምንጭ ከተማ አስተዳደር ጉርባ በሚባል ቀበሌ የገዛ እጮኛዉን ጭካኔ በተሞላበት መልኩ በአሰቃቂ ሁኔታ በስለት አንገቷን በመቁረጥ ሕይወቷ እንዲያልፍ ያደረገዉ ወጣት በፅኑ እስራት መቀጣቱን የአርባ ምንጭ ከተማ አስተዳደር ፖሊስ መምሪያ አዛዥ ም/ኢንስፔክተር ጋፋሮ ቶማስ ለቲክቫህ ኢትዮጵያ ተናግረዋል።\n",
      "\n",
      "የወንጀል ድርጊቱ የተፈፀመዉ ነሐሴ 16/2016 ዓ/ም በአርባምንጭ ከተማ ጉርባ ቀበሌ ነው።\n",
      "\n",
      "ተከሳሽ ዮናስ ጫፊቄ የተባለው ግለሰብ የጂንካ ዩኒቨርሲቲ 1ኛ  ዓመት ተማሪ የሆነችዉን ሟች ሊዲያ ዮሐንስ እሱን ለማስመረቅ ወደ አርባ ምንጭ ከተማ በመጣችበት ተከራይቶ በሚኖርበት ቤት አሰቃቂ ድርጊቱን መፈፀሙን የምርመራ መዝገቡ ያስረዳል።\n",
      "\n",
      "ወጣቷ \" እጮኛዬ ይመረቅልኛል \" በሚል ደስታ ከቤተሰቦቿ ተደብቃ ተከሳሽ ተከራይቶ ወደ ሚማርበት  ቤት መጥታ በዋዜማዉ ለምረቃዉ የሚሆኑ የዲኮር፣ የዳቦና ለስላሳ መጠጦችና በቡና ዝግጅት ቤቱን አሰማምራ በምሽቱም ግቢ ዉስጥ ያሉ ተከራዮችን ጠርተዉ ከሸኙ በኋላ ሟች ሀገር ሰላም ብላ በተኛችበት ከሌሊቱ 7 ሰዓት ገደማ እራሷን መከላከል በማትችልበት ሁኔታ በቢላዋ አንገቷን አርዶ መግደሉን የምርመራ መዝገቡን ዋቢ አድርገው ፖሊስ አዛዡ ገልፀዋል።\n",
      "\n",
      "ፖሊስ አዛዡ አክለው እንደገለጹት ፥ በወቅቱ በተደረገዉ ማጣራትም ሆነ በክስ መዝገቡ ላይ እንደሰፈረዉ ወንጀለኛው  \" ወደ ዩኒቨርሲቲ በሄድሽበት ሌላ የወንድ ጓደኛ ይዘሻል \" በሚል ነው በር ዘግቶ አሰቃቂ የወንጀል ድርጊቱን የፈጸመው።\n",
      "\n",
      "ፖሊስ በዚህ ዘግናኝ ወንጀል ዙሪያ ተገቢዉን ማጣራትና ምርመራ አድርጎ ለዐቃቤ ሕግ ማቅረቡን አስታውቀዋል።\n",
      "\n",
      "ዐቃቤ ሕግም ክስ በመመስረት ለፍርድ ቤቱ ተከሳሽ የወንጀል ድርጊቱን መፈፀሙን የሰዉ፣ የሰነድና የህክምና ማስረጃዎችን በማቅረብ አስረድቷል።\n",
      "\n",
      "በቀረቡ ማስረጃዎች እና ምስክሮች ግራ ቀኙን ሲያጣራ የቆየዉ የጋሞ ዞን ከፍተኛዉ ፍርድ ቤት በቀን 29/5/2017 ዓም በዋለዉ ችሎት ተከሳሽ ዮናስ ጨፊቄ በተከሰሰበት በአሰቃቂ ሁኔታ ነብስ የማጥፋት ወንጀል ጥፋተኛ መሆኑን በማረጋገጥ በ20 ዓመት ፅኑ እስራት እንዲቀጣ መወሰኑንም ኢንስፔክተር ጋፋሮ ቶማስ ለቲክቫህ ኢትዮጵያ ተናግረዋል።\n",
      "\n",
      "#TikvahEthiopiaFamilyHW\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "#መቄዶንያ\n",
      "\n",
      "\" ሰውን ለመርዳት ሰው መሆን በቂ ነው !! \"\n",
      "\n",
      "መቄዶንያ የአረጋዊያንና የአእምሮ ህሙማን መርጃ ማዕከል በሚያስገነባው ሆስፒታል ጭምር ያለው ህንፃ ለማጠናቀቅ የገንዘብ እጥረት ስለገጠመው እግዛ እንዲደረግ ለመላው ኢትዮጵያውያን ጥሪ መቅረቡ ይታወሳል።\n",
      "\n",
      "ዛሬ “ በሰይፉ ኢቢኤስ የዩቱብ ቻነል ” ድጋፍ ማድረጊያ መርሀ ግብር እየተካሄደ ነው።\n",
      "\n",
      "ደጋጎች ሁሉ ድጋፍ እንድታደርጉ ጥሪ እናቀርባለን።\n",
      "\n",
      "የድጋፍ ማድረጊያ አማራጮች ከላይ ተያይዘዋል።\n",
      "\n",
      "መቄዶንያ “ ህንፃው ለማጠናቀቅ ገንዘብ ተቸግረናል። ለማጠናቀቅ ወደ 5 ቢሊዮን ብር ያስፈልገናል ” ማለቱ አይዘነጋም።\n",
      "\n",
      "በቀጥታ ይከታተሉ 👇\n",
      "https://www.youtube.com/live/q0bMjwt9PvM?feature=shared\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=load_data(\"datas/totaldata.json\")\n",
    "number_of_contents=len(data)\n",
    "print(f'Total number of contents: {number_of_contents}\\n')\n",
    "print(f'First 5 contents: \\n')\n",
    "for news in data[:5]:\n",
    "    print(news)\n",
    "    print(\"---------------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean our data using clean_text function and sample our data to see the difference between the original and raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data=[clean_text(content) for content in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at index 182822 before cleaning: \n",
      "\n",
      " #የዛሬ (ለነሐሴ 4/2014 የተመረጡ የቲክቫህ ዜናዎች)\n",
      "\n",
      "💐 የሙርሌ ጎሳዎች ባደረሱት ጥቃት የሁለት ሰዎች ህይወት አለፈ....\n",
      "\n",
      "- በጋምቤላ ከደቡብ ሱዳን የሚነሱ የ \"ሙርሌ ጎሳ\" ታጣቂዎች ድንበር አቋርጠው በመግባት ከትናንት በስቲያ ከምሽቱ 4:30 ላይ በፈጸሙት ጥቃት የ2 ሰው ህይወት ሲያልፍ አንድ ሰው ቆስሎ 2 ህፃናት በታጣቂዎች መወሰዳቸውን የክልሉ ፖሊስ አሳውቋል። በአሁኑ ሰዓት ልዩ ኃይሉንና ሚሊሻ በመጠቀም ሰላምና ፀጥታ የማስከበር ስራውን እየሰራ መሆኑን አሳውቋል።\n",
      "\n",
      "🏛 የክላስተር አደረጃጀትን ያላጸደቀው ብቸኛው የጉራጌ ዞን ምክር ቤት ነገ አስቸኳይ ስብሰባውን ያካሂዳል.....\n",
      "\n",
      "- በደቡብ ክልል ከሚገኙት ዞኖችና ልዩ ወረዳዎች ውስጥ የ \" ክላስተር \" አደረጃጀትን ባለማጽድቅ ብቸኛ የሆነው የጉራጌ ዞን ምክር ቤት አስቸኳይ ጉባኤውን ነገ ሐሙስ ነሐሴ 5/2014 እንደሚያካሂድ ተገልጿል። ከክልልነት ጥያቄ ጋር ተያይዞ በትላንትናው ዕለት በወልቂጤ ከተማ የስራ ማቆም አድማ ተደርጎ የነበር ሲሆን ዛሬ ሩቡዕ  ከተማዋ ወደ መደበኛ እንቅስቃሴ መግባቷ ተሰምቷል።\n",
      "\n",
      "🇺🇸🇸🇴 አሜሪካ 4 የአልሸባብ የሽብር ቡድን አባላትን መግደሏን ገለጸች...\n",
      "\n",
      "- አሜሪካ ትላንትና ማክሰኞ ፤ ከሶማሊያ መንግስት ጋር በመተባበር በፈፀመችው የአየር ድብደባ 4 የአልሸባብ የሽብር ቡድን አባላትን መግደሏን አሳውቃለች።\n",
      "\n",
      "4⃣ የደቡብ ምዕራብ ኢትዮጵያ ህዝቦች ክልል የብዝሃ ዋና ከተሞች ረቂቅ አዋጅን አጸደቀ....\n",
      "\n",
      "አዲሱ የደቡብ ምዕራብ ኢትዮጵያ ህዝቦች ክልል መንግስት የብዝሃ ዋና ከተሞች ረቂቅ አዋጅን መርምሮ ማፅደቁ ተሰምቷል። በዚህም ቦንጋ ከተማ የክልሉ የፖለቲካ እና የርዕሰ መስተዳደሩ መቀመጫ፤ ተርጫ ከተማ የክልሉ ምክር ቤት መቀመጫ፤ ሚዛን አማን ከተማ የክልሉ የዳኝነት አካል መቀመጫ እንዲሁም ቴፒ ከተማ የክልሉ የብሔረሰቦች ምክር ቤት መቀመጫ እንደሚሆኑ በረቂቅ አዋጁ ተቀምጧል።\n",
      "\n",
      "🧑‍💻 በቴክኖሎጂ ሳምንት የተዘጋጀው ኤግዚቢሽን ዛሬ በይፋ ተጀምሯል....\n",
      "\n",
      "- የኦሮሚያ ቱሪዝም ኮሚሽን ያዘጋጀው \"የቱሪዝም እና ቴክኖሎጂ ሳምንት ዛሬ በስካይ ላይት ሆቴል መካሄድ ጀምሯል። በቱሪዝም እና በቴክኖሎጂ ሴክተሩ የተሰማሩ ተቋማት ተሳትፈዋል። ኤግዚቢሽኑ ዛሬን ጨምሮ ለተከታታይ 4 ቀናት ለሁሉም ሰው ክፍት ሆኖ የሚካሄድ ይሆናል።\n",
      "\n",
      "✅ ባህርዳር ስታዲየም ከካፍ ፈቃድ ተሰጠው ...\n",
      "\n",
      "ሀገራችንን በመወከል በአፍሪካ መድረክ የሚሳተፉት ቅዱስ ጊዮርጊስ እና ፋሲል ከነማ ጨዋታቸውን በባህርዳር አለም አቀፍ ስታዲየም እንደሚያደርጉ ካፍ በቅድመ ማጣርያው የሚሳተፉ ክለቦች መርሐ ግብሩን የሚያደርጉበትን ስታዲየም ይፋ ባደረገበት ወቅት ገልጿል። በዚህም ቅዱስ ጊዮርጊስ የመጀመሪያ ጨዋታውን ጳጉሜ 4/2014 - መስከረም 1/2015 ባሉት ቀናቶች ውስጥ የሚያደርጉ ይሆናል።\n",
      "\n",
      "🧍‍♂ መነጋገሪያ ሆኖ የቆየው አቡበከር ናስር ዛሬ ለአዲሱ ክለቡ ጨዋታውን አድርጓል...\n",
      "\n",
      "የዋልያዎቹ የፊት መስመር አጥቂ አቡበከር ናስር ለአዲሱ ክለቡ ማሜሎዲ ሰንዳውንስ ተቀይሮ በመግባት የመጀመሪያ ጨዋታውን አድርጓል። በጨዋታው ማሜሎዲ ሰንዳውንስ በመጀመሪያው አጋማሽ በተቆጠረበት ግብ 1 ለ 0 ሽንፈትን አስተናግዷል።\n",
      "\n",
      "@tikvahethmagazine  @tikvahmagbot\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Data at index 182822 after cleaning: \n",
      "\n",
      "  (ለነሐሴ 4/2014 የተመረጡ የቲክቫህ ዜናዎች)\n",
      "\n",
      "  የሙርሌ ጎሳዎች ባደረሱት ጥቃት የሁለት ሰዎች ህይወት አለፈ....\n",
      "\n",
      "- በጋምቤላ ከደቡብ ሱዳን የሚነሱ የ \"ሙርሌ ጎሳ\" ታጣቂዎች ድንበር አቋርጠው በመግባት ከትናንት በስቲያ ከምሽቱ 4:30 ላይ በፈጸሙት ጥቃት የ2 ሰው ህይወት ሲያልፍ አንድ ሰው ቆስሎ 2 ህፃናት በታጣቂዎች መወሰዳቸውን የክልሉ ፖሊስ አሳውቋል። በአሁኑ ሰዓት ልዩ ኃይሉንና ሚሊሻ በመጠቀም ሰላምና ፀጥታ የማስከበር ስራውን እየሰራ መሆኑን አሳውቋል።\n",
      "\n",
      "  የክላስተር አደረጃጀትን ያላጸደቀው ብቸኛው የጉራጌ ዞን ምክር ቤት ነገ አስቸኳይ ስብሰባውን ያካሂዳል.....\n",
      "\n",
      "- በደቡብ ክልል ከሚገኙት ዞኖችና ልዩ ወረዳዎች ውስጥ የ \" ክላስተር \" አደረጃጀትን ባለማጽድቅ ብቸኛ የሆነው የጉራጌ ዞን ምክር ቤት አስቸኳይ ጉባኤውን ነገ ሐሙስ ነሐሴ 5/2014 እንደሚያካሂድ ተገልጿል። ከክልልነት ጥያቄ ጋር ተያይዞ በትላንትናው ዕለት በወልቂጤ ከተማ የስራ ማቆም አድማ ተደርጎ የነበር ሲሆን ዛሬ ሩቡዕ  ከተማዋ ወደ መደበኛ እንቅስቃሴ መግባቷ ተሰምቷል።\n",
      "\n",
      "   አሜሪካ 4 የአልሸባብ የሽብር ቡድን አባላትን መግደሏን ገለጸች...\n",
      "\n",
      "- አሜሪካ ትላንትና ማክሰኞ ፤ ከሶማሊያ መንግስት ጋር በመተባበር በፈፀመችው የአየር ድብደባ 4 የአልሸባብ የሽብር ቡድን አባላትን መግደሏን አሳውቃለች።\n",
      "\n",
      "  የደቡብ ምዕራብ ኢትዮጵያ ህዝቦች ክልል የብዝሃ ዋና ከተሞች ረቂቅ አዋጅን አጸደቀ....\n",
      "\n",
      "አዲሱ የደቡብ ምዕራብ ኢትዮጵያ ህዝቦች ክልል መንግስት የብዝሃ ዋና ከተሞች ረቂቅ አዋጅን መርምሮ ማፅደቁ ተሰምቷል። በዚህም ቦንጋ ከተማ የክልሉ የፖለቲካ እና የርዕሰ መስተዳደሩ መቀመጫ፤ ተርጫ ከተማ የክልሉ ምክር ቤት መቀመጫ፤ ሚዛን አማን ከተማ የክልሉ የዳኝነት አካል መቀመጫ እንዲሁም ቴፒ ከተማ የክልሉ የብሔረሰቦች ምክር ቤት መቀመጫ እንደሚሆኑ በረቂቅ አዋጁ ተቀምጧል።\n",
      "\n",
      "  በቴክኖሎጂ ሳምንት የተዘጋጀው ኤግዚቢሽን ዛሬ በይፋ ተጀምሯል....\n",
      "\n",
      "- የኦሮሚያ ቱሪዝም ኮሚሽን ያዘጋጀው \"የቱሪዝም እና ቴክኖሎጂ ሳምንት ዛሬ በስካይ ላይት ሆቴል መካሄድ ጀምሯል። በቱሪዝም እና በቴክኖሎጂ ሴክተሩ የተሰማሩ ተቋማት ተሳትፈዋል። ኤግዚቢሽኑ ዛሬን ጨምሮ ለተከታታይ 4 ቀናት ለሁሉም ሰው ክፍት ሆኖ የሚካሄድ ይሆናል።\n",
      "\n",
      "  ባህርዳር ስታዲየም ከካፍ ፈቃድ ተሰጠው ...\n",
      "\n",
      "ሀገራችንን በመወከል በአፍሪካ መድረክ የሚሳተፉት ቅዱስ ጊዮርጊስ እና ፋሲል ከነማ ጨዋታቸውን በባህርዳር አለም አቀፍ ስታዲየም እንደሚያደርጉ ካፍ በቅድመ ማጣርያው የሚሳተፉ ክለቦች መርሐ ግብሩን የሚያደርጉበትን ስታዲየም ይፋ ባደረገበት ወቅት ገልጿል። በዚህም ቅዱስ ጊዮርጊስ የመጀመሪያ ጨዋታውን ጳጉሜ 4/2014 - መስከረም 1/2015 ባሉት ቀናቶች ውስጥ የሚያደርጉ ይሆናል።\n",
      "\n",
      "  መነጋገሪያ ሆኖ የቆየው አቡበከር ናስር ዛሬ ለአዲሱ ክለቡ ጨዋታውን አድርጓል...\n",
      "\n",
      "የዋልያዎቹ የፊት መስመር አጥቂ አቡበከር ናስር ለአዲሱ ክለቡ ማሜሎዲ ሰንዳውንስ ተቀይሮ በመግባት የመጀመሪያ ጨዋታውን አድርጓል። በጨዋታው ማሜሎዲ ሰንዳውንስ በመጀመሪያው አጋማሽ በተቆጠረበት ግብ 1 ለ 0 ሽንፈትን አስተናግዷል።\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "index=random.randint(0,number_of_contents)\n",
    "print(f\"Data at index {index} before cleaning: \\n\\n\",data[index])\n",
    "print(\"\\n---------------------------------------------------------------------------------\\n\\n\")\n",
    "print(f\"Data at index {index} after cleaning: \\n\\n\",cleaned_data[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, We Will Train the Tokenizer\n",
    "\n",
    "Tokenization is a critical step in natural language processing (NLP) as it converts raw text into smaller, meaningful units (tokens) that can be processed by machine learning models. Effective tokenization ensures that the model can understand and interpret the text accurately, which is essential for tasks like text classification, machine translation, and sentiment analysis.\n",
    "\n",
    "For this task, we will use the **SentencePiece tokenizer** instead of traditional word-based tokenization. The [SentencePieceTokenizer](https://www.tensorflow.org/text/api_docs/python/text/SentencepieceTokenizer) is a powerful tool that tokenizes text into **subword units**, which offers several advantages:\n",
    "\n",
    "1. **Handling Complex Word Structures**: SentencePiece breaks words into smaller subword units, making it effective for handling complex word structures and morphological variations, which are common in languages like Amharic.\n",
    "2. **Out-of-Vocabulary (OOV) Words**: By using subword tokenization, SentencePiece can handle out-of-vocabulary words more gracefully, as it can decompose them into known subword units.\n",
    "3. **Multilingual Support**: SentencePiece is language-agnostic, making it suitable for multilingual datasets. This is particularly useful for Amharic, as it can handle the repetition of common subwords and morphological patterns unique to the language.\n",
    "4. **Simplified Preprocessing**: SentencePiece works directly on raw text, eliminating the need for extensive preprocessing steps like word segmentation or stemming.\n",
    "5. **Seamless Integration**: It integrates seamlessly with popular machine learning frameworks like TensorFlow and PyTorch, ensuring consistent tokenization across training and inference pipelines.\n",
    "\n",
    "Given these benefits, SentencePiece is an ideal choice for tokenizing Amharic text, as it can effectively capture the language's unique characteristics while simplifying the overall preprocessing workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train sentencepiece tokenizer model first. in order to do that we need to save our cleaned data into a single corpus of text in .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datas/cleaned_data.txt\", \"a\") as file:\n",
    "    for content in cleaned_data:\n",
    "        file.write(content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Check 'amharic_bpe.model' and 'amharic_bpe.vocab'.\n"
     ]
    }
   ],
   "source": [
    "input_file=\"datas/cleaned_data.txt\"\n",
    "model_prefix=\"amharic_sp_model\"\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=input_file,\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=32000,  \n",
    "    model_type=\"bpe\",  \n",
    "    character_coverage=0.99, \n",
    "    num_threads=12,  \n",
    "    max_sentence_length=8192, \n",
    "    split_by_whitespace=True,\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    \n",
    ")\n",
    "print(\"Training complete! Check 'amharic_bpe.model' and 'amharic_bpe.vocab'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the sentencepeice tokenizer the next step is to load the trainied tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=spm.SentencePieceProcessor(model_file=\"amharic_sp_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code shows the process of tokenizing individual words from a given text, in this case, the first entry of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\t-->\tTokenization\n",
      "----------------------------------------\n",
      "በአማራ    \t-->\t[1763]\n",
      "ክልል     \t-->\t[233]\n",
      "መዲና     \t-->\t[5973]\n",
      "በባህር    \t-->\t[6161]\n",
      "ዳር      \t-->\t[1575]\n",
      "ከተማ     \t-->\t[140]\n",
      "ቀበሌ     \t-->\t[1582]\n",
      "14      \t-->\t[1448]\n",
      "ትላንት    \t-->\t[1534]\n",
      "መጋቢት    \t-->\t[2900]\n",
      "29      \t-->\t[2460]\n",
      "የመግሪብ   \t-->\t[66, 1901, 31797]\n",
      "ሰላት     \t-->\t[25, 200]\n",
      "ሰግደው    \t-->\t[25, 31795, 476]\n",
      "ሲመለሱ    \t-->\t[11140]\n",
      "የነበሩ    \t-->\t[824]\n",
      "አባት     \t-->\t[2970]\n",
      "ከ3      \t-->\t[10, 31875]\n",
      "ልጆቹ     \t-->\t[14764]\n",
      "እንዲሁም   \t-->\t[342]\n",
      "አንድ     \t-->\t[278]\n",
      "ጎረቤታቸውን \t-->\t[13460, 416]\n",
      "ጨምሮ     \t-->\t[774]\n",
      "አጠቃላይ   \t-->\t[1739]\n",
      "5       \t-->\t[285]\n",
      "ሰዎች     \t-->\t[146]\n",
      "በተከፈተባቸው\t-->\t[18007, 293]\n",
      "የጥይት    \t-->\t[17380]\n",
      "እሩምታ    \t-->\t[7, 3700, 31800]\n",
      "መገደላቸው  \t-->\t[10615]\n",
      "ተነግሯል።  \t-->\t[1928]\n",
      "ትላንትና   \t-->\t[14907]\n",
      "ምሽት     \t-->\t[1473]\n",
      "በግፍ     \t-->\t[13965]\n",
      "የተገደሉት  \t-->\t[12161]\n",
      "፥       \t-->\t[31769, 1]\n",
      "አቶ      \t-->\t[261]\n",
      "ሙሄ      \t-->\t[234, 31920]\n",
      "፣       \t-->\t[164]\n",
      "ልጃቸው    \t-->\t[9806]\n",
      "አበባዉ    \t-->\t[255, 31894]\n",
      "ሙሄ      \t-->\t[234, 31920]\n",
      "፣       \t-->\t[164]\n",
      "ሽኩር     \t-->\t[485, 17455]\n",
      "ሙሄ      \t-->\t[234, 31920]\n",
      "፣       \t-->\t[164]\n",
      "ሙላት     \t-->\t[16311]\n",
      "ሙሄ      \t-->\t[234, 31920]\n",
      "እና      \t-->\t[24]\n",
      "ጎረቤታቸው  \t-->\t[13460, 248]\n",
      "አቶ      \t-->\t[261]\n",
      "እንድሪስ   \t-->\t[28567]\n",
      "የተባሉ    \t-->\t[3631]\n",
      "ሲሆኑ     \t-->\t[2769]\n",
      "ስርዓት    \t-->\t[2510]\n",
      "ቀብራቸው   \t-->\t[24767]\n",
      "በዛሬው    \t-->\t[1129]\n",
      "ዕለት     \t-->\t[908]\n",
      "ተፈፅሟል።  \t-->\t[16431]\n",
      "እስካሁን   \t-->\t[1245]\n",
      "ገዳዮች    \t-->\t[28228]\n",
      "ስለመያዛቸው \t-->\t[2610, 15996]\n",
      "የተባለ    \t-->\t[2001]\n",
      "ነገር     \t-->\t[344]\n",
      "የለም።    \t-->\t[2930]\n",
      "በከተማዋ   \t-->\t[3509]\n",
      "ከተገደሉት  \t-->\t[18307]\n",
      "ሰዎች     \t-->\t[146]\n",
      "ባሻገር    \t-->\t[5045]\n",
      "ባህርዳር   \t-->\t[6786]\n",
      "ከተማ     \t-->\t[140]\n",
      "አባይ     \t-->\t[9324]\n",
      "ማዶ      \t-->\t[14198]\n",
      "የሚገኘው   \t-->\t[1836]\n",
      "መስጂድ    \t-->\t[15469]\n",
      "ከፍተኛ    \t-->\t[317]\n",
      "የመሳሪያ   \t-->\t[23970]\n",
      "ድብደባ    \t-->\t[5835]\n",
      "እንደተፈፀመበት\t-->\t[17342, 75]\n",
      "ተገልጿል።  \t-->\t[704]\n",
      "ከዚሁ     \t-->\t[10465]\n",
      "ጋር      \t-->\t[96]\n",
      "በተያያዘ   \t-->\t[1841]\n",
      "ዛሬ      \t-->\t[324]\n",
      "የባህር    \t-->\t[4162]\n",
      "ዳር      \t-->\t[1575]\n",
      "ሙስሊሞች   \t-->\t[16623]\n",
      "በክልሉ    \t-->\t[1826]\n",
      "በሙስሊሞች  \t-->\t[5, 11645, 888]\n",
      "ላይ      \t-->\t[30]\n",
      "አነጣጥረዋል \t-->\t[3660, 3617, 3677]\n",
      "ያሉትን    \t-->\t[4738]\n",
      "ግድያ     \t-->\t[2526]\n",
      "እና      \t-->\t[24]\n",
      "እገታ     \t-->\t[11047]\n",
      "በመቃወም   \t-->\t[8121]\n",
      "ሰልፍ     \t-->\t[2661]\n",
      "ማድረጋቸውን \t-->\t[6937]\n",
      "\"       \t-->\t[57]\n",
      "ሀሩን     \t-->\t[87, 516]\n",
      "ሚዲያ     \t-->\t[1386]\n",
      "\"       \t-->\t[57]\n",
      "ዘግቧል።   \t-->\t[1660]\n",
      "እስካሁን   \t-->\t[1245]\n",
      "በአማራ    \t-->\t[1763]\n",
      "ክልል     \t-->\t[233]\n",
      "እስልምና   \t-->\t[7407]\n",
      "ጉዳዮች    \t-->\t[1031]\n",
      "ከፍተኛ    \t-->\t[317]\n",
      "ምክር     \t-->\t[770]\n",
      "ቤትም     \t-->\t[13157]\n",
      "ሆነ      \t-->\t[517]\n",
      "በኢትዮጵያ  \t-->\t[616]\n",
      "እስልምና   \t-->\t[7407]\n",
      "ጉዳዮች    \t-->\t[1031]\n",
      "ጠቅላይ    \t-->\t[753]\n",
      "ምክር     \t-->\t[770]\n",
      "ቤት      \t-->\t[171]\n",
      "የተሰጠ    \t-->\t[4102]\n",
      "አስተያየት  \t-->\t[2172]\n",
      "የለም።    \t-->\t[2930]\n",
      "ቲክቫህ    \t-->\t[3353]\n",
      "ኢትዮጵያ   \t-->\t[210]\n",
      "በክልሉ    \t-->\t[1826]\n",
      "ተፈፅመዋል  \t-->\t[7854, 4434]\n",
      "ስለተባሉ   \t-->\t[3908, 2533]\n",
      "ግድያዎች   \t-->\t[20124]\n",
      "፣       \t-->\t[164]\n",
      "ጥቃቶች    \t-->\t[5671]\n",
      "፣       \t-->\t[164]\n",
      "ዘረፋና    \t-->\t[8667, 31786]\n",
      "እገታዎች   \t-->\t[21109, 4958]\n",
      "የአማራ    \t-->\t[1562]\n",
      "ክልል     \t-->\t[233]\n",
      "እስልምና   \t-->\t[7407]\n",
      "ጉዳዮች    \t-->\t[1031]\n",
      "ከፍተኛ    \t-->\t[317]\n",
      "ምክር     \t-->\t[770]\n",
      "ቤት      \t-->\t[171]\n",
      "እና      \t-->\t[24]\n",
      "የሚመለከታቸው\t-->\t[7102]\n",
      "አካላትን   \t-->\t[6013]\n",
      "ለማነጋገር  \t-->\t[20680]\n",
      "ጥረት     \t-->\t[1503]\n",
      "እያደረገ   \t-->\t[2814]\n",
      "ይገኛል፤   \t-->\t[2094, 31916]\n",
      "ምላሽ     \t-->\t[1243]\n",
      "እንዳገኘ   \t-->\t[18994]\n",
      "ተጨማሪ    \t-->\t[992]\n",
      "መረጃዎችን  \t-->\t[3299]\n",
      "ያቀርባል።  \t-->\t[17836]\n"
     ]
    }
   ],
   "source": [
    "# printing the encoding of each word to see how subwords are tokenized\n",
    "tokenized_text = [(list(tokenizer.tokenize(word)), word) for word in cleaned_data[3000].split()]\n",
    "\n",
    "print(\"Word\\t\\t-->\\tTokenization\")\n",
    "print(\"-\"*40)\n",
    "for element in tokenized_text:\n",
    "    print(f\"{element[1]:<8}\\t-->\\t{element[0]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take data from the cleaned_data  and see how the tokenization of the whole content looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at index 890 before tokenization: \n",
      "\n",
      "\" ሰላም ከአንገት በላይና ዝም ላለማለት ያህል የምንናገረው ሳይሆን ዋጋ ከፍለን የምናመጣው ነው \" - ቅዱስነታቸው\n",
      "\n",
      "ዛሬ የሰላም ሚኒስቴር አንድ  ዓለም አቀፍ ኮንፈረንስ አዘጋጅቶ ነበር።\n",
      "\n",
      "በዚህ መድረክ ላይም የሰላም ሚኒስትር አቶ ብናልፍ አዱዓለም ፣ የኢትዮጵያ የሃይማኖት ተቋማት የበላይ ጠባቂ አባቶች፣ ብፁዓን አበው ሊቃነ ጳጳሳት ወኤጲስ ቆጶሳት ፣ የመንግሥት ባለስልጣናት ፣ አባሳደሮች ጭምር ተገኝተው ነበር።\n",
      "\n",
      "በመድረኩ  ፤ ብፁዕ ወቅዱስ አቡነ ማትያስ ቀዳማዊ ፓትርያርክ ርእሰ ሊቃነ ጳጳሳት ዘኢትዮጵያ ሊቀ ጳጳስ ዘአክሱም ወእጨጌ ዘመንበረ ተክለሃይማኖት መልዕክት አስተላልፈዋል።\n",
      "\n",
      "ቅዱስነታቸው ምን አሉ ?\n",
      "\n",
      "(ከመልዕክታቸው የተወሰደ)\n",
      "\n",
      "\" ሰላም የሰው ልጆች ፍላጎት፣ የብዙ ምንዱባን የየዕለት ናፍቆት ነው። የበርካታ ዘመናት ቅርሶች፤ ጊዜ፣ ገንዘብ እና የሰው ጉልበት የፈሰሰባቸው ግንባታዎች በሰላም ማጣት በቅጽበት ይፈርሳሉ። \n",
      "\n",
      "ሰላም ካለ የዓለም ሀብት ለሁሉም በቂ ነው። ሰላም ማጣት ግን ብዙ ሠራዊት፣ ብዙ የጦር መሳሪያ እንዲዘጋጅ እያደረገ ሀብትን ያወድማል። \n",
      "\n",
      "ጦርነት ማለት ሀብትና ሕይወትን ወደሚነድ እሳት ውስጥ መጣል ነው። \n",
      "\n",
      "የአንደኛና የሁለተኛ ዓለም ጦርነት፣ ታሪክ ብቻ ሳይሆን ጠባሳው አሁንም የዓለምን መልክ አበላሽቶታል። \n",
      "\n",
      "ሰላም በውስጥዋ ገራምነት፣ ትዕግሥት፣ ታዛዥነትና በትህትና ዝቅ ማለት ስለሚገኙ መራራ ትመስላለች፤ በውጤቷ ግን ሀገርን ከጥፋት፣ ሕዝብን ከመከራ ማትረፍ የሚቻል በመሆኑ ዋጋዋ ከፍ ያለ ነው። \n",
      "\n",
      "ቅድስት ቤተ ክርስቲያናችን ፦\n",
      "° ሰላም የሆነው ክርስቶስ የሚሰበክባት፣ \n",
      "° የሰላም መልእክተኞች በውስጥዋ የሚመላለሱባት፣ \n",
      "° በግብረ ኃጢአት የወደቁት በንስሓ ከእግዚአብሔር ጋር የሚታረቁባት የሰላም ድልድይ ስለሆነች በሥርዓተ ቅዳሴዋ ሰላምን ደጋግማ ታውጃለች፤ በጸሎቷ ለመላው ዓለም ሰላምን ትለምናለች፤ በጉልላትዋ ላይ የሰቀለችው መስቀልም ሰላምን የሚሰብክ ነው፤ የመስቀሉ ቅርፅ ወደ ላይና ወደ ጎን መሆኑም ከእግዚአብሔርና ከሰው ጋር ሰላም መሆን እንዳለብን የሚያስገነዝበን ነው። \n",
      "\n",
      "ታሪካችን እንደሚነግረን ወንድማማቾች ሲጋደሉ፣ በሕዝብ መካከል መተላለቅ ሲመጣ ቤተ ክርስቲያን ታቦት አክብራ፣ በእሳት መካከል ገብታ ሰላምን ስታወርድ የኖረች ናት።\n",
      "\n",
      "በሀገር ውስጥ ግጭቶች በተፈጠሩበት ጊዜም የሰላም ጥሪን ያላስተላለፈችበት ቀንና ሰዓት አይገኝም፡፡ \n",
      "\n",
      "ሰላምን የመወያያ ርእስ አድርገን ስንሰባስብ በጦርነት መካከል የተጨነቁ ሕዝቦች፣ ራሳቸውን መከላከል የማይችሉ እና ምን እየተደረገ እንዳለ በውል የማይገነዘቡ ደካሞች ተስፋ ያደርጉናል። \n",
      "\n",
      "ስለዚህ ሰላም ከአንገት በላይና ዝም ላለማለት ያህል የምንናገረው ሳይሆን ዋጋ ከፍለን የምናመጣው ስለሆነ ይህ ጉባኤ ከውይይት ባሻገር በተግባር ጭምር የሰላም ተምሳሌት መሆን እንደሚገባው ለማሳሰብ እንወዳለን። \"\n",
      "\n",
      "(ሙሉ መልዕክታቸው ከላይ ተያይዟል)\n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Data at index 890 after tokenization:  [57, 926, 14478, 1939, 246, 31786, 4081, 17646, 95, 1676, 1564, 13386, 1587, 1009, 225, 286, 2988, 12024, 39, 57, 98, 23162, 324, 1551, 520, 278, 1035, 789, 9395, 12209, 701, 349, 2691, 3806, 1551, 620, 261, 27, 25063, 10839, 2992, 164, 274, 5804, 775, 4127, 2546, 15815, 27, 1, 11264, 22484, 8700, 31769, 1, 445, 15, 31885, 1, 31778, 490, 1, 445, 164, 2634, 2491, 164, 250, 31813, 4950, 2909, 4587, 701, 8376, 376, 27, 1, 31896, 8278, 4000, 9654, 10128, 7152, 8186, 8700, 31769, 1, 445, 15013, 1987, 31769, 1, 31778, 19541, 21746, 16845, 17963, 1728, 7581, 23162, 183, 1696, 641, 130, 31789, 314, 16442, 248, 7080, 31926, 57, 926, 1655, 2379, 1799, 31842, 8426, 183, 31913, 5905, 6844, 15359, 19298, 3996, 188, 11595, 17823, 12728, 31916, 23108, 1063, 24, 1655, 12117, 739, 31798, 8071, 22956, 2246, 10742, 2566, 31948, 75, 24738, 11058, 926, 1747, 1725, 3087, 5156, 2204, 188, 926, 10742, 147, 614, 27849, 614, 1609, 1433, 28765, 2814, 30834, 22, 4244, 8663, 1332, 1193, 24275, 22631, 6300, 21599, 4622, 114, 18269, 188, 10961, 31786, 4968, 1035, 1332, 31842, 1628, 388, 1587, 15755, 31776, 1457, 31566, 3889, 208, 985, 31832, 813, 926, 11397, 31808, 44, 31799, 658, 31842, 2367, 31795, 602, 31842, 10395, 1, 3070, 21161, 2978, 1193, 31312, 5304, 31799, 47, 767, 6616, 31916, 14549, 31938, 147, 13486, 27515, 31842, 17817, 346, 1244, 13247, 17115, 906, 1009, 31808, 225, 216, 188, 8204, 437, 22961, 904, 31769, 1, 926, 1897, 11468, 23, 1932, 31805, 27439, 31769, 1, 1551, 9909, 771, 11397, 31808, 3057, 534, 31882, 27439, 31769, 1, 881, 279, 254, 1, 1959, 4009, 1872, 5, 156, 1, 811, 6916, 96, 3524, 10581, 480, 1551, 6763, 2397, 31787, 5, 5046, 5567, 247, 2512, 31808, 8101, 8284, 31790, 1786, 31888, 22872, 9775, 31840, 31938, 6755, 1035, 8101, 47, 113, 31786, 22872, 30219, 200, 31808, 30, 272, 31806, 6883, 12365, 993, 8101, 23, 27276, 3108, 3008, 25992, 16870, 79, 30801, 79, 3044, 6612, 811, 6916, 31786, 8187, 96, 926, 1468, 19979, 727, 1671, 31837, 3973, 188, 4083, 46, 106, 1887, 699, 11088, 1, 31787, 29, 31814, 31788, 11116, 10331, 513, 10540, 31820, 9400, 437, 2692, 27981, 1134, 809, 31842, 7078, 513, 20978, 8101, 21, 163, 243, 18219, 31787, 6926, 3302, 114, 6001, 27541, 75, 11819, 1551, 909, 31770, 2324, 107, 4045, 4480, 22381, 501, 149, 405, 2408, 8101, 66, 15700, 23675, 10337, 1793, 792, 5247, 7737, 513, 28, 22676, 8862, 31842, 5527, 3337, 12786, 24, 183, 1938, 1167, 10400, 1136, 24840, 17121, 1472, 18123, 1842, 2983, 926, 14478, 1939, 246, 31786, 4081, 17646, 95, 1676, 1564, 13386, 1587, 1009, 225, 286, 2988, 12024, 2397, 189, 1515, 2389, 1082, 5045, 8971, 2909, 1551, 17388, 1468, 14066, 2744, 169, 19142, 57, 130, 859, 30743, 2154, 2038, 1, 31774, 31926]\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Data at index 890 after detokenization: ['▁\"', '▁ሰላም', '▁ከአን', 'ገት', '▁በላይ', 'ና', '▁ዝም', '▁ላለማ', 'ለት', '▁ያህል', '▁የምን', 'ናገረው', '▁ሳይሆን', '▁ዋጋ', '▁ከፍ', 'ለን', '▁የምና', 'መጣው', '▁ነው', '▁\"', '▁-', '▁ቅዱስነታቸው', '▁ዛሬ', '▁የሰላም', '▁ሚኒስቴር', '▁አንድ', '▁ዓለም', '▁አቀፍ', '▁ኮንፈረንስ', '▁አዘጋጅቶ', '▁ነበር።', '▁በዚህ', '▁መድረክ', '▁ላይም', '▁የሰላም', '▁ሚኒስትር', '▁አቶ', '▁ብ', 'ናልፍ', '▁አዱ', 'ዓለም', '▁፣', '▁የኢትዮጵያ', '▁የሃይማኖት', '▁ተቋማት', '▁የበላይ', '▁ጠባቂ', '▁አባቶች፣', '▁ብ', 'ፁ', 'ዓን', '▁አበው', '▁ሊቃነ', '▁', 'ጳጳ', 'ሳት', '▁ወ', 'ኤ', 'ጲ', 'ስ', '▁ቆ', 'ጶ', 'ሳት', '▁፣', '▁የመንግሥት', '▁ባለስልጣናት', '▁፣', '▁አባ', 'ሳ', 'ደሮች', '▁ጭምር', '▁ተገኝተው', '▁ነበር።', '▁በመድረኩ', '▁፤', '▁ብ', 'ፁ', 'ዕ', '▁ወቅዱስ', '▁አቡነ', '▁ማትያስ', '▁ቀዳማዊ', '▁ፓትርያርክ', '▁ርእሰ', '▁ሊቃነ', '▁', 'ጳጳ', 'ሳት', '▁ዘኢትዮጵያ', '▁ሊቀ', '▁', 'ጳጳ', 'ስ', '▁ዘአክሱም', '▁ወእጨጌ', '▁ዘመንበረ', '▁ተክለሃይማኖት', '▁መልዕክት', '▁አስተላልፈዋል።', '▁ቅዱስነታቸው', '▁ምን', '▁አሉ', '▁?', '▁(', 'ከ', 'መል', 'ዕክ', 'ታቸው', '▁የተወሰደ', ')', '▁\"', '▁ሰላም', '▁የሰው', '▁ልጆች', '▁ፍላጎት', '፣', '▁የብዙ', '▁ምን', 'ዱ', 'ባን', '▁የየ', 'ዕለት', '▁ናፍ', 'ቆት', '▁ነው።', '▁የበርካታ', '▁ዘመናት', '▁ቅርሶች', '፤', '▁ጊዜ፣', '▁ገንዘብ', '▁እና', '▁የሰው', '▁ጉልበት', '▁የፈ', 'ሰ', 'ሰባቸው', '▁ግንባታዎች', '▁በሰላም', '▁ማጣት', '▁በቅ', 'ጽ', 'በት', '▁ይፈር', 'ሳሉ።', '▁ሰላም', '▁ካለ', '▁የዓለም', '▁ሀብት', '▁ለሁሉም', '▁በቂ', '▁ነው።', '▁ሰላም', '▁ማጣት', '▁ግን', '▁ብዙ', '▁ሠራዊት፣', '▁ብዙ', '▁የጦር', '▁መሳሪያ', '▁እንዲዘጋጅ', '▁እያደረገ', '▁ሀብትን', '▁ያ', 'ወድ', 'ማል።', '▁ጦርነት', '▁ማለት', '▁ሀብትና', '▁ሕይወትን', '▁ወደሚ', 'ነድ', '▁እሳት', '▁ውስጥ', '▁መጣል', '▁ነው።', '▁የአንደኛ', 'ና', '▁የሁለተኛ', '▁ዓለም', '▁ጦርነት', '፣', '▁ታሪክ', '▁ብቻ', '▁ሳይሆን', '▁ጠባሳ', 'ው', '▁አሁንም', '▁የዓለምን', '▁መልክ', '▁አበ', 'ላሽ', 'ቶ', 'ታል።', '▁ሰላም', '▁በውስጥ', 'ዋ', '▁ገ', 'ራ', 'ምነት', '፣', '▁ትዕ', 'ግ', 'ሥት', '፣', '▁ታዛ', 'ዥ', 'ነትና', '▁በትህትና', '▁ዝቅ', '▁ማለት', '▁ስለሚገኙ', '▁መራ', 'ራ', '▁ት', 'መስ', 'ላለች', '፤', '▁በውጤ', 'ቷ', '▁ግን', '▁ሀገርን', '▁ከጥፋት', '፣', '▁ሕዝብን', '▁ከመ', 'ከራ', '▁ማትረፍ', '▁የሚቻል', '▁በመሆኑ', '▁ዋጋ', 'ዋ', '▁ከፍ', '▁ያለ', '▁ነው።', '▁ቅድስት', '▁ቤተ', '▁ክርስቲያናችን', '▁፦', '▁', '°', '▁ሰላም', '▁የሆነው', '▁ክርስቶስ', '▁የሚ', 'ሰበ', 'ክ', 'ባት፣', '▁', '°', '▁የሰላም', '▁መልእክ', 'ተኞች', '▁በውስጥ', 'ዋ', '▁የሚመ', 'ላለ', 'ሱ', 'ባት፣', '▁', '°', '▁በግ', 'ብረ', '▁ኃ', 'ጢ', 'አት', '▁የወደ', 'ቁት', '▁በ', 'ንስ', 'ሓ', '▁ከእ', 'ግዚአብሔር', '▁ጋር', '▁የሚታ', 'ረቁ', 'ባት', '▁የሰላም', '▁ድልድይ', '▁ስለሆነ', 'ች', '▁በ', 'ሥር', 'ዓተ', '▁ቅ', 'ዳሴ', 'ዋ', '▁ሰላምን', '▁ደጋግ', 'ማ', '▁ታው', 'ጃ', 'ለች፤', '▁በጸ', 'ሎ', 'ቷ', '▁ለመላው', '▁ዓለም', '▁ሰላምን', '▁ት', 'ለም', 'ና', 'ለች፤', '▁በጉል', 'ላት', 'ዋ', '▁ላይ', '▁የሰ', 'ቀ', 'ለችው', '▁መስቀ', 'ልም', '▁ሰላምን', '▁የሚ', 'ሰብክ', '▁ነው፤', '▁የመስ', 'ቀሉ', '▁ቅርፅ', '▁ወደ', '▁ላይና', '▁ወደ', '▁ጎን', '▁መሆኑም', '▁ከእ', 'ግዚአብሔር', 'ና', '▁ከሰው', '▁ጋር', '▁ሰላም', '▁መሆን', '▁እንዳለብን', '▁የሚያስ', 'ገነ', 'ዝ', 'በን', '▁ነው።', '▁ታሪካ', 'ችን', '▁እንደሚ', 'ነግ', 'ረን', '▁ወንድማማ', 'ቾ', 'ች', '▁ሲ', 'ጋ', 'ደ', 'ሉ፣', '▁በሕዝብ', '▁መካከል', '▁መተላለ', 'ቅ', '▁ሲመጣ', '▁ቤተ', '▁ክርስቲያን', '▁ታቦት', '▁አክ', 'ብራ', '፣', '▁በእሳት', '▁መካከል', '▁ገብታ', '▁ሰላምን', '▁ስ', 'ታወ', 'ርድ', '▁የኖረ', 'ች', '▁ናት።', '▁በሀገር', '▁ውስጥ', '▁ግጭቶች', '▁በተፈጠሩ', 'በት', '▁ጊዜም', '▁የሰላም', '▁ጥሪ', 'ን', '▁ያላ', 'ስተ', 'ላለፈ', 'ችበት', '▁ቀንና', '▁ሰዓት', '▁አይ', 'ገኝ', 'ም፡፡', '▁ሰላምን', '▁የመ', 'ወያያ', '▁ርእስ', '▁አድርገን', '▁ስን', 'ሰባ', 'ስብ', '▁በጦርነት', '▁መካከል', '▁የተ', 'ጨነቁ', '▁ሕዝቦች', '፣', '▁ራሳቸውን', '▁መከላከል', '▁የማይችሉ', '▁እና', '▁ምን', '▁እየተደረገ', '▁እንዳለ', '▁በውል', '▁የማይ', 'ገነዘቡ', '▁ደካሞች', '▁ተስፋ', '▁ያደርጉ', 'ናል።', '▁ስለዚህ', '▁ሰላም', '▁ከአን', 'ገት', '▁በላይ', 'ና', '▁ዝም', '▁ላለማ', 'ለት', '▁ያህል', '▁የምን', 'ናገረው', '▁ሳይሆን', '▁ዋጋ', '▁ከፍ', 'ለን', '▁የምና', 'መጣው', '▁ስለሆነ', '▁ይህ', '▁ጉባኤ', '▁ከው', 'ይይት', '▁ባሻገር', '▁በተግባር', '▁ጭምር', '▁የሰላም', '▁ተምሳሌት', '▁መሆን', '▁እንደሚገባው', '▁ለማሳ', 'ሰብ', '▁እንወዳለን።', '▁\"', '▁(', 'ሙሉ', '▁መልዕክታቸው', '▁ከላይ', '▁ተያይ', 'ዟ', 'ል', ')']\n"
     ]
    }
   ],
   "source": [
    "index=890\n",
    "print(f\"Data at index {index} before tokenization:\", cleaned_data[index])\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(f\"Data at index {index} after tokenization: \", tokenizer.encode(cleaned_data[index]))\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(f\"Data at index {index} after detokenization:\", tokenizer.encode_as_pieces(cleaned_data[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pretrain our Transformer network, we will use the Masked Language Model (MLM) approach. This technique involves randomly masking a percentage of words in a sentence and replacing them with special tokens. The model then attempts to predict these masked words, enabling it to learn contextual and semantic representations effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be implementing the Masked language model (MLM) as shown in the following image. \n",
    "\n",
    "<img src = \"images/losses.png\" width=\"600\" height = \"400\">\n",
    "\n",
    "Assume you have the following text: <span style = \"color:blue\"> **ሰላም <span style = \"color:red\">የሰው ልጆች </span> ፍላጎት፣ የብዙ ምንዱባን የየዕለት <span style = \"color:red\">ናፍቆት</span>  ነው።** </span>     \n",
    "\n",
    "\n",
    "Now as input you will mask the words in red in the text: \n",
    "\n",
    "<span style = \"color:blue\"> **Input:**</span> ሰላም  **X** ፍላጎት፣ የብዙ ምንዱባን የየዕለት **Y** ነው።\n",
    "\n",
    "<span style = \"color:blue\">**Output:**</span> The model should predict the words(s) for **X** and **Y**. \n",
    "\n",
    "**[EOS]** will be used to mark the end of the target sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, I were able to take a piece of string and tokenize it. \n",
    "\n",
    "Now I will create `input` and `target` pairs that will allow me to pre-train the model. The model uses the ids at the end of the vocab file as sentinels. For example, it will replace: \n",
    "   - `vocab_size - 1` by `<Z>`\n",
    "   - `vocab_size - 2` by `<Y>`\n",
    "   - and so forth. \n",
    "   \n",
    "It assigns every word a `chr`.\n",
    "\n",
    "The `pretty_decode` function below, which I will use in a bit, helps in handling the type when decoding. \n",
    "\n",
    "Notice that:\n",
    "```python\n",
    "string.ascii_letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentinels(tokenizer, display=False):\n",
    "    sentinels = {}\n",
    "    vocab_size = tokenizer.vocab_size()\n",
    "    for i, char in enumerate(reversed(string.ascii_letters), 1):\n",
    "        decoded_text = tokenizer.detokenize([vocab_size - i])\n",
    "        \n",
    "        # Sentinels, ex: <Z> - <a>\n",
    "        sentinels[decoded_text] = f'<{char}>'    \n",
    "    \n",
    "        if display:\n",
    "            print(f'The sentinel is <{char}> and the decoded token is:', decoded_text)\n",
    "\n",
    "    return sentinels\n",
    "\n",
    "def pretty_decode(encoded_str_list, sentinels, tokenizer):\n",
    "    # If already a string, just do the replacements.\n",
    "    if type(encoded_str_list) == str:\n",
    "        for token, char in sentinels.items():\n",
    "            encoded_str_list = re.sub(token, char,encoded_str_list)\n",
    "        return encoded_str_list\n",
    "  \n",
    "    # We need to decode and then prettyfy it.\n",
    "    return pretty_decode(tokenizer.detokenize(encoded_str_list), sentinels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentinel is <Z> and the decoded token is: C\n",
      "The sentinel is <Y> and the decoded token is: ጌ\n",
      "The sentinel is <X> and the decoded token is: f\n",
      "The sentinel is <W> and the decoded token is: E\n",
      "The sentinel is <V> and the decoded token is: S\n",
      "The sentinel is <U> and the decoded token is: ጪ\n",
      "The sentinel is <T> and the decoded token is: ጁ\n",
      "The sentinel is <S> and the decoded token is: ቼ\n",
      "The sentinel is <R> and the decoded token is: ”\n",
      "The sentinel is <Q> and the decoded token is: b\n",
      "The sentinel is <P> and the decoded token is: ሟ\n",
      "The sentinel is <O> and the decoded token is: ዬ\n",
      "The sentinel is <N> and the decoded token is: ሂ\n",
      "The sentinel is <M> and the decoded token is: T\n",
      "The sentinel is <L> and the decoded token is: •\n",
      "The sentinel is <K> and the decoded token is: ቧ\n",
      "The sentinel is <J> and the decoded token is: y\n",
      "The sentinel is <I> and the decoded token is: ፦\n",
      "The sentinel is <H> and the decoded token is: ሔ\n",
      "The sentinel is <G> and the decoded token is: p\n",
      "The sentinel is <F> and the decoded token is: ቄ\n",
      "The sentinel is <E> and the decoded token is: g\n",
      "The sentinel is <D> and the decoded token is: ጻ\n",
      "The sentinel is <C> and the decoded token is: ኋ\n",
      "The sentinel is <B> and the decoded token is: “\n",
      "The sentinel is <A> and the decoded token is: ሠ\n",
      "The sentinel is <z> and the decoded token is: ሯ\n",
      "The sentinel is <y> and the decoded token is: ጆ\n",
      "The sentinel is <x> and the decoded token is: A\n",
      "The sentinel is <w> and the decoded token is: '\n",
      "The sentinel is <v> and the decoded token is: ኳ\n",
      "The sentinel is <u> and the decoded token is: ጓ\n",
      "The sentinel is <t> and the decoded token is: ኬ\n",
      "The sentinel is <s> and the decoded token is: ጿ\n",
      "The sentinel is <r> and the decoded token is: ፒ\n",
      "The sentinel is <q> and the decoded token is: ጦ\n",
      "The sentinel is <p> and the decoded token is: ቫ\n",
      "The sentinel is <o> and the decoded token is: ፎ\n",
      "The sentinel is <n> and the decoded token is: ሺ\n",
      "The sentinel is <m> and the decoded token is: ፌ\n",
      "The sentinel is <l> and the decoded token is: ?\n",
      "The sentinel is <k> and the decoded token is: m\n",
      "The sentinel is <j> and the decoded token is: u\n",
      "The sentinel is <i> and the decoded token is: ሐ\n",
      "The sentinel is <h> and the decoded token is: ፃ\n",
      "The sentinel is <g> and the decoded token is: ቪ\n",
      "The sentinel is <f> and the decoded token is: c\n",
      "The sentinel is <e> and the decoded token is: d\n",
      "The sentinel is <d> and the decoded token is: ጂ\n",
      "The sentinel is <c> and the decoded token is: ዞ\n",
      "The sentinel is <b> and the decoded token is: h\n",
      "The sentinel is <a> and the decoded token is: ጽ\n"
     ]
    }
   ],
   "source": [
    "sentinels = get_sentinels(tokenizer, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the `pretty_decode` function in the following sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-5'></a>\n",
    "### 3.4 - Tokenizing and Masking\n",
    "\n",
    "In this task, I will implement the `tokenize_and_mask` function, which tokenizes and masks input words based on a given probability. The probability is controlled by the `noise` parameter, typically set to mask around `15%` of the words in the input text. The function will generate two lists of tokenized sequences following the algorithm outlined below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  tokenize_and_mask\n",
    "\n",
    "- Start with two empty lists: `inps` and `targs`\n",
    "- Tokenize the input text using the given tokenizer.\n",
    "- For each `token` in the tokenized sequence:\n",
    "  - Generate a random number(simulating a weighted coin toss)\n",
    "  - If the random value is greater than the given threshold(noise):\n",
    "    - Add the current token to the `inps` list\n",
    "  - Else:\n",
    "    - If a new sentinel must be included:\n",
    "      - Compute the next sentinel ID using a progression.\n",
    "      - Add a sentinel into the `inps` and `targs` to mark the position of the masked element.\n",
    "    - Add the current token to the `targs` list.\n",
    "\n",
    "** There's a special case to consider. If two or more consecutive tokens get masked during the process, no need to add a new sentinel to the sequences. To account for this, use the `prev_no_mask` flag, which starts as `True` but is turned to `False` each time I mask a new element. The code that adds sentinels will only be executed if, before masking the token, the flag was in the `True` state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_mask(text, \n",
    "                      noise=0.15, \n",
    "                      randomizer=np.random.uniform, \n",
    "                      tokenizer=None):\n",
    "    \"\"\"Tokenizes and masks a given input.\n",
    "\n",
    "    Args:\n",
    "        text (str or bytes): Text input.\n",
    "        noise (float, optional): Probability of masking a token. Defaults to 0.15.\n",
    "        randomizer (function, optional): Function that generates random values. Defaults to np.random.uniform.\n",
    "        tokenizer (function, optional): Tokenizer function. Defaults to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        inps, targs: Lists of integers associated to inputs and targets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Current sentinel number (starts at 0)\n",
    "    cur_sentinel_num = 0\n",
    "    \n",
    "    # Inputs and targets\n",
    "    inps, targs = [], []\n",
    "\n",
    "    # Vocab_size\n",
    "    vocab_size = int(tokenizer.vocab_size())\n",
    "    \n",
    "    # EOS token id \n",
    "    # Must be at the end of each target!\n",
    "    eos = tokenizer.piece_to_id(\"</s>\")\n",
    "    \n",
    "\n",
    "    \n",
    "    # prev_no_mask is True if the previous token was NOT masked, False otherwise\n",
    "    # set prev_no_mask to True\n",
    "    prev_no_mask = True\n",
    "    \n",
    "    # Loop over the tokenized text\n",
    "    for token in tokenizer.tokenize(text):\n",
    "        \n",
    "        # Generate a random value between 0 and 1\n",
    "        rnd_val = randomizer() \n",
    "        \n",
    "        # Check if the noise is greater than a random value (weighted coin flip)\n",
    "        if noise > rnd_val:\n",
    "            \n",
    "            # Check if previous token was NOT masked\n",
    "            if prev_no_mask:\n",
    "                \n",
    "                # Current sentinel increases by 1\n",
    "                cur_sentinel_num += 1\n",
    "                \n",
    "                # Compute end_id by subtracting current sentinel value out of the total vocabulary size\n",
    "                end_id = vocab_size - cur_sentinel_num\n",
    "                \n",
    "                # Append end_id at the end of the targets\n",
    "                targs.append(end_id)\n",
    "                \n",
    "                # Append end_id at the end of the inputs\n",
    "                inps.append(end_id)\n",
    "                \n",
    "            # Append token at the end of the targets\n",
    "            targs.append(token)\n",
    "            \n",
    "            # set prev_no_mask accordingly\n",
    "            prev_no_mask = False\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Append token at the end of the inputs\n",
    "            inps.append(token)\n",
    "            \n",
    "            # Set prev_no_mask accordingly\n",
    "            prev_no_mask = True\n",
    "    \n",
    "    \n",
    "    # Add EOS token to the end of the targets\n",
    "    targs.append(eos)\n",
    "    \n",
    "\n",
    "    \n",
    "    return inps, targs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now take random value from the cleaned_data and pass it to `tokenize_and_mask` function and see how it randomly masks and separate inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random data before tokenization: \n",
      "\n",
      " \n",
      "\n",
      "በአማራ ክልል መዲና በባህር ዳር ከተማ ቀበሌ 14 ትላንት መጋቢት 29 የመግሪብ ሰላት ሰግደው ሲመለሱ የነበሩ አባት ከ3 ልጆቹ እንዲሁም አንድ ጎረቤታቸውን ጨምሮ አጠቃላይ 5 ሰዎች በተከፈተባቸው የጥይት እሩምታ መገደላቸው ተነግሯል።\n",
      "\n",
      "ትላንትና ምሽት በግፍ የተገደሉት ፥ አቶ ሙሄ ፣ ልጃቸው አበባዉ ሙሄ ፣ ሽኩር ሙሄ ፣ ሙላት ሙሄ እና ጎረቤታቸው አቶ እንድሪስ የተባሉ ሲሆኑ ስርዓት ቀብራቸው በዛሬው ዕለት ተፈፅሟል።\n",
      "\n",
      "እስካሁን ገዳዮች ስለመያዛቸው የተባለ ነገር የለም።\n",
      "\n",
      "በከተማዋ ከተገደሉት ሰዎች ባሻገር ባህርዳር ከተማ አባይ ማዶ የሚገኘው መስጂድ ከፍተኛ የመሳሪያ ድብደባ እንደተፈፀመበት ተገልጿል።\n",
      "\n",
      "ከዚሁ ጋር በተያያዘ ዛሬ የባህር ዳር ሙስሊሞች በክልሉ በሙስሊሞች ላይ አነጣጥረዋል ያሉትን ግድያ እና እገታ በመቃወም ሰልፍ ማድረጋቸውን \" ሀሩን ሚዲያ \" ዘግቧል።\n",
      "\n",
      "እስካሁን በአማራ ክልል እስልምና ጉዳዮች ከፍተኛ ምክር ቤትም ሆነ በኢትዮጵያ እስልምና ጉዳዮች ጠቅላይ ምክር ቤት የተሰጠ አስተያየት የለም።\n",
      "\n",
      "ቲክቫህ ኢትዮጵያ በክልሉ ተፈፅመዋል ስለተባሉ ግድያዎች ፣ ጥቃቶች ፣ ዘረፋና እገታዎች የአማራ ክልል እስልምና ጉዳዮች ከፍተኛ ምክር ቤት እና የሚመለከታቸው አካላትን ለማነጋገር ጥረት እያደረገ ይገኛል፤ ምላሽ እንዳገኘ ተጨማሪ መረጃዎችን ያቀርባል።\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_data=cleaned_data[3000]\n",
    "print(\"Random data before tokenization: \\n\\n\", random_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps_sample,targs_sample=tokenize_and_mask(random_data,noise=0.15,randomizer=np.random.uniform,tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[308], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInputs: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[43mpretty_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43minps_sample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentinels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTargets: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, pretty_decode(targs_sample, sentinels, tokenizer))\n",
      "Cell \u001b[0;32mIn[300], line 27\u001b[0m, in \u001b[0;36mpretty_decode\u001b[0;34m(encoded_str_list, sentinels, tokenizer)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# If the input is a TensorFlow tensor, decode it to a string\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoded_str_list, (\u001b[38;5;28mlist\u001b[39m, tf\u001b[38;5;241m.\u001b[39mTensor)):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Detokenize the list of token IDs\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     decoded_text_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_str_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     decoded_text \u001b[38;5;241m=\u001b[39m decoded_text_tensor\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Perform replacements on the decoded string\u001b[39;00m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlEnv/lib/python3.10/site-packages/sentencepiece/__init__.py:790\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Decode\u001b[0;34m(self, input, out_type, num_threads)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_threads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(num_threads) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    788\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_threads must be int\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[1;32m    791\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlEnv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:321\u001b[0m, in \u001b[0;36m_EagerTensorBase.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 321\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "print('Inputs: \\n\\n', pretty_decode(inps_sample, sentinels, tokenizer))\n",
    "print('\\nTargets: \\n\\n', pretty_decode(targs_sample, sentinels, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
